library(caret)
install.packages("caret")
library(mlbench)
install.packages("mlbench")
library(mlbench)
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
## The format of the results 
## The output is a set of integers for the rows of Sonar 
## that belong in the training set. 
str(inTrain)
training <- Sonar[inTrain,]
testing <- Sonar[-inTrain,]
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
library(caret)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
## The format of the results 
## The output is a set of integers for the rows of Sonar 
## that belong in the training set. 
str(inTrain)
training <- Sonar[inTrain,]
testing <- Sonar[-inTrain,]
nrow(testing)
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
install.packages("AzureML")
library(tm)
# Create corpus
docs <- Corpus(VectorSource(ds[1:10000,]$Preprocessed.tokenized_text))
install.packages("tm")
library(tm)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(ds[1:10000,]$Preprocessed.tokenized_text))
# Clean corpus
names(ds)
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
install.packages("e1071")
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
plot(plsFit)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(ds[1:10000,]$Preprocessed.tokenized_text))
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
install.packages("snowballC")
2
install.packages("SnowballC")
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.95)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, ds$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-train]
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(modeldata$equities == 1)
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
factor(modeldata$equities == 1)
modeldata$equity
docs
names(doccol)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
dim(mat.df)
colnames(mat.df)[ncol(mat.df)] <- "category"
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-train]
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
dim(mat.df)
dim(cl)
dim(training)
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
length(training)
length(testing)
cl <- mat.df[, "category"]
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
names(modeldata)
dim(modeldata)
factor(modeldata$equities == 1)
names(modeldata)
modeldata$equities <- factor(cl == 1)
model1 <- train(equities ~., data=training, method="svmLinear")
model1 <- train(equities ~., data = training, method="svmLinear")
model1 <- train(equities ~., data = modeldata[training,], method="svmLinear")
model1 <- train(equities ~., data = modeldata[training,], method="svmLinear")
plot(model1)
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
accuracy(pred)
table(true = modeldata[testing,]$equities, pred = pred)
model1
plot(pred)
confusionMatrix(pred, modeldata[testing,]$equities)
model1 <- train(equities ~., data = modeldata[training,], trControl = train_control, method="svmLinear")
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
model1 <- train(equities ~., data = modeldata[training,], trControl = train_control, method="svmLinear")
print(model1)
plot(model1)
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.95)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
library(kernlab)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
model1 <- train(equities ~., data = modeldata[training,], trControl = train_control, method="svmLinear")
model1
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
tm_map(reuters, content_transformer(tolower))
tm_map(docs, content_transformer(tolower))
tm_map(docs, content_transformer(gsub))
tm_map(docs, content_transformer(gsub("FOO", "BAR")))
gsub("FOO", "BAR", "FOOBAR")
tm_map(docs, content_transformer(gsub))
tm_map(docs, content_transformer(function(x, pattern) gsub(pattern, " ", x)))
tm_map(docs, content_transformer(function(x, pattern) gsub("foobar", " ", x)))
bar <- "I am just a poor boy"
substr(bar,1 , 25)
substr(bar,10 , 25)
chophead <- content_transformer(function(x, start, stop) substr(x, start, stop))
docs <- tm_map(docs, chophead, 10, 25)
inspect(docs[[1]])
inspect(docs[1])
inspect(docs[1:2])
inspect(docs[[2]])
lapply(ovid[1:2], as.character)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start, stop) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start, stop) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
bar
head(bar,10)
tail(bar, 10)
chophead <- content_transformer(function(x, start) substr(x, start, 25))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, 25))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, 100))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start) {substr(x, start, 100)} )
docs <- tm_map(docs, chophead, 10)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {substr(x, start, 100)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start) {stop <- length(x); substr(x, start, 100)} )
docs <- tm_map(docs, chophead, 10)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- length(x); substr(x, start, stop)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- length(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
bar[25:length(bar)]
bar
length(bar)
length(bar[1])
length(bar[[1]])
nchar(bar)
bar[1:5]
bar[[1]]
bar[[1]][1:5]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- tm_map(docs, toSpace, "\\( \\w+.+@+\\w+.* GMT \\)")
lapply(docs[1:10], as.character)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.+@+\\w+.* GMT \\)")
lapply(docs[1:10], as.character)
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
lapply(docs[1:10], as.character)
baz <- " ( kate . geenty @ wsj.com ) "
baz
gsub("\\( \\w+.+@+\\w+.* GMT \\)", baz)
gsub("\\( \\w+.+@+\\w+.* GMT \\)")
gsub("\\( \\w+.+@+\\w+.* GMT \\)", "BAR", baz)
gsub("\\( \\w+ \. +@+\\w+.* GMT \\)", "BAR", baz)
gsub("\\( \\w+ \\. +@+\\w+.* GMT \\)", "BAR", baz)
gsub("\\( \\w+ \\. +@+\\w+.* \\)", "BAR", baz)
gsub("\\( \\w+ \\. +@+\\w+.* ", "BAR", baz)
gsub("\\( \\w+ \\. ", "BAR", baz)
gsub("\\( \\w+ \\. \\w+", "BAR", baz)
gsub("\\( \\w+ \\. \\w+ @", "BAR", baz)
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baz)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+ \\. \\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+ \\. \\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
lapply(docs[1:10], as.character)
baf <- "( yvonne.lee @ wsj.com )"
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baz)
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baf)
baz
baf
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baf)
gsub("\\( \\w+ \\. \\w+ @.*", "BAR", baf)
gsub("\\( \\w+ \\. \\w+", "BAR", baf)
gsub("\\( \\w+ \\.", "BAR", baf)
gsub("\\( \\w+", "BAR", baf)
gsub("\\( \\w+ \\.", "BAR", baf)
bar
baz
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baf)
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baz)
gsub("\\( \\w+.*\\w+ @.* \\)", "BAR", baz)
gsub("\\( \\w+.*\\w+ @.* \\)", "BAR", baf)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
lapply(docs[1:10], as.character)
dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.95) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
lapply(docs[1:10], as.character)
# Clean corpus
#docs <- tm_map(docs, content_transformer(tolower))
#docs <- tm_map(docs, removeNumbers)
#docs <- tm_map(docs, removeWords, stopwords("english"))
#docs <- tm_map(docs, removePunctuation)
#docs <- tm_map(docs, stripWhitespace)
#docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.95)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
library(kernlab)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.95) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
library(kernlab)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) #docs <- tm_map(docs, chophead, 25) #docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") #docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") #docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") #docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") #docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
docs <- tm_map(docs, toSomth, "\\d+\\w", "NUMCHAR")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 25) docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR") #lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.95)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.91)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.2)
dtm.sparse
dtm.ng
removeSparseTerms(dtm.ng, 0.2)
removeSparseTerms(dtm.ng, 0.99)
removeSparseTerms(dtm.ng, 0.995)
dtm.spars<-removeSparseTerms(dtm.ng, 0.995)
dtm.spars
dtm.spars
dtm.sparse<-removeSparseTerms(dtm.ng, 0.995)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
colnames(mat.df)
mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
doccol <- ds[1:50000,] mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text)) library(NLP) BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE) library(tm) # Create corpus docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 25) docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR") #lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer)) dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
library("AzureML") ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net") ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV") names(ds)
library("AzureML") ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net") ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV") names(ds)
doccol <- ds[1:5000,]
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
#lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
library(caret)
library(mlbench)
# Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
doccol <- ds[1:10000,] #mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text)) library(NLP) BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE) library(tm) # Create corpus docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 25) docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR") #lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer)) dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
library(RWeka)
install.packages("RWeka")
library(RWeka)
Sys.getenv("R_ARCH")
Sys.getenv("R_ARCH")
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
#lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
inspect(dtm[1:10,1:10])
inspect(dtm.sparse[1:10,1:10])
inspect(dtm.sparse[1,1:50])
inspect(dtm.sparse[1,1:100])
inspect(dtm.sparse[1,])
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
#lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.98)
inspect(dtm.sparse[1,1:100])
inspect(dtm.sparse[1,])
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
lapply(docs[1:10], as.character)
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))lapply(docs[1:10], as.character)
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
lapply(docs[1:10], as.character)
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- tm_map(docs, chophead, 30)
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- tm_map(docs, chophead, 33)
lapply(docs[1:10], as.character)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
lapply(docs[1:10], as.character)
lapply(docs[1:100], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
lapply(docs[90], as.character)
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
lapply(docs[90], as.character)
inspect(dtm.sparse[1,])
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.98)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
inspect(dtm.sparse[1,])
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.996)
dtm.sparse
inspect(dtm.sparse[1,])
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm, 0.995)
inspect(dtm.sparse[1,])
# Create corpus docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 33) docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "More at.* GMT \\)") docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)") docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSpace, "Write to.* GMT \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
inspect(dtm.sparse[1,])
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
    method="svmLinear"))
model1 <- train(equities ~ ., data = modeldata[training,], trControl = train_control, method = "svmLinear")
)
)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
confusionMatrix(pred, modeldata[testing,]$equities, mode="prec_recall")
2313/(2313+61)
2313/(2313+48)
confusionMatrix(modeldata[testing,]$equities, pred,mode="prec_recall")
confusionMatrix(pred, modeldata[testing,]$equities, mode="prec_recall", positive="TRUE")
names(valds)
docs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, ds$equities)
# Change name of new column to "category"
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, ds$equities)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
dim(mat.df)
dim(doccol)
mat.df <- cbind(mat.df, doccol$equities)
mat.df <- cbind(mat.df, doccol$equities)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
inspect(dtm.sparse[1,])
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
mat.df <- cbind(mat.df, valds$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl$equities == 1)
head(cl)
modeldata$equities <- factor(cl == 1)
length(cl)
valpred <- predict(model1, modeldata, probability = FALSE)
length(modeldata)
length(mat.df)
dim(modeldata)
''
foo <- Terms(dt)
foo <- Terms(dtm.sparse)
foo
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
inspect(dtm.sparse[1,])
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
trainVB
trainVocav
trainVocab
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) mat.df <- cbind(mat.df, valds$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1)
length(modeldata)
length(cl)
dim(modeldata)
length(modeldata$equities)
modeldata$equities <- factor(cl == 1)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, method="svmLinear"))
PP <- c('center', 'scale')
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, method="svmLinear"))
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
names(doccol)
names(valdocs)
names(valds)
mat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
mat.df <- cbind(mat.df, valds$equities)
colnames(mat.df)[ncol(mat.df)] <- "category"
cl <- mat.df[, "category"]
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
pred <- predict(model1, modeldata, probability = FALSE)
length(pred)
confusionMatrix(pred, modeldata$equities, mode = "prec_recall", positive = "TRUE")
predict(model1, modeldata[testing,], probability = TRUE)
predict(model1, modeldata[testing,], probability = TRUE)$probabilities
pred <- predict(model1, modeldata[testing,], probability = TRUE)
pred$levels
trControl <- trainControl(classProbs = TRUE)
ctrl <- trainControl(classProbs = TRUE)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl, method="svmLinear"))
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
mat.df <- cbind(mat.df, doccol$equities)
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl, method="svmLinear"))
modeldata$equities
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl, method="svmLinear")
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP,  method="svmLinear")
str(training)
str(modeldata)
str(cl)
str(modeldata$equities)
modeldata$equities <- cl
str(modeldata$equities)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
library(caret) library(mlbench) library("AzureML") ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net") ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV") names(ds) valds <- download.datasets(dataset = ws,name = "val_peder_csv") # subset dataset for processing doccol <- ds[1:1000,]
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
dtm.ng
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.95)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- cl
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- mat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata$equities <- factor(cl == 1)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata$equities <- factor(cl == 1)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata[equities = 1]
modeldata[equities == 1]
modeldata[modeldata$equities == 1]
modeldata[modeldata$equities = 1]
modeldata[modeldata$equities == 1231]
modeldata$equities
modeldata[modeldata$equities == TRUE]
modeldata[modeldata$equities == TRUE,]
modeldata[modeldata$equities == FALSE,]
dim(modeldata[modeldata$equities == FALSE,])
dim(modeldata[modeldata$equities == TRUE,])
modeldata[modeldata$equities == TRUE,]$equities
modeldata[modeldata$equities == TRUE,]$equities <- "P"
modeldata[modeldata$equities == TRUE,]$equities <- P
modeldata[modeldata$equities == TRUE,]$equities <- c("P")
modeldata$equities <- cl
modeldata[modeldata$equities == TRUE,]$equities
modeldata[modeldata$equities == TRUE,]$equities <- c("P")
modeldata[modeldata$equities == FALSE,]$equities <- c("F")
modeldata[modeldata$equities == FALSE,]$equities 
modeldata[modeldata$equities == TRUE,]$equities 
modeldata[modeldata$equities == 1,]$equities 
modeldata[modeldata$equities == 0,]$equities 
modeldata$equities <- cl
modeldata$equities 
modeldata[modeldata$equities == 0,]$equities 
modeldata[modeldata$equities == 1,]$equities 
modeldata[modeldata$equities == 0,]$equities <- c("F")
modeldata[!modeldata$equities == 0,]$equities <- c("P")
modeldata[modeldata$equities == 0,]$equities 
modeldata$equities 
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities 
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities 
modeldata[modeldata$equities == 1,]$equities  <- "JA"
modeldata$equities 
str(modeldata$equities)
typeof(modeldata$equities)
as.factor(modeldata$equities)
modeldata$equities <- as.factor(modeldata$equities)
modeldata$equities 
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
table(true = modeldata[testing,]$equities, pred = pred)
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
pred <- predict(model1, valmodeldata, probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
table(true = valmodeldata$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
length(valpred)
length(valmodeldata)
length(valmat.df)
valmat.df
dim(valmat.df)
valmodeldata[modeldata$equities == 0,]$equities <- "NEI"modeldata[modeldata$equities == 1,]$equities  <- "JA"
valmodeldata[modeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
as.factor(valmodeldata$equities)
valmodeldata$equities <- cl
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[modeldata$equities == 0,]$equities <- "NEI"
valmodeldata[modeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "TRUE")
pred
predict(model1, valmodeldata, probability = TRUE)
predict(model1, valmodeldata, probability = TRUE)
head(attr(pred, "probabilities"))
ctrl <- trainControl(classProbs = TRUE, probability = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
attr(pred)
pred
str(pred)
attr(pred)
attr(pred, "Probabilities")
attr(pred, "probabilities")
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.98)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE, probability = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(valmodeldata$equities)
modeldata$equities <- as.factor(modeldata$equities)
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
head(attr(pred, "probabilities"))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
doccol <- ds[1:5000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear"))
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
#mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text))
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
ctrl <- trainControl(method="None")
valmodeldata[1:10]
valmodeldata[1,]
dim(valmodeldata[1,])
dim(valmodeldata[1,])
dim(valmodeldata[1,])
typeof(valmodeldata[1,])
typeof(valmodeldata)
valmat.df[,]
valmat.df[1,]
valmat.df[1,]
typeof(valmat.df[1,])
typeof(valmat.df)
typeof(valdtm)
typeof(valdtm)
valdtm
valdtm[1]
valdtm[1,]
inspect(valdtm)
inspect(valdtm)[1,]
valmat.df[1,]
names(valmat.df[1,])
valmat.df[1,]
valmat.df[1,!= 0]
colSums(valmat.df[1,])
rowSums(valmat.df[1,])
rowSums(valmat.df[2,])
valdtm
rowSums(valmat.df)
valmat.df[1,]
valmat.df[2,]
row(valmat.df)
row(valmat.df[1,])[which(!X == 0)]
row(valmat.df[1,])[which(!valmat.df == 0)]
row(valmat.df[1,])[which(!valmat.df[1,] == 0)]
col(valmat.df[1,])[which(!valmat.df[1,] == 0)]
valmat.df[1,]
valmat.df[1,516]
names(valmat.df)
col(valmat.df[1,])[which(!valmat.df[1,] == 0)]
col(valmat.df[1:10,])[which(!valmat.df[1:10,] == 0)]
col(valmat.df[1:100,])[which(!valmat.df[1:100,] == 0)]
rowSums(valmat.df)
col(valmat.df[281,])[which(!valmat.df[281,] == 0)]
col(valmat.df[282,])[which(!valmat.df[282,] == 0)]
col(valmat.df[2,])[which(!valmat.df[2,] == 0)]
rowSums(valmat.df)
valdocs
inspelibrary(qdap)valdocs
library(qdap)
install.packages("qdap")
library(qdap)
    with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
valdocs
valdocs.df
valmat.df
valmat.df[1,]
valmat.df[2,]
mat.df[2,]
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
dim(modeldata)
system.time(model1 <- train(equities ~ .,data = modeldata[training,], trControl = ctrl,  method="svmLinear2"))
head(modeldata[training,])
ctrl <- trainControl(method="none")
system.time(model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear2"))
model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear2")
model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear")
model1 <- train(equities ~ ., data = modeldata[1:100,],  method = "svmLinear")
warnings()
typeof(modeldata$equities)
modeldata$equities = as.factor(modeldata$equities)
typeof(modeldata$equities)
levels(modeldata$equities)
model1 <- train(equities ~ ., data = modeldata[1:100,],  method = "svmLinear")
warnings/()
warnings()
model1 <- train(equities ~ ., data = modeldata[1:100,],  method = "svmLinear2")
warnings()
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
#mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text))
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(method="none")
system.time(model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[1:100,], method = "svmLinear2"))
warnings()
model1
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainContrl(method="none"), method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneLength=0))
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneLength=0)
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear", tuneLength=0)
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
pred
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear", tuneLength=0)
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear", tuneLength=2)
model1
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneLength=2)
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneGrid = NULL)
model1 <- train(equities ~ ., data = modeldata[training,] method = "svmLinear2")
model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2")
model1 <- svmLinear(equities ~ ., data = modeldata[training,], method = "svmLinear2")
svm
model1 <- svm(equities ~ ., data = modeldata[training,])
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model1 <- svm(equities ~ ., data = modeldata[training,], kernel="linear")
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2")
model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear")
model1 <- train(equities ~ ., data = modeldata[training,], trainControl=ctrl, method = "svmLinear")
model1 <- train(equities ~ ., data = modeldata[training,], trControl = trainControl(method = "none"), method = "svmLinear", tuneLength = 0)
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
pred
table(true = modeldata[testing,]$equities, pred = pred)
names(valmodeldata)
names(valmodeldata)
names(modeldata)
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
library(magrittr)
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
valdocs
valmat.df
valmat.df[1,]
valmodeldata$equities
valdocs
valdocs[[1]]
valdocs[[1]]$content
valdocs[[1]]$content
strWrap(valdocs[[1]]$content)
library(qdap)
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
model1 <- train(equities ~ ., data = modeldata[training,], trControl = trainControl(method = "none"), method = "svmLinear", tuneLength = 3)
model1
model1 <- train(equities ~ ., data = modeldata[training,],  method = "svmLinear", tuneLength = 3)
model1
model1$finalModel
pred <- predict(model1, modeldata[testing,], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
pred <- predict(model1$finalModel, modeldata[testing,], probability = TRUE)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")pred <- predict(model1$finalModel, modeldata[testing,], probability = TRUE)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
pred <- predict(model1, modeldata[testing,], probability = TRUE)
pred
model_none <- train(equities ~ ., data = modeldata[training,], trControl = trainControl(method = "none"), method = "svmLinear", tuneLength = 0)
pred <- predict(model_none, modeldata[testing,], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
pred <- predict(model1, modeldata[testing,], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model1
model_none
model1
model1$fin
model1$finalModel
predict(model1$finalModel, modeldata[testing,])
system.time(model1 <- train(equities ~ ., data = modeldata[training,],  method = "svmLinear2", tuneLength = 3))
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "JA")
head(attr(pred, "probabilities"))
valpred <- predict(model1, valmodeldata, type="pred")
valpred <- predict(model1, valmodeldata, type="prob")
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trainControl(repeats = 2),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trainControl(repeats = 2, method = "none"),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trainControl(method = "none"),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trControl = trainControl(method = "none", repeats = 2),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trControl = trainControl(repeats = 2),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2"))
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
}
}
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
}
       with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
    as.data.frame(valdocs)[1,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo <- function(doc)     as.data.frame(valdocs)[1,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo(1)
displayDoc <- function(doc)     as.data.frame(doc %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") }))) )
foo <- function(doc)     as.data.frame(doc)[1,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo <- function(docs, num)     as.data.frame(docs)[num,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo(valdocs, 1)
foo(docs, 1)
foo(docs, 2)
foo(docs, 3)
foo(docs, 4)
stopwords('english')
c(stopwords('english'), "gmt")
type(valmat.df)
typeof(valmat.df)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
typeof(valmat.df)
names(valmat.df)
names(modeldata)
names(valmat.df)[1:10]
foo <- names(valmat.df)[1:10]
modeldata[1:2,foo]
modeldata[1:2,-foo]
modeldata[1:2,!foo]
colnames(valmat.df)[1:10]
modeldata[1:2,!colnames(valmat.df)[1:10]]
modeldata[1:2,colnames(valmat.df)[1:10]]
modeldata[1:2,-colnames(valmat.df)[1:10]]
modeldata[-colnames(valmat.df)[1:10]]
modeldata[foo]
head(modeldata[foo])
head(modeldata[!foo])
head(modeldata[-foo])
head(modeldata[c("numchar")])
head(modeldata[,c("numchar")])
head(modeldata[,numchar])
names(modeldata)
head(modeldata[,"year numchar"])
head(modeldata[,!"year numchar"])
head(modeldata[,c("year numchar")])
head(modeldata[,!c("year numchar")])
head(modeldata[,-c("year numchar")])
head(modeldata[,colnames(valdata.df)])
head(modeldata[,colnames(valmat.df)])
head(modeldata[,colnames(valmat.df)[1:10]])
head(modeldata[,!colnames(valmat.df)[1:10]])
head(modeldata[,-colnames(valmat.df)[1:10]])
colnames(valmat.df)[1:10]]
colnames(valmat.df)[1:10]
colnames(valmat.df)[1:5]] -  colnames(valmat.df)[1:5]]
colnames(valmat.df)[1:10]] -  colnames(valmat.df)[1:5]]
head(modeldata[,!=colnames(valmat.df)[1:10]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:10]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:10]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:200]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:300]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:500]])
require(spacyr)
install.packages(spacyr)
install.packages(openNLP)
install.packages("spacyr")
install.packages("openNLP")
library(openNLP)
mystr <- "this is very nice"
tagPOS(mystr)
Maxent_POS_Tag_Annotator(mystr)
install.packages(openNLPdata)
install.packages("openNLPdata")
install.packages("openNLPmodels.en")
install.packages("openNLPmodels")
foo(docs, 1)
displayData <- function(docs, num)     as.data.frame(docs)[num,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
displayData(docs, 1)
library(magrittr)
displayData(docs, 1)
displayData(valddocs, 1)
displayData(valdocs, 1)
docs[1,]''
docs
docs$content
docs$content[1,]
docs$content[[1]]
docs$content[[1,]]
docs$content[[1]]
str(docs$content[[1]])
docs$content[[1]]$content
docs$content[[2]]$content
strWrap(docs$content[[2]]$content)
library(qdap)
strWrap(docs$content[[2]]$content)
valdtm
str(valdtm)
m <- as.matrix(valdtm)
v <- sort(rowSums(m), decreasing = TRUE)
head(v, N)
head(v, 10)
head(v, 100)
library(tm)
findFreqTerms(valdocs, 20)
findFreqTerms(valdtm, 20)
findFreqTerms(dfm.sparse, 20)
findFreqTerms(dtm.sparse, 20)
findFreqTerms(dtm.sparse, 1)
findFreqTerms(dtm, 1)
m <- as.matrix(dtm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
N <- 10
head(v, N)
head(m)
head(v)
v
dim(v)
length(v)
length(m)
dim(m)
m[v]
head(m[v])
v
head(v, N)
colnames(head(v, N))
col(head(v, N))
head(v, N)
head(v, N)
names(head(v, N))
m[names(head(v, N))]
m[,names(head(v, N))]
head(v, N)
m[1:10,1398]
m
m <- as.matrix(dfm.sparse)
N <- 10
m <- as.matrix(dfm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
Reset
N <- 10
m <- as.matrix(dfm.sparse)
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
#mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text))
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, c(stopwords("english"), "gmt"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
dtm.sparse
N <- 10
m <- as.matrix(dfm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
N <- 10
m <- as.matrix(dtm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
head(v, N)
m
head(v, N)
m[1:10,4774]
dim(m)
m[1:10,1:10]
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
N <- 10
m <- as.matrix(valdtm)
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
N <- 10
m <- as.matrix(dtm.sparse)
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
svm
library(e1071)
svm
?svm
model2 <- svm(equities ~ ., data=training, kernel = "linear")
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)model2 <- svm(equities ~ ., data=training, kernel = "linear")
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear")
model2
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear", cost=1)
model2
levels(modeldata$equities)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear", cost=1, probability = TRUE)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear", cost=10, probability = TRUE)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
pred
pred[1:10,]
pred[1,]
pred[1]
length(pred)
pred[3999]
attr(pred, "probabilities")
attr(pred, "probabilities")[3999]
doccol[3999]
doccol[3999,]
table(true = modeldata[testing,]$equities, pred = pred)
valpred <- predict(model2, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model2, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "JA")
trainVocab
model2$coefs
length(trainVocab)
model2
str(model2)
str(model2)
trainVocab
)
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
head(ds)
names(ds)
dim(ds)
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, c(stopwords("english"), "gmt"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
ctrl <- trainControl(method="none")
model2 <- svm(equities ~., data= modeldata[training,], kernel = "linear", cost=5)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data = modeldata[training,], kernel = "linear", cost = 1)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data = modeldata[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
valpred <- predict(model2, valmodeldata, probability = TRUE)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model2, valmodeldata, probability = TRUE)
valmodeldata
dim(valmodeldata)
predict(model2, valmodeldata, probability = TRUE)
dim(modeldata)
trainVocab <- Terms(dtm.sparse)
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
dim(valmodeldata)
valpred <- predict(model2, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "JA")
trainVocab
}
filter_corp <- function(corpus) {     docs <- tm_map(docs, chophead, 33)     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33)     return (docs) }
filter_corp(docs)
library(magrittr)
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%      tm_map(toSpace, "Subscribe to WSJ.* GMT \\)")     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "") %>%     tm_map(docs, content_transformer(tolower)) %>%     tm_map(docs, removeNumbers) %>%     tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(docs, removePunctuation) %>%     tm_map(docs, stripWhitespace) %>%     tm_map(docs, stemDocument, language = "english")     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "") %>%     tm_map(docs, content_transformer(tolower)) %>%     tm_map(docs, removeNumbers) %>%     tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(docs, removePunctuation) %>%     tm_map(docs, stripWhitespace) %>%     tm_map(docs, stemDocument, language = "english")     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "") %>%     tm_map(docs, content_transformer(tolower)) %>%     tm_map(docs, removeNumbers) %>%     #tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     #tm_map(docs, removePunctuation) %>%     #tm_map(docs, stripWhitespace) %>%     #tm_map(docs, stemDocument, language = "english")     return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "")      #tm_map(docs, content_transformer(tolower)) %>%     #tm_map(docs, removeNumbers) %>%     #tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     #tm_map(docs, removePunctuation) %>%     #tm_map(docs, stripWhitespace) %>%     #tm_map(docs, stemDocument, language = "english")     return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "")      return (docs) } filter_corp(docs)
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)")      return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)")      return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "")      return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (docs) } filter_corp(docs)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(mat.df, doccol$equities)
names(nmat)
names(nmat.df)
colnames(mat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
colnames(nmat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(mat.df, doccol$equities)
colnames(mat.df)[ncol(mat.df)] <- "equities"
namescolnames(nmat.df)[ncol(mat.df)] <- "equities"
colnames(nmat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(nmat.df, doccol$equities)
colnames(nmat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
names(modeldata)
names(modeldata) == names(nmat.df)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     return (nmat.df) }
prepare_for_classifier(dtm.sparse, doccol)
foo <- prepare_for_classifier(dtm.sparse, doccol)
dim(foo)
dim(modeldata)
model2 <- svm(equities ~ ., data = foo[training,], kernel = "linear", cost = 1, probability = TRUE)
levels(foo$equities)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, doccol)
levels(foo$equities)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NEI"     nmat.df[nmat.df$equities == 1,]$equities <- "JA"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NEI"     nmat.df[nmat.df$equities == 1,]$equities <- "JA"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, doccol)
levels(foo$equities)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, doccol)
levels(foo$equities)
model2 <- svm(equities ~ ., data = foo[training,], kernel = "linear", cost = 1, probability = TRUE)
asdasdasdasdasdasdasdsdafasdfasdfasdf
library(caret)
library(mlbench)
library(magrittr)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
doccol <- ds[1:10000,]
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpora objects
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (docs) } docs <- filter_corp(docs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm <- DocumentTermMatrix(docs)
docs <- filter_corp(docs)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
doccol <- ds[1:10000,]
head(ds)
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
valdocs <- filter_corp(valdocs)
valdocs <- filter_corp(valdocs)
filter_corp <- function(corpus) {     corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (docs) }
valdocs <- filter_corp(valdocs)
valdocs
dtm <- DocumentTermMatrix(valdocs)
dtm
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
N <- 10
m <- as.matrix(dtm.sparse)
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
foo <- prepare_for_classifier(dtm.sparse, valdocs)
foo <- prepare_for_classifier(dtm.sparse, valdocs)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, valdocs)
valdocs
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, valdocs)
nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
nmat.df <- cbind(nmat.df, doccol$equities)
nmat.df <- cbind(nmat.df, valdocs$equities)
valdocs$equities
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, valds)
foo
model2 <- svm(equities ~ ., data = foo, kernel = "linear", cost = 1, probability = TRUE)
library(e1071)
model2 <- svm(equities ~ ., data = foo, kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, foo, probability = TRUE)
pred
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, foo$equities, mode = "prec_recall", positive = "YES")
training <- sample(nrow(val_set_for_classifier), ceiling(nrow(val_set_for_classifier) * .50))
val_set_for_classifier <- prepare_for_classifier(dtm.sparse, valds)
training <- sample(nrow(val_set_for_classifier), ceiling(nrow(val_set_for_classifier) * .50))
testing <- (1:nrow(val_set_for_classifier))[-training]
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "JA")
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "JA")
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
doccol <- ds[1:10000,]
ds <- download.datasets(dataset = ws, name = "10k_EN_R_script")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
ds <- download.datasets(dataset = ws, name = "10k_EN_R_script")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- filter_corp(docs)
valdocs <- filter_corp(valdocs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
trainVocab
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm, valds)
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "JA")
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
dtm <- DocumentTermMatrix(valdocs)
valdtm_create <- dtm
dtm
filter_corp <- function(corpus) {     corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (corpus) }
valdocs <- filter_corp(valdocs)
valddtm_create <- DocumentTermMatrix(valdocs)
valdtm_createVocab <- Terms(valdtm_create)
valddtm_create
valdocs
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
valdocs
valdocs <- filter_corp(valdocs)
valdocs
valddtm_create <- DocumentTermMatrix(valdocs)
valdtm_createVocab <- Terms(valdtm_create)
valddtm_create
valdtm_create <- DocumentTermMatrix(valdocs)
valdtm_createVocab <- Terms(valdtm_create)
valddtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valddtm.sparse
train_dtm <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm
valdtm_createVocab <- Terms(valdtm.sparse)
valddtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valdtm_createVocab <- Terms(valdtm.sparse)
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
valddtm.sparse
valdtm_createVocab <- Terms(valdtm.sparse)
valddtm.sparsevaldtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valdtm_createVocab <- Terms(valdtm.sparse)
length(valdtm_createVocab)
valdtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valdtm.sparse
valdtm_createVocab <- Terms(valdtm.sparse)
length(valdtm_createVocab)
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
val_set_for_classifier <- prepare_for_classifier(valdtm_create, valds)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
dim(val_set_for_classifier)
dim(train_set_for_classifier)
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
dim(train_set_for_classifier)
dim(val_set_for_classifier)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
valdtm.sparse <- removeSparseTerms(valdtm_create, 0.98)
valdtm_createVocab <- Terms(valdtm.sparse)
valdtm.sparse
valdtm.sparse <- removeSparseTerms(valdtm_create, 0.95)
valdtm.sparse
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
names(train_set_for_classifier)
dim(train_set_for_classifier)
dim(val_set_for_classifier)
model2
dim(val_set_for_classifier)
dim(train_set_for_classifier)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
dim(train_set_for_classifier)
train_dtm_from_val
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
length(valdtm_createVocab)
valdtm.sparse
valdtm_createVocab <- Terms(valdtm.sparse)
length(valdtm_createVocab)
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
model2
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
valds
docs
library(caret)
library(mlbench)
library(magrittr)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.datasets(dataset = ws, name = "10k_EN_R_script")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
library(tm)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
filter_corp <- function(corpus) {     corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (corpus) }
docs <- filter_corp(docs)
valdocs <- filter_corp(valdocs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
val_set_for_classifier <- prepare_for_classifier(valdtm, valds)
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
dim(training)
dim(train_set_for_classifier)
dim(val_set_for_classifier)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
library(e1071)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
table(true = val_set_for_classifier$equities, pred = valpred)
foo <- function(bar) { levels(docs$bar) }
foo("equities")
levels(docs$equities)
levels(dtm.sparse$equities)
length(dtm.sparse$equities)
length(doccol$equities)
foo <- function(bar) { length(docs$bar) }
foo("equities")
foo <- function(bar) { length(doccol$bar) }
foo("equities")
length(doccol$equities)
foo(equities)
length(doccol$equities)
names(doccol)
colnames(doccol)
names(doccol)
foo(c("equities"))
source("~/visual studio 2017/Projects/rproject1/rproject1/script.R", echo = TRUE, encoding = "Windows-1252")
prepare_for_classifier <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$commentary)     colnames(nmat.df)[ncol(nmat.df)] <- c("commentary")     nmat.df[nmat.df$commentary == 0,]$commentary <- "NO"     nmat.df[nmat.df$commentary == 1,]$commentary <- "YES"     nmat.df$equities <- as.factor(nmat.df$commentary)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
dtm.sparse
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$commentary)     colnames(nmat.df)[ncol(nmat.df)] <- c("commentary")     nmat.df[nmat.df$commentary == 0,]$commentary <- "NO"     nmat.df[nmat.df$commentary == 1,]$commentary <- "YES"     nmat.df$equities <- as.factor(nmat.df$commentary)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE     )
nmat.df <- cbind(nmat.df, docs$commentary)
nmat.df <- cbind(nmat.df, doccol$commentary)
names(doccol)
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
ctrl <- trainControl(method="cv")
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2"))
ctrl <- trainControl(method="cv", allowParallel = TRUE)
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2"))
model1
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
grid <- expand.grid(sigma = c(.01, .015, 0.2),                     C = c(0.75, 0.9, 1, 1.1, 1.25) )
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
grid <- expand.grid(sigma = c(.01, .015, 0.2),                     cost = c(0.75, 0.9, 1, 1.1, 1.25) )
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
grid <- expand.grid(cost = c(0.75, 0.9, 1, 1.1, 1.25))
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
allds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
allds <- ds
names(allds)
foofunc <- function(col) { typeof(docs$col) }
foofuc("equities")
foofunc("equities")
typeof(docs$equities)
typeof(ds$equities)
length(ds$equities)
foofunc <- function(col) { length(ds$col) }
foofunc("equities")
foofunc <- function(col) { length(ds$eval(col)) }
foofunc("equities")
length(ds$equities)
names(ds)
colnames(ds)
foofunc <- function(col) { length(ds[[col]]) }
foofunc("equities")
train_set_for_classifier
dim(train_set_for_classifier)
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs[[tag]])     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
train_set_for_classifier_new <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
dim(train_set_for_classifier)
dim(train_set_for_classifier_new)
model2 <- svm(equities ~ ., data = train_set_for_classifier_new[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model1, train_set_for_classifier_new[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm, valds, "equities")
names(valds)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
grid <- expand.grid(cost = c(1, 1.5, 2))
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
dim(train_set_for_classifier)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
docs <- filter_corp(docs)
valdocs <- filter_corp(valdocs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
doccol <- allds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- filter_corp(docs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm_from_train_dtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
dim(training)
length(training)
dim(allds)
doccol
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
doccol <- allds[1:20000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- filter_corp(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm_from_train_dtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
dtm.sparse
dtm
docs
dtm <- DocumentTermMatrix(docs)
dtm.sparse
dtm
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm_from_train_dtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
ctrl <- trainControl(method="cv", allowParallel = TRUE)
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model1, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
model1
ctrl <- trainControl(method="cv", allowParallel = TRUE, repeats = 2)
model1
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "outlook")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "outlook")
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
dim(train_set_for_classifier)
names(train_set_for_classifier)
dim(train_set_for_classifier)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
dim(train_set_for_classifier)
names(train_set_for_classifier)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
names(train_set_for_classifier)
train_set_for_classifier$equities
names(train_set_for_classifier)
names(train_set_for_classifier$equities)
train_set_for_classifier$equities
names(train_set_for_classifier$equities)
colnames(train_set_for_classifier$equities)
colnames(train_set_for_classifier)
training_tag <- "outlook"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
model2 <- svm(training_tag ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
training_tag <- "equities"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
model2 <- svm(training_tag ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
names(train_set_for_classifier)
length(names(train_set_for_classifier))
length(colnames(train_set_for_classifier))
colnames(train_set_for_classifier)
svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
ctrl <- trainControl(method="cv", allowParallel = TRUE, repeats = 2)
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
training_tag <- "outlook"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier$outlook
doccol$outlook
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
train_set_for_classifier$outlook
doccol$outlook
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
train_set_for_classifier$outlook
doccol$outlook
train_set_for_classifier$outlook
train_set_for_classifier$outlookprepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier$outlook
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
training_tag <- "outlook"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier$outlook
dtm.sparse
nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(nmat.df, docs$outlook)
nmat.df <- cbind(nmat.df, doccol$outlook)
names(nmat.df)
colnames(nmat.df)[ncol(nmat.df)] <- "outlook"
names(nmat.df)
nmat.df$outlook
doccol$outlook
nmat.df <- cbind(nmat.df, docs$outlook)
nmat.df <- cbind(nmat.df, doccol$outlook)
names(nmat.df)
nmat.df$doccol$outlook
nmat.df[,746]
colnames(nmat.df)[ncol(nmat.df)] <- "outlook"
nmat.df[,746]
nmat.df$outlook
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
training_tag <- "outlook_tag"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
model1
plot(model1)
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
names(train_set_for_classifier)
table(true = train_set_for_classifier[testing,]$outlook_tag, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$outlook_tag, mode = "prec_recall", positive = "YES")
levels(train_set_for_classifie$outlook_tag)
levels(train_set_for_classifier$outlook_tag)
names(train_set_for_classifier$outlook_tag)
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
training_tag <- "outlook_tag"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
names(train_set_for_classifier)
model1
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
levels$outlook_tag
levels(train_set_for_classifier$outlook_tag)
levels(train_set_for_classifier$outlook_taprepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }g)
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
training_tag <- "outlook_tag"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
names(train_set_for_classifier)
levels(train_set_for_classifier$outlook_tag)
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook_tag <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook_tag <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
levels(train_set_for_classifier$outlook_tag)
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook_tag == 0,]$outlook_tag <- "NO"     nmat.df[nmat.df$outlook_tag == 1,]$outlook_tag <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
levels(train_set_for_classifier$outlook_tag)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
levels(train_set_for_classifier$outlook_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
dim(train_set_for_classifier[testing,])
length(pred)
table(true = train_set_for_classifier[testing,]$outlook_tag, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
confusionMatrix(pred, train_set_for_classifier[testing,]$outlook_tag, mode = "prec_recall", positive = "YES")
valpred <- predict(model1, val_set_for_classifier, probability = TRUE)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds)
valpred <- predict(model1, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$outlook_tag, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$outlook_tag, mode = "prec_recall", positive = "YES")
table(true = train_set_for_classifier[testing,]$outlook_tag, pred = pred)
table(true = val_set_for_classifier$outlook_tag, pred = valpred)
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
val_set_for_classifier2 <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "outlook_tag")
val_set_for_classifier == val_set_for_classifier2
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds)
valpred <- predict(model1, val_set_for_classifier2, probability = TRUE)
predict(model1, val_set_for_classifier, probability = TRUE) == predict(model1, val_set_for_classifier2, probability = TRUE)
names(ds)
training_tag <- "FXFI"
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs[[tag]])     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
model2 <- svm(training_tag ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
model2 <- svm(string(training_tag) ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
model2 <- svm(FXFI ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "7ea3268f-ebe0-4ce9-b942-029d2811d2f5-58393",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.03f3f2e2f38e4777b1fed27d00b87aae",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) head(ds)
colSums(ds)
dim(ds)
valds
displayData <- function(docs, num)     as.data.frame(docs)[num,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
displayData(valdocs, 1)
valdocs[1,]
valds[1,]
displayData(valds, 1)
library(qdap)
displayData(valds, 1)
displayData(valds, 2)
valds[1]$content
valdocs$content
valdocs$content[[1]]$content
strWrap(valdocs$content[[1]]$content)
strWrap(valdocs$content[[1]]$content)
head(ds)
head(ds, 50)
strWrap(valdocs$content[[43]]$content)
strWrap(valdocs$content[[46]]$content)
head(ds, 50)
ds
strWrap(valdocs$content[[296]]$content)
ds
names(ds)
ds[Scored.Labels == 1]
ds[ds$Scored.Labels == 1]
ds$Scored.Labels
ds[ds$Scored.Labels == 1]
ds[ds$Scored.Labels == 1,]
ds[ds$Scored.Labels == 1,1]
ds[ds$Scored.Labels == 1,]
strWrap(valdocs$content[[294]]$content)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
library(mldr)
install.packages(mldr)
install.packages("mldr")
names(ds)
length(names(ds))
c(seq(1:4))
c(seq(1:4), seq(5:6))
names(ds)
names(ds)[10]
names(ds)[15]
ds[1:10, c(15, seq(1:14))]
ds[1:10, c(15, seq(1:14), seq(16:17))]
c(15, seq(1:14), seq(16:17))
c(15, seq(1:14), 16, 17)
ds[1:10, c(15, seq(1:14), 16,17)]
mydf <- ds[, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(my_chosen_data, c(seq(2:17)))
library(mldr)
mymldr <- mldr_from_dataframe(my_chosen_data, c(seq(2:17)))
mymldr <- mldr_from_dataframe(mydf, c(seq(2:17)))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) library(mldr)
mydf <- ds[1:10, c(15, seq(1:14), 16, 17)]
names(mydf)
mydf[2:17]
mymldr <- mldr_from_dataframe(mydf, c(seq(2:17)))
plot(mymldr)
mymldr
mymldr <- mldr_from_dataframe(mydf, c(seq(3:17)))
mymldr
mymldr <- mldr_from_dataframe(mydf, c(seq(4:17)))
mymldr
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(seq(4:17)))
mymldr
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(4:17))
mymldr
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(4,5))
mymldr
mldr_from_dataframe(mydf, labelIndices = c(4,5))
mldr_from_dataframe(mydf, labelIndices = c(4,5), name = "FOO!")
summary(mldr_from_dataframe(mydf, labelIndices = c(4,5), name = "FOO!"))
summary(mldr_from_dataframe(mydf, labelIndices = c(2:17), name = "FOO!"))
mydf <- ds[1:1000, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
plot(mymldr, type = "LSB")
plot(mymldr, type = "AT")
plot(mymldr, type = "LSH")
my_predictions <- mydf[, seq(2, length(names(mydf)), 2)]
mydf <- ds[1:10000, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
mydf <- ds[1:100000, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
plot(mymldr, type = "LSB")
plot(mymldr, type = "AT")
mydf <- ds[, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
plot(mymldr, type = "LSB")
plot(mymldr, type = "AT")
plot(mymldr, type = "LSH")
resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
my_predictions <- mydf[, seq(2, length(names(resds)), 2)]
names(resds)
seq(2, length(names(resds)), 2)
length(names(resds))
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
res <- mldr_evaluate(mymldr, my_predictions)
names(mydf)
head(mydf)
head(my_predictions)
typeof(my_predictions)
typeof(my_predictions$Scored.Labels)
as.matrix(my_predictions)
asm<-as.matrix(my_predictions)
res <- mldr_evaluate(mymldr, asm)
trueLabels <- mldr$dataset[, mldr$labels$index]
trueLabels <- mymldr$dataset[, mymldr$labels$index]
trueLabels
names(trueLabels)
length(trueLabels)
length(my_predictions)
names(my_predictions)
names(resds)
ds[1:10, order(names(ds))]
mydf[1:10, order(names(mydf))]
mydf[1:10, order(names(mydf))]
ds[1:10, order(names(ds))]
names(ds)
names(ds$newsid)
ds[1:10, -c(15)]
ds[1:10, -c("newsid")]
names(ds)
ds[1:10, c("newsid")]
ds[1:10, !c("newsid")]
ds[1:10, !=c("newsid")]
library(dplyr)
ds[1:10,] %>% select(newsid)
ds[1:10, -which(names(ds) == "newsid")]
order(ds[1:10, -which(names(ds) == "newsid")])
ds[1:10, -which(names(ds) == "newsid")]
ds[1:10, order(names(ds))]
order(names(ds[1:10, -which(names(ds) == "newsid")]))
order(names(ds[1:10, -which(names(ds) == "newsid")]))
ds[1:10,order(names(ds[1:10, -which(names(ds) == "newsid")]))]
length(order(names(ds[1:10, -which(names(ds) == "newsid")])))
order(names(ds[1:10, -which(names(ds) == "newsid")]))
lapply(order(names(ds[1:10, -which(names(ds) == "newsid")])), +1)
order(names(ds[1:10, - which(names(ds) == "newsid")]))
order(names(ds[1:10, - which(names(ds) == "newsid")])) + 1
order(names(ds[1:10, - which(names(ds) == "newsid")])) + 1
order(names(ds[1:10, - which(names(ds) == "newsid")]))
order(names(ds[1:10,]) )
order(names(ds[1:10,]))
ds[1:10,order(names(ds[1:10,]))]
mydf[1:10, order(names(ds[1:10,]))]
mydf <- ds[1:10, order(names(ds[1:10,]))]
which(colnames(ds) == c("newsid"))
goo <- which(colnames(ds) == c("newsid"))
mydf[,seq(1,goo)]
mydf
goo <- which(colnames(ds) == c("newsid"))
goo <- which(colnames(mydf) == c("newsid"))
mydf[,seq(1,goo)]
mydf[,seq(1,goo-1)]
seq(1,15)
seq(1,15)+seq(16:17)
seq(1:15)+seq(16:17)
c(seq(1:15),seq(16:17))
c(seq(1:15),seq(16:17)+15)
c(seq(1:goo-1),seq(16:17)+15)
goo
c(seq(1:goo-1),seq(goo:17)+15)
c(seq(1:goo-1),seq(goo:length(names(ds)))+15)
c(seq(1:goo-1),seq(goo:length(names(ds)))+goo)
c(seq(1:goo-1),seq(goo:length(names(mydf)))+goo)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
order(names(ds[1:10,]))
order(names(ds[1:10, ]))
mydf <- ds[1:10, order(names(ds[1:10,]))]
mydf
which(colnames(mydf) == c("newsid"))
newsid_col <- which(colnames(mydf) == c("newsid"))
c(seq(1:newsid_col))
c(seq(1:newsid_col-1), seq(newsid_col,length(colnames(mydf))) )
c(seq(1:newsid_col-1), seq(newsid_col,length(colnames(mydf))))
c(seq(1:newsid_col-1), seq(newsid_col+1,length(colnames(mydf))))
c(seq(1:newsid_col-1)))
c(seq(1:newsid_col-1))
seq(newsid_col + 1, length(colnames(mydf)) )
seq(newsid_col + 1, length(colnames(mydf)))
colnames(mydf)
newsid_col
c(seq(1:newsid_col-1)))
c(seq(1:newsid_col-1))
c(seq(1:newsid_col))
c(seq(1:(newsid_col-1)))
c(seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))) )
c(seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
mydf <- ds[1:10, order(names(ds[1:10,]))]
mydf
mydf <- ds[1:10000, order(names(ds[1:10,]))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
library(mldr)
mydf <- ds[1:10000, order(names(ds[1:10,]))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
labels(mymldr)
mymldr$labels
summary(mymldr)
c(seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
c(newsid_co, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
head(mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))])
mydf<- ds[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))])
mydf<- ds[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf
mydf <- ds[1:10000, order(names(ds[1:10,]))]
mydf<- ds[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf
mydf[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
newsid_col
mydf[,12]
colnames(mydf)[12]
colnames(mydf)
newsid_col <- which(colnames(mydf) == c("newsd"))
newsid_col
newsid_col <- which(colnames(mydf) == c("newsid"))
newsid_col
newsid_col <- which(colnames(ds) == c("newsid"))
ds[1:10,c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf <- ds[1:10000, order(names(ds[1:10,]))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf[1:10,c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf
mydf[1:10,c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
mymldr$labels
names(resds)
names(resds)
colnames(resds)
which(colnames(resds) != "Scored")
which(colnames(resds) != "Scored*")
resds %>% select(contains"Scored")
resds %>% select(contains("Scored"))
library(magrittr)
resds %>% select(contains("Scored"))
library(dplyr)
resds %>% select(contains("Scored"))
head(resds %>% select(contains("Scored")))
head(resds %>% select(!contains("Scored")))
head(resds %>% select(-contains("Scored")))
resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
seq(1:length(resds))
seq(2, 1:length(resds))
seq(1:length(resds))
seq(1:length(resds), 2)
seq(1:length(resds))
seq(2, length(names(resds)), 2)
seq(1,length(resds))
seq(1,length(resds),2 )
seq(2,length(resds),2 )
my_predictions <- resds[, seq(2, length(names(resds)), 2)]seq(2,length(resds),2 )
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
names(my_predictions)
my_predictions <- resds[, order(names(my_predictions))]
names(my_predictions)
my_predictions <- resds[, seq(2, length(names(resds)), 2)]names(my_predictions)
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
names(my_predictions)
names(mydf)
names(my_predictions)
names(mydf)
names(resds)
length(names(resds))
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
names(my_predictions)
mydf <- ds[1:10000, order(names(ds[1:10,]))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
names(mydf)
names(my_predictions)
mydf <- ds[, order(names(ds[1:10,]))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
length(names(my_predictions))
length(names(mydf))
names(mydf)
mymldr <- mldr_from_dataframe(mydf, c(2:length(mydf)))
summary(mymldr)
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
res
plot(res)
res
dim(my_predictions)
mydf <- ds[1:1000, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
plot(mymldr, type = "LC")
my_predictions <- resds[1:1000, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
head(my_predictions)
str(my_predictions)
res
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
subset_size <- 10000;
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
res
subset_size <- 100;
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
res
warnings()
my_predictions
max(my_predictions)
max(my_predictions$FCL.labels)
sum(my_predictions$FCL.labels)
sum(my_predictions$BSC.labels)
sum(my_predictions$IDU.labels)
warnings()
predictions <- as.matrix(emotions$dataset[, emotions$labels$index]) # and introduce some noise (alternatively get the predictions from some classifier) predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE) # then evaluate predictive performance res <- mldr_evaluate(emotions, predictions) str(res) plot(res$ROC, main = "ROC curve for emotions")
emotions
dim(emotions)
dim(predictions)
typeof(emotions)
typeof(predictions)
typeof(my_predictions)
head(my_predictions)
head(as.matrix(my_predictions))
head(predictions)
typeof(predictions)
typeof(my_predictions)
res <- mldr_evaluate(emotions, predictions)
str(res)
plot(res$ROC, main = "ROC curve for emotions")
predictions <- as.matrix(emotions$dataset[, emotions$labels$index])
res <- mldr_evaluate(emotions, predictions)
str(res)
plot(res$ROC, main = "ROC curve for emotions")
predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE)
res <- mldr_evaluate(emotions, predictions)
str(res)
library(mldr) predictions <- as.matrix(emotions$dataset[, emotions$labels$index]) # and introduce some noise (alternatively get the predictions from some classifier) predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE) # then evaluate predictive performance res <- mldr_evaluate(emotions, predictions) str(res) plot(res$ROC, main = "ROC curve for emotions")
library(mldr) predictions <- as.matrix(emotions$dataset[, emotions$labels$index]) # and introduce some noise (alternatively get the predictions from some classifier) predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE) # then evaluate predictive performance res <- mldr_evaluate(emotions, predictions) str(res) plot(res$ROC, main = "ROC curve for emotions")
sessionInfo
sessionInfo()
str(res)
str(res)library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" )
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
mydf <- ds[1:subset_size, order(colnames(ds))] newsid_col <- which(colnames(mydf) == c("newsid")) mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))] mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf)))) summary(mymldr)
library(mldr)
subset_size <- 100;
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
res
mymldr@labels
mymldr
str(mymldr)
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(2:length(names(mydf))), name = "foo")
mymldr$labels
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
mymldr$labels
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
res
warnings()
is.na(my_predictions)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
library(readr)
install.packages("readr")
exp <- read_csv("~/temp/Experiment created on _31_._07_._2017 - 506153734175476c4f62416c57734963.faa6ba63383c4086ba587abf26b85814.v1-default-1643 - Results data.csv")
library(readr)
exp <- read_csv("~/temp/Experiment created on _31_._07_._2017 - 506153734175476c4f62416c57734963.faa6ba63383c4086ba587abf26b85814.v1-default-1643 - Results data.csv")
exp <- read_csv("c:\\users\\lars bungum\\documents\exp.csv")
exp <- read_csv("c:\\users\\lars bungum\\documents\\exp.csv")
getwd()
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-209632",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) dim(ds)
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-209632",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-216896",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
names(ds)
dim(ds)
head(ds)
names(ds)
ds[1:2]
ds[1:2,]
ds[1:3,]
names(ds)
dim(ds)
library(tm)
b <- VectorSource(ds$dj_clean)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
# Clean corpus
dj.corpus
dj.corpus.clean <- tm_map(docs, content_transformer(tolower)) %>% tm_map(docs, removeNumbers) %>% tm_map(docs, removeWords, stopwords("english")) %>% tm_map(docs, removePunctuation) %>% tm_map(docs, stripWhitespace) %>% tm_map(docs, stemDocument, language = "english")
# Create dtm
library(magrittr)
# Clean corpus
dj.corpus.clean <- tm_map(docs, content_transformer(tolower)) %>% tm_map(docs, removeNumbers) %>% tm_map(docs, removeWords, stopwords("english")) %>% tm_map(docs, removePunctuation) %>% tm_map(docs, stripWhitespace) %>% tm_map(docs, stemDocument, language = "english")
dj.corpus.clean <- tm_map(docs, content_transformer(tolower)) %>% tm_map(removeNumbers) %>% tm_map(removeWords, stopwords("english")) %>% tm_map(removePunctuation) %>% tm_map(stripWhitespace) %>% tm_map(stemDocument, language = "english")
dj.corpus.clean <- tm_map(dj.corpus, content_transformer(tolower)) %>% tm_map(removeNumbers) %>% tm_map(removeWords, stopwords("english")) %>% tm_map(removePunctuation) %>% tm_map(stripWhitespace) %>% tm_map(stemDocument, language = "english")
# Create dtm
dj.corpus.clean
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus.clean, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.01917]
summary(slam::col_sums(djreduced.dtm))
# Run model
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
library(topicmodels)
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
install.packages("topicmodels")
library(topicmodels)
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.020380]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.010380]
djreduced.dtm
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.10380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.20380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.30380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.50380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.70380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
rowTotals <- apply(djreduced.dtm, 1, sum)
head(rawTotals)
head(rowTotals)
dtm.new <- djreduced.dtm[rowTotals > 0,]
dtm.new
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.02380]
summary(slam::col_sums(djreduced.dtm))
rowTotals <- apply(djreduced.dtm, 1, sum)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.52380]
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
dtm.new
system.time(dj.model <- topicmodels::LDA(dtm.new, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
dj.topics <- topicmodels::topics(llis.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
# label topics
topicTerms <- tidyr::gather(dj.terms, Topic)
topicTerms <- cbind(topicTerms, Rank = rep(1:30))
topTerms <- dplyr::filter(topicTerms, Rank < 4)
topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))
topTerms$Topic <- as.numeric(topTerms$Topic)
topicLabel <- data.frame()
for (i in 1:27) {     z <- dplyr::filter(topTerms, Topic == i)     l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "), stringsAsFactors = FALSE)     topicLabel <- rbind(topicLabel, l) }
colnames(topicLabel) <- c("Label")
topicLabel
# Visualize LDA model topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(djreduced.dtm)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) }
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
library(LDAvis)
install.packages("LDAvis")
library(LDAvis)
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
serVis(dj.json)
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, dtm.new)
djcorp.new <- dj.corpus[rowTotals > 0,]
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-10253",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(ds)
names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
names(ds)
ds[order(dsignrank$Topic.1),]
ds[order(ds$Topic.1),]
tail(ds[order(ds[, 3]), c(1, 3)])
tail(ds[order(ds[, 3]), c(1, 2)])
tail(ds[order(ds[, 1]), c(1, 2)])
tail(ds[order(ds[, 2]), c(1, 2)])
tail(ds[order(ds[, 3]), c(1, 3)])
tail(ds[order(ds[, 4]), c(1, 4)])
tail(ds[order(ds[, 5]), c(1, 5)])
tail(ds[order(ds[, 6]), c(1, 6)])
tail(ds[order(ds[, 7]), c(1, 7)])
tail(ds[order(ds[, 7]), c(1, 8)])
tail(ds[order(ds[, 7]), c(1, 6)])
tail(ds[order(ds[, 7]), c(1, 7)])
tail(ds[order(ds[, 8]), c(1, 8)])
tail(ds[order(ds[, 9]), c(1, 9)])
tail(ds[order(ds[, 10]), c(1, 10)])
dim(ds)
t(ds)
phi <- t(ds)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
filename <- "thetaDS_LDA_Prep.csv"
foo <- read.csv(filename)
filename <- "c:\\users\\bungun\\documents\\thetaDS_LDA_Prep.csv"
foo <- read.csv(filename)
filename <- "c:\\users\\bungum\\documents\\thetaDS_LDA_Prep.csv"
foo <- read.csv(filename)
library(readr)
read_csv(filename)
thethads <- read_csv(filename)
dim(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
filename <- "c:\\users\\bungum\\documents\\thetaTSV.tsv"
thetads <- read.csv(filename, header = T, sep = "\t")
names(thethads)
names(thethads)[1:10]
names(phi)
dim(phi)
phi[1:10,1:10]
phi[1,1]
head(thetads)
thetads[1:10,1:10]
phi[1:10,1:10]
phi.df <- as.data.frame(phi)
names(phi.df)
dim(phi.df)
dim(phi)
colnames(phi)
attributes(phi)
phi[1:10,1:10]
phi[1,1:10]
phi.df[1,1:10]
colnames(phi.df) <- phi.df[1,]
colnames(phi.df)
phi.df[1,1:10]
phi[1,1:10]
foo <- phi[1,]
typeof(foo)
colnames(phi.df)
dim(colnames(phi.df))
length(colnames(phi.df))
length(foo)
colnames(phi.df) <- foo
colnames(phi.df)
phi.df[1,1:10]
bar <- phi.df[-1,1:10]
dim(bar)
bar <- phi.df[-1,]
bar
dim(bar)
colnames(bar)
phi.done.df <- bar
dim(phi.done.df)
dim(theta)
dim(thetads)
dim(ds)
head(ds)
ds[1,1:10]
dim(ds)
thetads <- t(ds[,2:501])
dim(thetads)
dim(phi.dine.df)
dim(phi.done.df)
dim(vocab)
vocab <- foo
length(foo)
foo[1:10]
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-27360",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
filename_TF <- "c:\\users\\bungum\\documents\\TF_LDA.tsv"
TF_LDA <- read.csv(filename_tf, header = T, sep = "\t")
TF_LDA <- read.csv(filename_TF, header = T, sep = "\t")
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109411",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(vocabds)
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
dim(phids)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109411",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(vocabds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) head(ds)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(phids)
dim(thetads)
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
phi.df <- as.data.frame(phids)
vocab <- phi[1,]
vocab[1:10]
length(vocab)
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
phi.df <- as.data.frame(phids)
# the vocabulary is in the first row
vocab <- phi[1,]
length(vocab)
dim(phids)
phi.df <- as.data.frame(phids)
vocab <- phids[1,]
length(vocab)
names(phids)
dim(phids)
dim(t(phids))
phi <- t(phids)
vocab <- phi[1,]
length(vocab)
colnames(phi.df) <- vocab
phi.df <- as.data.frame(phi)
# the vocabulary is in the first row
vocab <- phi[1,]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
# get the first row out of the data frame
phi.done.df <- phi.df[-1,]
thetads <- t(ds[, 2:31])
dim(vocabds)
names(vocabds)
names(vocabds)[1:10]
vocabds[1:10,1:10]
vocabds[1:5,1:5]
vocabds[2:5,1:5]
vocabds[1:5,2:5]
foo <- as.data.frame(vocabs)
foo <- as.data.frame(vocabds)
foo <- foo[,-c(1)]
dim(foo)
colnames(foo) <- vocab
names(foo)
foo <- foo[,-c(1)]vocab.df <-  vocab.df[,c(1)]
vocab.df <-  vocab.df[,c(1)]
vocab.df <- as.data.frame(vocabs)
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <-  vocab.df[,c(1)]
colnames(vocab.df) <- vocab
vocab.df <- vocab.df[, c(1)]
dim(vocab.df)
vocab.df <- as.data.frame(vocabds)
dim(vocab.df)
vocab.df <- vocab.df[, -c(1)]
dim(vocab.df)
colnames(vocab.df) <- vocab
head(vocab.df)
colSums(vocab.df)
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
                           Freq = colSums(vocab.df))
head(freq_matrix)
names(thetads)[1:2]
names(phids)[1:2]
phids[1:10,1]
thetads[1,1]
ds[1,1]
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
dim(lengthds)
typeof(lengthds)
vector(lengthds)
lengthds$length.Preprocessed.dj_clean.  
lengthds$length.Preprocessed.dj_clean. <- as.numeric(lengthds$length.Preprocessed.dj_clean.)
vector(lengthds)
vector(lengthds$length.Preprocessed.dj_clean.)
as.matrix(lengthds$length.Preprocessed.dj_clean.)
foo <- as.matrix(lengthds$length.Preprocessed.dj_clean.)
foo
typeof(foo)
typeof(as.integer(foo))
length:mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))foo <- as.matrix(lengthds$length.Preprocessed.dj_clean.)
length_mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))
typeof(length_mat)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.done.df)
length(vocab)
dim(thetads)
thetads <- t(ds[, 2:31])
dim(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(thetads)
thetads <- t(thetads)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(thetads)
dim(phi.done.df)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(thetads)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
                                    term.frequency = freq_matrix$Freq)
rowSums(phi.done.df)
str(phi.done.df)
typeof(phi.done.df[1,])
typeof(phi.done.df[1,1])
typeof(phi.done.df[1,])
typeof(phi.done.df[2,])
names(phi.done.df-d)
names(phi.done.df)
dim(phi.done.df)
typeof(phi.done.df[,1])
typeof(phi.done.df[,2])
typeof(phi.done.df[,3])
typeof(phi.done.df[,4])
sum(hi.done.df[,4])
sum(phi.done.df[,4])
sum(as.integer(phi.done.df[,4]))
sum(as.numeric(phi.done.df[,4]))
phi.done.df[,4])
phi.done.df[,4])
phi.done.df[,4]
rowSum(phi.done.df[,4])
rowSums(phi.done.df[,4])
sum(phi.done.df[,4])
str(phi.done.df[,4])
str(as.float(phi.done.df[,4]))
as.numeric(as.float(phi.done.df[,4]))
as.numeric(phi.done.df[,4])
phi.done.df[,4]
phi.done.df[1,4]
names(phi.done.df)
phi.done.df[1,4]
foo <- as.matrix(phi.done.df)
dim(foo)
names(foo)
head(foo)
colnames(foo)
rowSums(foo)
typeof(foo)
rowSums(as.numeric(foo))
bar <- as.numeric(foo))
bar <- as.numeric(foo)
dim(bar)
names(bar)
bar[1:10,1:10]
dim(bar)
bar <- as.numeric(foo)
dim(bar)
as.numeric(foo)
foo <- as.matrix(phi.done.df)
tyoeof(foo)
typeof(foo)
typeof(foo[,1])
sum(foo[,1])
sum(as.numeric(foo[,1]))
as.numeric(foo[,1])
sum(as.numeric(foo[,1]))
sum(as.numeric(phi.done.df[,1]))
as.numeric(phi.done.df[,1])
phi_mat <- as.matrix(phi.done.df)foo <- as.matrix(phi.done.df)
phi_mat <- as.matrix(phi.done.df)
phi_mat <- sapply(phi_mat, as.numeric)
rowSums(phi_mat)
rowSums(as.data.frame(phi_mat))
dim(as.data.frame(phi_mat))
dim(phi_mat)
length(phi_mat)
phi_mat <- as.matrix(phi.done.df)
dim(phi_mat)
phi_mat <- sapply(phi_mat, as.numeric)
dim(phi_mat)
phi_mat <- as.matrix(phi.done.df)
dim(sapply(phi_mat, as.numeric))
dim(lapply(phi_mat, as.numeric))
dim(apply(phi_mat, as.numeric))
df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
lapply(phi.done.df, function(x) as.numeric(as.character(x)))
phi.done.num.df <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
rowSums(phi.done.num.df)
dim(phi.done.num.df)
length(phi.done.num.df)
phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
dim(phi.done.df)
rowSums(phi.done.df)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
phi <- t(phids)
#convert the phi you read in as a data frame
phi.df <- as.data.frame(phi)
# the vocabulary is in the first row
vocab <- phi[1,]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
# get the first row out of the data frame
phi.done.df <- phi.df[-1,]
# Convert all columns from factors to numeric
phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
# make the theta dataset out of he transpose of the vocabulary and the 500 last rows (minus the feature ID).  NB the topic number is hard coded in her
thetads <- t(thetads)
# Convert the vocabds into a data.frame
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))
# Convert phi to matrix
phi_mat <- as.matrix(phi.done.df)
phi_mat <- sapply(phi_mat, as.numeric)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(dj.json)
dim(thetads)
dim(phi.done.df)
thetads <- t(thetads)
dim(thetads)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
head(phi.done.df)
phi.done.df[1,]
sum(phi.done.df[1,])
rowS
rowSums(phi.done.df)
rowSums(phi.done.df)==1
rowSums(phi.done.df)[1]
rowSums(phi.done.df)[1]==1
rowSums(phi.done.df)[2]==1
rowSums(phi.done.df)[2]
rowSums(phi.done.df)[2]==1
typeof(rowSums(phi.done.df)[2])
dim(phids)
colSums(phids[2,:31])
colSums(phids[,2:31])
rowSums(phi)
rowSums(phi[2,31,])
dim(phi)
typeof(phi[2,31,])
typeof(phi[2:31,])
typeof(phids[,2:31])
rowSums(phi[2,31,])
colSums(phids[,2:31])
typeof(phids[,2:31])
typeof(phids[,2:31])
colSums(phids[,2:31])
typeof(phids[,2:31])
typeof(phids[1,2:31])
typeof(phids[1,2])
colSums(phids[,2:31])
rowSums(phi)
rowSums(phi[2:30,1])
rowSums(phi[2:30,])
rowSums(as.numeric(phi[2:30,]))
dim(phids)
foo <- phids[,2:31]
colSums(foo)
rowSums(t(foo))
phi <- t(phids[,2:31])
json_lda <- LDAvis::createJSON(phi = phi, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(dj.json)
dim(thetads)
rowSums(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(thetads)
rowSums(thetads)
typeof(thetads)
typeof(thetads[,1])
typeof(thetads[,2])
sum(thetads[,2])
sum(thetads[1,])
sum(thetads[2,])
sum(thetads[2,])==1
sum(thetads[2,])
thetads[2,]
dim(thetads)
dim(thetads[2,])
length(thetads[2,])
colSums(thetads)
rowSums(thetads)
length(thetads[2,])
length(thetads[,2])
sum(thetads[,2])
dim(thetads)
sum(thetads[1,])
sum(thetads[2,])
rowSums(thetads[2,])
rowSums(thetads)
typeof(thetads)
typeof(thetads[1,2])
typeof(thetads[1,])
sum(thetads[1,])
colSums(phi_transpose_nums)
phi_transpose_nums <- t(phids[,2:31])
colSums(phi_transpose_nums)
rowSums(phi_transpose_nums)
rowSums(phi_transpose_nums)
rowSums(thetads)
colSums(thetads)
rowSums(thetads)
rowSums(phi_transpose_nums)
rowSums(phi_transpose_nums) == 1
trace(createJSON, edit = T)
all.equal(rowSums(phi_transpose_nums), rep(1, K), check.attributes = FALSE)
trace(createJSON, edit = T)
dt <- dim(theta)
K = dt[2]
dt <- dim(thetads)
K = dt[2]
K
phi.test <- all.equal(rowSums(phi), rep(1, K), check.attributes = FALSE)
theta.test <- all.equal(rowSums(theta), rep(1, dt[1]), check.attributes = FALSE)
phi.test <- all.equal(rowSums(phi_transpose_nums), rep(1, K), check.attributes = FALSE)
theta.test <- all.equal(rowSums(thetads), rep(1, dt[1]), check.attributes = FALSE)
phi.test
theta.test
trace(createJSON, edit = T)
trace(createJSON, edit = T)
trace(createJSON, edit = T)
rowSums(thetads)
rowSums(thetads)[1:10]
rowSums(thetads)[1:1]
equal(rowSums(thetads)[1:1],1)
all.equal(rowSums(thetads)[1:1],1)
all.equal(rowSums(thetads)[2],1)
all.equal(rowSums(thetads)[3],1)
all.equal(rowSums(phi_transpose_sums)[3],1)
all.equal(rowSums(phi_transpose_nums)[3],1)
all.equal(rowSums(phi_transpose_nums)[1],1)
rowSums(phi_transpose_nums)[1]
rowSums(phi_transpose_nums)[2]
rowSums(phi_transpose_nums)[3]
phi_transpose_nums[1,]
sum(phi_transpose_nums[1,])
all.equal(rowSums(phi_transpose_nums)[1],1)
all.equal(rowSums(phi_transpose_nums)[1],1) == T
phi.test
all.equal(rowSums(phi_transpose_nums)[1],1, chec.attributes= F) == T
all.equal(rowSums(phi_transpose_nums)[1],1, check.attributes= F) == T
all.equal(rowSums(thetads)[1],1, check.attributes= F) == T
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
all.equal(rowSums(thetads)[3],1, check.attributes= F) == T
all.equal(rowSums(thetads)[4],1, check.attributes= F) == T
theta.test
theta.test == T
phi.test == T
thetads[1,]
sum(thetads[1,])
length(thetads[1,])
sum(thetads[1,])
1-sum(thetads[1,])
thethads[1,30] <- thetads[1,30]+(1-sum(thetads[1,]))
thetads[1,]
sum(thetads[1,])
thetads[1,30]+(1-sum(thetads[1,]))
thethads[1,30] <- (thetads[1,30]+(1-sum(thetads[1,])))
thethads[1,30]
sum(thetads[1,])
1-sum(thetads[1,])
thethads[1,30] <- (thetads[1,30]+(1-sum(thetads[1,])))
1-sum(thetads[1,])
thethads[1,30]
thethads[1,30] <- (thetads[1,30]+(1-sum(thetads[1,])))
thethads[1,30]
1-sum(thetads[1,])
(thetads[1,30]+(1-sum(thetads[1,])))
thethads[1,30]
1-sum(thetads[1,])
thetads[1, 30] + 1.156e-07
thetads[1, 30]
thetads[1, 30] + 1.156e-07
thetads[1, 30]
thetads[1, 30] <- thetads[1, 30] + 1.156e-07
thetads[1, 30]
1-sum(thetads[1,])
all.equal(rowSums(thetads)[1],1, check.attributes= F) == T
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
1-sum(thetads[2,])
1-sum(thetads[2,])[[1]]
1-sum(thetads[2,])[1]
thetads[2,30] + (1-sum(thetads[2,])[1])
thetads[2,30]
(1-sum(thetads[2,])[1])
thetads[2, 30] + 4.13e-08
thetads[2,30] + (1-sum(thetads[2,])[1])
thetads[2,30] + (1-sum(thetads[2,]))
thetads[2,30] + (1-sum(thetads[2,]))
thetads[2,30] 
thetads[2,30] + (1-sum(thetads[2,]))
thetads[2,30] <- thetads[2,30] + (1-sum(thetads[2,]))
thetads[2, 30]
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
dim(thetads)
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(thetads)
    R <- dt[1]
    for (r in 1:R) {
        df[2, 30] <- df[2, 30] + (1 - sum(df[2,]))
    }
}
impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[2, 30] <- df[2, 30] + (1 - sum(df[2,]))
    }
    return df
}
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[2, 30] <- df[2, 30] + (1 - sum(df[2,]))
    }
    return (df)
}
foo <- impute_df_to_sum_to_one(thetads)
dim(foo)
theta.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
theta.test
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[R, 30] <- df[R, 30] + (1 - sum(df[R,]))
    }
    return (df)
}
theta.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
theta.test
foo <- impute_df_to_sum_to_one(thetads)
theta.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
theta.test
dt <- dim(df)
R <- dt[1]
R
dt
dt <- dim(thetads)
R <- dt[1]
R
thetads[2,30] + (1-sum(thetads[2,]))
(1-sum(thetads[2,]))
(1-sum(thetads[3,]))
(1-sum(thetads[3,]))
thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3, 30]
thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3,30] <- thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3,30] 
foo <- impute_df_to_sum_to_one(thetads)
all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
all.equal(rowSums(foo)[2],1, check.attributes= F) == T
all.equal(rowSums(foo)[3],1, check.attributes= F) == T
all.equal(rowSums(foo)[4],1, check.attributes= F) == T
thetads[4,30] <- thetads[4, 30]+(1-sum(thetads[4,]))
all.equal(rowSums(thetads)[4],1, check.attributes= F) == T
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[R, 30] <- df[R, 30] + (1 - sum(df[R,]))
        all.equal(rowSums(df)[R], 1, check.attributes = F) == T
    }
    return (df)
}
foo <- impute_df_to_sum_to_one(thetads)
foo
foo == thetads
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    df_out <- df
    for (r in 1:R) {
        df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))
        all.equal(rowSums(df)[R], 1, check.attributes = F) == T
    }
    return (df_out)
}
foo <- impute_df_to_sum_to_one(thetads)
foo == thetads
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
foo <- imp
foo <- impute_df_to_sum_to_one(thetads)
foo == thetads
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(R)         df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
impute_df_to_sum_to_one(thetads[1:19,])
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
f <- impute_df_to_sum_to_one(thetads[1:19,])
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
f <- impute_df_to_sum_to_one(thetads[1:19,])
f <- impute_df_to_sum_to_one(thetads)
f == thetads
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         print(df_out)     }     return (df_out) }
foo <- impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         print(temp)     }     return (df_out) }foo <- impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         print(temp)     }     return (df_out) }
foo <- impute_df_to_sum_to_one(thetads)
dt <- dim(thetads)
K = dt[2]
theta.test <- all.equal(rowSums(thetads), rep(1, dt[1]), check.attributes = FALSE)
theta.test
foo.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
foo.test
foo.test
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
phi_transpose_nums <- t(phids[,2:31])
colnames(phi.df) <- vocab
vocab <- phi[1,]
rownames(phids)
colnames(phids)
phi.df <- as.data.frame(phi_transpose_nums)
colnames(phi.df) <- vocab
names(phids)
phids[1,]
phids[,1]
vocab <- phids[,1]
colnames(phi.df) <- vocab
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))
# Convert phi to matrix
phi_mat <- as.matrix(phi.done.df)
phi_mat <- sapply(phi_mat, as.numeric)
json_lda <- LDAvis::createJSON(phi = phi_transpose_nums, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(dj.json)
dt <- dim(thetads)
K = dt[2]
phi.test <- all.equal(rowSums(phi_transpose_nums), rep(1, K), check.attributes = FALSE)
theta.test <- all.equal(rowSums(thetads), rep(1, dt[1]), check.attributes = FALSE)
foo.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
foo.test
json_lda <- LDAvis::createJSON(phi = phi_transpose_nums, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(freq_matrix)
dim(doc.length)
dim(length_mat)
length(vocab)
trace("createJSON", edit = TRUE)
library(LDAvis)
trace("createJSON", edit = TRUE)
vocab_df[order(saliency, decreasing = TRUE)][1:R]
vocab
vocab.df[order(saliency, decreasing = TRUE)][1:R]
trace("createJSON", edit = TRUE)
dim(phi_transpose_nums)
dim(foo)
dim(freq_matrix)
length(length_mat)
str(length_mat)
json_lda <- LDAvis::createJSON(phi = phi_transpose_nums, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
trace("createJSON", edit = TRUE)
topic.frequency <- colSums(foo * length_mat)
topic.proportion <- topic.frequency / sum(topic.frequency)
o <- order(topic.proportion, decreasing = TRUE)
phi <- phi_transpose_nums[o,]
theta <- thetads[,o]
topic.frequency <- topic.frequency[o]
topic.proportion <- topic.proportion[o]
term.topic.frequency <- phi * topic.frequency
term.frequency <- colSums(term.topic.frequency)
stopifnot(all(term.frequency > 0))
term.proportion <- term.frequency / sum(term.frequency)
phi <- t(phi)
topic.given.term <- phi / rowSums(phi)
kernel <- topic.given.term * log(sweep(topic.given.term, MARGIN = 2, topic.proportion, `/`))
distinctiveness <- rowSums(kernel)
saliency <- term.proportion * distinctiveness
default.terms <- vocab[order(saliency, decreasing = TRUE)][1:R]
default.terms <- vocab[order(saliency, decreasing = TRUE)][1:30]
default.terms
    counts <- as.integer(term.frequency[match(default.terms,
        vocab)])
    Rs <- rev(seq_len(R))
Rs <- rev(seq_len(30))
    default <- data.frame(Term = default.terms, logprob = Rs,
        loglift = Rs, Freq = counts, Total = counts, Category = "Default",
        stringsAsFactors = FALSE)
    topic_seq <- rep(seq_len(K), each = R)
topic_seq <- rep(seq_len(K), each = 30)
    category <- paste0("Topic", topic_seq)
    lift <- phi / term.proportion
    find_relevance <- function(i) {
        relevance <- i * log(phi) + (1 - i) * log(lift)
        idx <- apply(relevance, 2, function(x) order(x, decreasing = TRUE)[seq_len(R)])
        indices <- cbind(c(idx), topic_seq)
        data.frame(Term = vocab[idx], Category = category, logprob = round(log(phi[indices]),
            4), loglift = round(log(lift[indices]), 4), stringsAsFactors = FALSE)
    }
    lambda.seq <- seq(0, 1, by = lambda.step)
lambda.step = 0.01, mds.method = jsPCA, cluster
lambda.step = 0.01, mds.method = jsPCA
lambda.step = 0.01
    lambda.seq <- seq(0, 1, by = lambda.step)
    if (missing(cluster)) {
        tinfo <- lapply(as.list(lambda.seq), find_relevance)
    }
    else {
        tinfo <- parallel::parLapply(cluster, as.list(lambda.seq),
            find_relevance)
    }
    tinfo <- unique(do.call("rbind", tinfo))
cluster
    if (missing(cluster)) {
        tinfo <- lapply(as.list(lambda.seq), find_relevance)
    }
default
    if (missing(cluster)) {
        tinfo <- lapply(as.list(lambda.seq), find_relevance)
    }
cluster = list()
tinfo <- lapply(as.list(lambda.seq), find_relevance)
    find_relevance <- function(i) {
        relevance <- i * log(phi) + (1 - i) * log(lift)
        idx <- apply(relevance, 2, function(x) order(x, decreasing = TRUE)[seq_len(30)])
        indices <- cbind(c(idx), topic_seq)
        data.frame(Term = vocab[idx], Category = category, logprob = round(log(phi[indices]),
            4), loglift = round(log(lift[indices]), 4), stringsAsFactors = FALSE)
    }
tinfo <- lapply(as.list(lambda.seq), find_relevance)
    tinfo <- unique(do.call("rbind", tinfo))
    tinfo$Total <- term.frequency[match(tinfo$Term, vocab)]
    rownames(term.topic.frequency) <- paste0("Topic", seq_len(K))
    colnames(term.topic.frequency) <- vocab
    tinfo$Freq <- term.topic.frequency[as.matrix(tinfo[c("Category",         "Term")])]
    tinfo <- rbind(default, tinfo)
    ut <- sort(unique(tinfo$Term))
    m <- sort(match(ut, vocab))
    tmp <- term.topic.frequency[, m]
    r <- row(tmp)[tmp >= 0.5]
    c <- col(tmp)[tmp >= 0.5]
    dd <- data.frame(Term = vocab[m][c], Topic = r, Freq = round(tmp[cbind(r,         c)]), stringsAsFactors = FALSE)
    dd[, "Freq"] <- dd[, "Freq"] / term.frequency[match(dd[, "Term"],         vocab)]
    token.table <- dd[order(dd[, 1], dd[, 2]),]
    RJSONIO::toJSON(list(mdsDat = mds.df, tinfo = tinfo, token.table = token.table,         R = R, lambda.step = lambda.step, plot.opts = plot.opts,         topic.order = o))
}
    mds.res <- mds.method(phi)
mds.method = jsPCA
    mds.res <- mds.method(phi)
str(phi)
dim(phi)
phi.df <- as.data.frame(phi)
dim(phi.df)
names(phi.df)
dim(phi)
dim(phids)
dim(phi)
dim(phi.df)
t(phi.df)
dim(t(phi.df))
names(t(phi.df))
dim(as.data.frame((t(phi.df))))
names(as.data.frame((t(phi.df))))
phi.df <- as.data.frame((t(phi.df))))
phi.df <- as.data.frame((t(phi.df)))
colnames(phi.df) <- vocab
jsPCA(phi.df)
dim(phi_transpose_nums)
dim(ph.df)
dim(phi.df)
str(phi_transpose_nums)
jsPCA(phi_transpose_nums)
jsPCA(phi.df)
jsPCA(phi.df)json_lda <- LDAvis::createJSON(phi = phi.df, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.df)
typeof(phi.df[1,])
typeof(phi.df[1,1])
jsPCA(phi.df[,1:1000])
jsPCA(phi.df[,1:10000])
system.time(jsPCA(phi.df[,]))
system.time(foo <- jsPCA(phi.df[,]))
dim(foo)
theta_imputed <- impute_df_to_sum_to_one(thetads)
phi=phi.df
theta = theta_imputed
doc.length = length.mat
doc.length = length_mat
dim(vocab)
length(vocab)
    dp <- dim(phi)
    dt <- dim(theta)
    N <- sum(doc.length)
    W <- length(vocab)
    D <- length(doc.length)
    K <- dt[2]
    if (dp[1] != K)
        stop("Number of rows of phi does not match \n      number of columns of theta; both should be equal to the number of topics \n      in the model.")
    if (D != dt[1])
        stop("Length of doc.length not equal \n      to the number of rows in theta; both should be equal to the number of \n      documents in the data.")
    if (dp[2] != W)
        stop("Number of terms in vocabulary does \n      not match the number of columns of phi (where each row of phi is a\n      probability distribution of terms for a given topic).")
    if (length(term.frequency) != W)
        stop("Length of term.frequency \n      not equal to the number of terms in the vocabulary.")
    if (any(nchar(vocab) == 0))
        stop("One or more terms in the vocabulary\n      has zero characters -- all terms must have at least one character.")
    phi.test <- all.equal(rowSums(phi), rep(1, K), check.attributes = FALSE)
    theta.test <- all.equal(rowSums(theta), rep(1, dt[1]), check.attributes = FALSE)
    if (!isTRUE(phi.test))
        stop("Rows of phi don't all sum to 1.")
    if (!isTRUE(theta.test))
        stop("Rows of theta don't all sum to 1.")
    topic.frequency <- colSums(theta * doc.length)
    topic.proportion <- topic.frequency / sum(topic.frequency)
    o <- order(topic.proportion, decreasing = TRUE)
    phi <- phi[o,]
    theta <- theta[, o]
    topic.frequency <- topic.frequency[o]
    topic.proportion <- topic.proportion[o]
    mds.res <- mds.method(phi)
    if (is.matrix(mds.res)) {         colnames(mds.res) <- c("x", "y")     }
    else if (is.data.frame(mds.res)) {
        names(mds.res) <- c("x", "y")
    }
    else {
        warning("Result of mds.method should be a matrix or data.frame.")
mds.method <- jsPCA
R <- 30
    mds.res <- mds.method(phi)
    if (is.matrix(mds.res)) {         colnames(mds.res) <- c("x", "y")     }
    else if (is.data.frame(mds.res)) {
        names(mds.res) <- c("x", "y")
    }
colnames(mds.res)
    mds.df <- data.frame(mds.res, topics = seq_len(K), Freq = topic.proportion *         100, cluster = 1, stringsAsFactors = FALSE)
    term.topic.frequency <- phi * topic.frequency
    term.frequency <- colSums(term.topic.frequency)
    stopifnot(all(term.frequency > 0))
    term.proportion <- term.frequency / sum(term.frequency)
    phi <- t(phi)
    topic.given.term <- phi / rowSums(phi)
    kernel <- topic.given.term * log(sweep(topic.given.term,         MARGIN = 2, topic.proportion, `/`))
    distinctiveness <- rowSums(kernel)
    saliency <- term.proportion * distinctiveness
    default.terms <- vocab[order(saliency, decreasing = TRUE)][1:R]
    counts <- as.integer(term.frequency[match(default.terms,         vocab)])
    Rs <- rev(seq_len(R))
    default <- data.frame(Term = default.terms, logprob = Rs,         loglift = Rs, Freq = counts, Total = counts, Category = "Default",         stringsAsFactors = FALSE)
    topic_seq <- rep(seq_len(K), each = R)
    category <- paste0("Topic", topic_seq)
    lift <- phi / term.proportion
    find_relevance <- function(i) {         relevance <- i * log(phi) + (1 - i) * log(lift)         idx <- apply(relevance, 2, function(x) order(x, decreasing = TRUE)[seq_len(30)])         indices <- cbind(c(idx), topic_seq)         data.frame(Term = vocab[idx], Category = category, logprob = round(log(phi[indices]),             4), loglift = round(log(lift[indices]), 4), stringsAsFactors = FALSE)     }
    lambda.seq <- seq(0, 1, by = lambda.step)
    if (missing(cluster)) {         tinfo <- lapply(as.list(lambda.seq), find_relevance)     }
    else {
        tinfo <- parallel::parLapply(cluster, as.list(lambda.seq),             find_relevance)
    }
    tinfo <- unique(do.call("rbind", tinfo))
    tinfo$Total <- term.frequency[match(tinfo$Term, vocab)]
    rownames(term.topic.frequency) <- paste0("Topic", seq_len(K))
    colnames(term.topic.frequency) <- vocab
missing(cluster)
tinfo <- lapply(as.list(lambda.seq), find_relevance)
    tinfo <- rbind(default, tinfo)
    ut <- sort(unique(tinfo$Term))
names(tfinfo)
names(tinfo)
names(tinfo$Term)
tinfo$Term
    ut <- sort(unique(tinfo$Term))
    tinfo$Total <- term.frequency[match(tinfo$Term, vocab)]
    rownames(term.topic.frequency) <- paste0("Topic", seq_len(K))
    colnames(term.topic.frequency) <- vocab
    tinfo$Freq <- term.topic.frequency[as.matrix(tinfo[c("Category",         "Term")])]
    tinfo$Freq <- term.topic.frequency[as.matrix(tinfo[c("Category","Term")])]
term.topic.frequency
names(term.topic.frequency)
colnames(term.topic.frequency)
dim(term.topic.frequency)
colnames(term.topic.frequency)
rownames(term.topic.frequency)
as.matrix(tinfo[c("Category", "Term")])
as.matrix(tinfo[c("Category", "Term")])
term.topic.frequency[as.matrix(tinfo[c("Category", "Term")])]
term.topic.frequency[tinfo[c("Category", "Term")]]
term.topic.frequency[as.matrix(tinfo[c("Category", "Term")])]
str(phi.df)
str(theta_imputed)
rownames(theta_imputed)
colames(theta_imputed)
colnames(theta_imputed)
colnames(theta_imputed) <- seq(1:30)
str(theta_imputed)
str(length_mat)
typeof(length_mat)
str(list(length_mat))
bar <- (as.integer(lengthds$length.Preprocessed.dj_clean.))
str(bar)
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
str(freq_matrix$Freq)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.df)
dim(theta_imputed)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
    tinfo <- rbind(default, tinfo)
Rs
length(Rs)
length(counts)
length(default.terms)
str(vocab.df)
str(vocab)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
dim(theta)
topic.frequency
serVis(json_lda)
topic.proportion
topic.proportion*100
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
phi_transpose_nums <- t(phids[,2:31])
#convert the phi you read in as a data frame
phi.df <- as.data.frame(phi_transpose_nums)
# the vocabulary is in the first row
vocab <- phids[,1]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
theta_imputed <- impute_df_to_sum_to_one(thetads)
library(LDAvis) json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
topic.frequency <- colSums(theta_imputed * doc.length)
topic.frequency <- colSums(theta_imputed * length_mat)
topic.proportion <- topic.frequency / sum(topic.frequency)
topic.proportion
o <- order(topic.proportion, decreasing = TRUE)
o
phi <- phi[o,]
phi <- phi.df[o,]
phi
theta <- theta_imputed[, o]
topic.frequency[o]
    topic.frequency <- topic.frequency[o]
    topic.proportion <- topic.proportion[o]
topic.frequency
topic.proportion*100
sum(topic.proportion)
colSums(theta_imputed)
dim(theta_imputed)
rowSums(theta_imputed)
colSums(theta_imputed)
colSums(theta_imputed*length_mat)
    term.topic.frequency <- phi * topic.frequency
    term.frequency <- colSums(term.topic.frequency)
term.frequency
dim(phi)
rowSums(phi)
rowSums(phi.df)
rowSums(t(phi.df))
t(phi)/rowSums(t(phi.df))
dim(t(phi)/rowSums(t(phi.df)))
topic.given.term <- (t(phi) / rowSums(t(phi.df)))
    kernel <- topic.given.term * log(sweep(topic.given.term,         MARGIN = 2, topic.proportion, `/`))
    distinctiveness <- rowSums(kernel)
    saliency <- term.proportion * distinctiveness
    term.proportion <- term.frequency / sum(term.frequency)
    distinctiveness <- rowSums(kernel)
    saliency <- term.proportion * distinctiveness
    default.terms <- vocab[order(saliency, decreasing = TRUE)][1:R]
default.terms <- vocab[order(saliency, decreasing = TRUE)][1:30]
dim(default.terms)
head(default.terms)
    counts <- as.integer(term.frequency[match(default.terms,         vocab)])
counts
dim(vocab.df)
colnames(vocab.df)
colnames(vocab.df)[1:10]
vocab[1:20]
vocab.df <- as.data.frame(vocabds)
vocab.df <- vocab.df[, -c(1)]
colnames(vocab.df)[1:10]
vocab.df[1:10,1]
vocab.df[1:10,2]
colSums(vocab.df)
colSums(vocab.df)[1:10]
vocab[1:10]
vocab=="uzu"
vocab[9681]
colSums(vocab.df)[9681]
head(freq_matrix)
freq_matrix[9681]
freq_matrix[9681,]
max(freq_matrix,2)
max(freq_matrix,3)
max(freq_matrix$Freq)
freq_matrix$Freq==20442
freq_matrix[19450,]
freq_matrix[19451,]
freq_matrix[19449,]
freq_matrix[19150,]
dim(phi)
dim(phi*topic.frequency)
dim(topic.frequency)
length(topic.frequency)
topic.frequency
dim(phi)
dim(term.topic.frequency)
term.topic.frequency[1:10,1:10]
term.topic.frequency[1:10,9680:9682]
term.topic.frequency[1:10,9680:9690]
phi[1:10,9680:9690]
dim(phids)
phids[9681,]
sum(phids[9681,])
sum(phids[9681,2:31])
sum(phids[9682,2:31])
sum(phids[9683,2:31])
phids[9681,]
dim(phi.df)
phi.df[,9681]
sum(phi.df[,9681])
sum(phi.df[1,])
sum(phi.df[,9681])
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
names(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
phi_transpose_nums <- t(phids[,2:11])
phi.df <- as.data.frame(phi_transpose_nums)
vocab <- phids[,1]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
thetads <- t(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
dim(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
dim(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
theta_imputed <- impute_df_to_sum_to_one(t(thetads))
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.df)
dim(theta_imputed)
R
dim(thetads)
dim(thetads)[1]
theta_imputed <- impute_df_to_sum_to_one(thetads)
dim(thetads)[2]
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, 30] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, K] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) }
theta_imputed <- impute_df_to_sum_to_one(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
# Transpose to get the right format for the 
phi_transpose_nums <- t(phids[,2:11])
#convert the phi you read in as a data frame
phi.df <- as.data.frame(phi_transpose_nums)
# the vocabulary is in the first row
vocab <- phids[,1]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
# get the first row out of the data frame
#phi.done.df <- phi.df[-1,]
# Convert all columns from factors to numeric
#phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
# make the theta dataset out of he transpose of the vocabulary and the 500 last rows (minus the feature ID).  NB the topic number is hard coded in her
#thetads <- t(ds[, 2:31])
#thetads <- t(thetads)
# Convert the vocabds into a data.frame
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, K] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
dim(thetads)
thetads[1:100,1]
thetads[1:100,]
thetads[1:40,]
thetads[1:20,]
thetads[1:10,]
thetads[,1]
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
# Transpose to get the right format for the  phi_transpose_nums <- t(phids[,2:11]) #convert the phi you read in as a data frame phi.df <- as.data.frame(phi_transpose_nums) # the vocabulary is in the first row vocab <- phids[,1] # give the dataframe this vocabulary as names colnames(phi.df) <- vocab # get the first row out of the data frame #phi.done.df <- phi.df[-1,] # Convert all columns from factors to numeric #phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x))) # make the theta dataset out of he transpose of the vocabulary and the 500 last rows (minus the feature ID).  NB the topic number is hard coded in her #thetads <- t(ds[, 2:31]) #thetads <- t(thetads) # Convert the vocabds into a data.frame vocab.df <- as.data.frame(vocabds) # remove the first column vocab.df <- vocab.df[, -c(1)] # realign column names with the above colnames(vocab.df) <- vocab # compute the total frequencies for each term in the corpus freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df)) # compute document lengths length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, K] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return(df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
head(thetads)
colSums(thetads)
rowSums(thetads)
rowSums(theta_imputed)
head(thetads)
head(phi.df)
vocab == "communication"
phi.df[612,]
phi.df[,612]
names(phi.df)[612]
names(phi.df)[613]
names(phi.df)[611]
phi.df[,611]
sum(phi.df[,611])
sum(phi.df[1,])
sum(phi.df[,611])
sum(phi.df[,610])
phi = phi.df
theta = thetads
doc.length = length_mat
term.frequency = freq_matrix$Freq
    dp <- dim(phi)
    dt <- dim(theta)
    N <- sum(doc.length)
    W <- length(vocab)
    D <- length(doc.length)
    K <- dt[2]
    if (dp[1] != K)
        stop("Number of rows of phi does not match \n      number of columns of theta; both should be equal to the number of topics \n      in the model.")
dim(thetads)
dim(phi.df)
theta = theta_imputed
sum(phi.df[,611])
    dp <- dim(phi)
    dt <- dim(theta)
    N <- sum(doc.length)
    W <- length(vocab)
    D <- length(doc.length)
    K <- dt[2]
    if (dp[1] != K)
        stop("Number of rows of phi does not match \n      number of columns of theta; both should be equal to the number of topics \n      in the model.")
    if (D != dt[1])
        stop("Length of doc.length not equal \n      to the number of rows in theta; both should be equal to the number of \n      documents in the data.")
    if (dp[2] != W)
        stop("Number of terms in vocabulary does \n      not match the number of columns of phi (where each row of phi is a\n      probability distribution of terms for a given topic).")
    if (length(term.frequency) != W)
        stop("Length of term.frequency \n      not equal to the number of terms in the vocabulary.")
    if (any(nchar(vocab) == 0))
        stop("One or more terms in the vocabulary\n      has zero characters -- all terms must have at least one character.")
    phi.test <- all.equal(rowSums(phi), rep(1, K), check.attributes = FALSE)
    theta.test <- all.equal(rowSums(theta), rep(1, dt[1]), check.attributes = FALSE)
    if (!isTRUE(phi.test))
        stop("Rows of phi don't all sum to 1.")
    if (!isTRUE(theta.test))
        stop("Rows of theta don't all sum to 1.")
    topic.frequency <- colSums(theta * doc.length)
    topic.proportion <- topic.frequency / sum(topic.frequency)
doc.length
doc.length[1:10]
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
library(readr)
mydata <- read_csv(filename)
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus.clean, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.52380]
summary(slam::col_sums(djreduced.dtm))
# find non-zero rows
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
djcorp.new <- dj.corpus[rowTotals > 0,]
b <- VectorSource(mydata$`Preprocessed dj_clean`)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus.clean, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
b <- VectorSource(mydata$`Preprocessed dj_clean`)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
require(tm)
library(topicmodels)
library(readr)
b <- VectorSource(mydata$`Preprocessed dj_clean`)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.050350]
summary(slam::col_sums(djreduced.dtm))
# find non-zero rows
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
djcorp.new <- dj.corpus[rowTotals > 0,]
djcorp.new
dtm.new
system.time(dj.model <- topicmodels::LDA(dtm.new, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
dj.topics <- topicmodels::topics(llis.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
# label topics
topicTerms <- tidyr::gather(dj.terms, Topic)
topicTerms <- cbind(topicTerms, Rank = rep(1:30))
topTerms <- dplyr::filter(topicTerms, Rank < 4)
topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))
topTerms$Topic <- as.numeric(topTerms$Topic)
topicLabel <- data.frame()
for (i in 1:27) {     z <- dplyr::filter(topTerms, Topic == i)     l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "), stringsAsFactors = FALSE)     topicLabel <- rbind(topicLabel, l) }
colnames(topicLabel) <- c("Label")
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(djreduced.dtm)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) }
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
dj.model
str(dj.model)
  djreduced.dtm
dj.corpus
phi <- posterior(dj.model)$terms %>% as.matrix
theta <- posterior(dj.model)$topics %>% as.matrix
dim(phi)
dim(theta)
vocab <- colnames(phi)
length(vocab)
doc_length <- vector()
for (i in 1:length(dj.corpus)) {     temp <- paste(dj.corpus[[i]]$content, collapse = ' ')     doc_length <- c(doc_length, stri_count(temp, regex = '\\S+')) }
doc_length
bar <- slam::col_sums(djreduced.dtm)
freq_matrix <- data.frame(ST = names(bar), Freq = bar)
#freq_matrix <- as.data.frame(slam::col_sums(djreduced.dtm))
dim(freq_matrix)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)
length(doc.length)
length(doc_length)
dim(theta)
dj.corpus
theta <- posterior(dj.model)$topics %>% as.matrix
dim(theta)
  djreduced.dtm
dim(phi)
str(dj.model)
dj.corpus
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, dtm.new)
dtm.new
dj.json <- topicmodels_json_ldavis(dj.model, dj.corp.new, dtm.new)
dj.json <- topicmodels_json_ldavis(dj.model, djcorp.new, dtm.new)
djcorp.new <- dj.corpus[rowTotals > 0,]
rowTotals
str(rowTotals)
length(rowTotals)
djcorp.new <- dj.corpus[rowTotals > 0,]
dj.corpus[1,1]
dj.corpus[1]
djcorp.new <- dj.corpus[rowTotals > 0]
djcorp.new
dj.json <- topicmodels_json_ldavis(dj.model, djcorp.new, dtm.new)
serVis(dj.json)
sessionInfo()
library(readr)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
library(readr)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
dim(mydata)
ss <- myddata[1:1000]''
ss <- mydata[1:1000]
ss <- mydata[1:1000,]
dim(ss)
names(ss)
colnames(ss) <- c("text")
b <- VectorSource(ss$text)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
library(tm)
b <- VectorSource(ss$text)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
          readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(docs)
dtm <- DocumentTermMatrix(dj.corpus)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.93)
dtm.sparse
library(lsa)
install.packages("lsa")
library(lsa)
# create some files 
td = tempfile()
dir.create(td)
write(c("dog", "cat", "mouse"), file = paste(td, "D1", sep = "/"))
write(c("hamster", "mouse", "sushi"), file = paste(td, "D2", sep = "/"))
write(c("dog", "monster", "monster"), file = paste(td, "D3", sep = "/"))
write(c("dog", "mouse", "dog"), file = paste(td, "D4", sep = "/"))
# read files into a document-term matrix 
myMatrix = textmatrix(td, minWordLength=1)
myMatrx
myMatrix
round(as.textmatrix(myLSAspace),2) # should give the original
myLSAspace = lsa(myMatrix, dims=dimcalc_raw())
round(as.textmatrix(myLSAspace),2) # should give the original
myLSAspace = lsa(myMatrix, dims=dimcalc_share())
myNewMatrix = as.textmatrix(myLSAspace)
myNewMatrix # should look be different!
# compare two terms with the cosine measure 
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
# compare two documents with pearson 
cor(myNewMatrix[,1], myNewMatrix[,2], method="pearson")
# clean up 
install.packages("svd")
library(svd)
myMatrix
svd(myMatrix)
foo <- svd(myMatrix)
str(foo)
foo$d*foo$u*foo$v
foo$u*foo$v
foo$u%*%foo$v
foo$d%*%foo$u%*%foo$v
foo$d%*%foo$u
foo$u%*%foo$v
dim(foo$u)
dim(foo$d)
dim(foo$v)
foo$d
diag$d
diag
diag(foo$d,1,4)
foo$d
?diag
diag(123)
diag(4)
diag(4)*foo$d
diag_d<-diag(4)*foo$d
foo$d%*%foo$u%*%foo$v
foo$u%*%foo$d%*%foo$v
foo$u %*% diag_d
foo$u %*% diag_d %*5 foo$v
foo$u %*% diag_d %*% foo$v
myMatrix
foo$d
foo$u %*% diag_d %*% foo$v
myMatrix
foo$v %*% diag_d %*% foo$u
foo$u %*% diag_d %*% foo$v
foo$u %*% diag_d
(foo$u %*% diag_d)%*% foo$v
myMatrix
(foo$u %*% diag_d)%*% t(foo$v)
(foo$u %*% diag_d)%*% t(foo$v)
myMatrix
round(foo$u %*% diag_d)%*% t(foo$v)
round((foo$u %*% diag_d)%*% t(foo$v))
myMatrix
dim(foo$u)
dim(foo$v)
d
foo$d
myMatrix
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
cosine(myMatrix["dog",], myMatrix["cat",])
foo$d
cosine(myMatrix["dog",], myMatrix["cat",])
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
cosine(myNewMatrix["dog",], myNewMatrix["sushi",])
cosine(myMatrix["dog",], myMatrix["sushi",])
library(svd)
mySVD <- svd(myMatrix)
mySVD$u %*% mySVD$d %*% mySVD$v
mySVD$u %*% mySVD$d %*% t(mySVD$v)
reconstructed <- mySVD$u %*% mySVD$d %*% t(mySVD$v)
reconstructed
mySVD$u
mySVD$d
diag(mySVD$d)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
reconstructed
round(reconstructed)
mySVD$u
mySVD$u[,1:2]
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% mySVD$v[1:2,1:2]
diag(mySVD$d[1:2])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[1:2,])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[1:2,])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[1:2,])
mySVD$u[, 1:2]
mySVD$v[1:2,]
mySVD$v[,1:2]
t(mySVD$v[,1:2])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[,1:2])
myNewMatrix
mySVD$u[, 1:2]
mySVD$v[, 1:2]
mySVD$v[,1:2]
t(mySVD$v[,1:2])
mySVD$v[,1:2]
library(graphics)
matplot((-4:5) ^ 2, main = "Quadratic") # almost identical to plot(*)
x <- 0:50 / 50 matplot(x, outer(x, 1:8, function(x, k) sin(k * pi * x)),         ylim = c(-2, 2), type = "plobcsSh",         main = "matplot(,type = \"plobcsSh\" )")
matplot(x, outer(x, 1:4, function(x, k) sin(k * pi * x)),         pch = letters[1:4], type = c("b", "p", "o"))
mySVD$v[,1:2]
x <- matrix(rnorm(1024), nrow = 32)
# simulate a correlation matrix with values -0.5 to 0.5 x <- rescale(x, c(-0.5, 0.5))
# add a column with the extreme values (-1,1) to calculate 
# the colors, then drop the extra column in the result 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
library(plotrix)
x <- matrix(rnorm(1024), nrow = 32)
# simulate a correlation matrix with values -0.5 to 0.5 x <- rescale(x, c(-0.5, 0.5))
# add a column with the extreme values (-1,1) to calculate 
# the colors, then drop the extra column in the result 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
# do the legend call separately to get the full range 
color.legend(0, -4, 10, -3, legend = c(-1, -0.5, 0, 0.5, 1), rect.col = color.scale(c(-1, -0.5, 0, 0.5, 1), c(0, 1), 0, c(1, 0)), align = "rb")
x <- matrix(rnorm(100), nrow = 10)
# generate colors that show negative values in red to brown 
# and positive in blue-green to green 
cellcol <- matrix(rep("#000000", 100), nrow = 10)
cellcol[x < 0] <- color.scale(x[x < 0], c(1, 0.8), c(0, 0.8), 0)
cellcol[x > 0] <- color.scale(x[x > 0], 0, c(0.8, 1), c(0.8, 0))
# now do hexagons without borders 
color2D.matplot(x, cellcolors = cellcol, xlab = "Columns", ylab = "Rows", do.hex = TRUE, main = "2D matrix plot (hexagons)", border = NA)
# for this one, we have to do the color legend separately 
# because of the two part color scaling 
legval <- seq(min(x), max(x), length.out = 6)
legcol <- rep("#000000", 6)
legcol[legval < 0] <- color.scale(legval[legval < 0], c(1, 0.8), c(0, 0.8), 0)
legcol[legval > 0] <- color.scale(legval[legval > 0], 0, c(0.8, 1), c(0.8, 0) color.legend(0, -1.8, 3, -1.4, round(c(min(x), 0, max(x)), 1), rect.col = legcol)
legcol[legval < 0] <- color.scale(legval[legval < 0], c(1, 0.8), c(0, 0.8), 0)
legcol[legval > 0] <- color.scale(legval[legval > 0], 0, c(0.8, 1), c(0.8, 0) )
color.legend(0, -1.8, 3, -1.4, round(c(min(x), 0, max(x)), 1), rect.col = legcol)
# do a color only association plot 
xt <- table(sample(1:10, 100, TRUE), sample(1:10, 100, TRUE))
observed <- xt[, rev(1:dim(xt)[2])]
expected <- outer(rowSums(observed), colSums(observed), "*") / sum(xt)
deviates <- (observed - expected) / sqrt(expected)
cellcol <- matrix(rep("#000000", 100), nrow = 10)
cellcol[deviates < 0] < color.scale(deviates[deviates < 0], c(1, 0.8), c(0, 0.5), 0)
cellcol[deviates > 0] < color.scale(deviates[deviates > 0], 0, c(0.7, 0.8), c(0.5, 0))
color2D.matplot(x = round(deviates, 2), cellcolors = cellcol, show.values = TRUE, main = "Association plot")
# Hinton diagram 
border.col <- color.scale(x, extremes = 2:3)
color2D.matplot(x, extremes = c(2, 3), main = "Hinton diagram (green +, red -)", Hinton = TRUE, border = border.col)
# waffle plot of percentages with two contributing elements 
waffle.col <- fill.corner(c(rep("red", 18), rep("blue", 45)), 10, 10)
color2D.matplot(matrix(1:100,nrow=10),cellcolors=waffle.col,yrev=FALSE, border="lightgray",xlab="",ylab="",main="Waffle plot",axes=FALSE)
color2D.matplot(x, cellcolors = cellcol, xlab = "Columns", ylab = "Rows", do.hex = TRUE, main = "2D matrix plot (hexagons)", border = NA)
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
x <- matrix(rnorm(1024), nrow = 32)
# simulate a correlation matrix with values -0.5 to 0.5 x <- rescale(x, c(-0.5, 0.5))
# add a column with the extreme values (-1,1) to calculate 
# the colors, then drop the extra column in the result 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
mySVD$v
color2D.matplot(mySVD$v)
color2D.matplot(mySVD$v, cellcolors = cellcol)
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol)
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[,1:2]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[,1:2]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol, main = "Document relatedness")color2D.matplot(mySVD$v[, 1:3], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(mySVD$v[, 1:3], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:3]), cellcolors = cellcol, main = "Document relatedness")
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
mySVD$u[, 1:dimensions]
wm <- mySVD$u[, 1:dimensions]
plot(wm[,1], wm[,2])
plot(wm[,1], wm[,2], col = wm[,3])
plot(wm[,1], wm[,2])
wm[,3]
plot(wm[,1], wm[,2], col = wm[,3], pchi="x")
plot(wm[,1], wm[,2], col = wm[,3])
plot(wm[,1], wm[,2], col = wm[,3])
?plot
plot(wm[,1], wm[,2], col = wm[,3], type = p)
plot(wm[,1], wm[,2], col = wm[,3], type = "p")
plot(wm[,1], wm[,2],  type = "p")
library(ggplot2)
qplot(data = wm, x = wm[,1], y=wm[,2])
wmdf <- as.data.frame(wm)
names(wmdf)
qplot(data = wmdf, x = V1, y=V2)
qplot(data = wmdf, x = V1, y=V2, color = V3)
qplot(data = wmdf, x = V1, y = V2, color = V3)
ggplot(data = wmdf, x = V1, y = V2, color = V3)
ggplot
plotwords <- ggplot(data = wmdf, x = V1, y = V2, color = V3)
plotwords
rainbow <- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)
ggplot(mtcars, aes(mpg, wt)) + geom_point() + annotation_raster(rainbow, 15, 20, 3, 4)
plotwords <- ggplot(data = wmdf, x = V1, y = V2, color = V3)
geom_point()
plotwords + geom_point()
plotwords <- ggplot(data = wmdf, x = V1, y = V2, color = V3)
plotwords + geom_point()
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point()
rainbow <- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)
ggplot(mtcars, aes(mpg, wt)) + geom_point() + annotation_raster(rainbow, 15, 20, 3, 4)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point()
myMatrix
colnames(myMatrix)
rownames(myMatrix)
words <- rownames(myMatrix)
plotwords + geom_point() + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point() + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point() + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords +  geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point()
geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
wmdf$V3
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V2)
plotwords <- plotwords  + geom_point()
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V2)
plotwords <- plotwords  + geom_point()
color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V2)
plotwords <- plotwords  + geom_point()
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
wmdf
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp
bp <- ggplot(ToothGrowth, aes(x = dose, y = len, fill = dose)) +   geom_boxplot()
bp
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp
sp + scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9"))
sp + scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9"))
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9"))
bp
td = tempfile()
dir.create(td)
write(c("dog", "cat", "mouse"), file = paste(td, "D1", sep = "/"))
write(c("hamster", "mouse", "sushi"), file = paste(td, "D2", sep = "/"))
write(c("dog", "monster", "monster"), file = paste(td, "D3", sep = "/"))
write(c("dog", "mouse", "dog"), file = paste(td, "D4", sep = "/"))
# read files into a document-term matrix 
myMatrix = textmatrix(td, minWordLength=1)
library(lsa)
myMatrix = textmatrix(td, minWordLength=1)
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed)
# how many dimensions?
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
library(ggplot2)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_brewer(palette = "Dark2")
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
words <- rownames(wmdf)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
words
words <- rownames(myMatrx)
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
rainbow <- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)
ggplot(mtcars, aes(mpg, wt)) + geom_point() + annotation_raster(rainbow, 15, 20, 3, 4)
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_brewer(palette = "Dark2")
typeof(wmdf)
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_continuous(values = c("#999999", "#E69F00", "#56B4E9"))
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_brewer(palette = "Dark2")
sp + scale_color_continuous(palette = "Dark2")
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
t(mySVD$v[, 1:dimensions])
diag(mySVD$d[, 1:dimensions])
mySVD$d[1:dimensions]
mySVD$v[, 1:dimensions]
t(mySVD$v[, 1:dimensions])
max(t(mySVD$v[, 1:dimensions]))
max(t(mySVD$v[, 1:dimensions])[1,])
max(t(mySVD$v[, 1:dimensions])[2,])
max(t(mySVD$v[, 1:dimensions])[3,])
max(t(mySVD$v[, 1:dimensions])[2,])
max(t(mySVD$v[, 1:dimensions])[3,])
max(t(mySVD$v[, 1:dimensions])[3,])
argmax(t(mySVD$v[, 1:dimensions])[1,])
max(t(mySVD$v[, 1:dimensions])[3,])
t(mySVD$v[, 1:dimensions])[3,]
myMatrix
t(mySVD$v[, 1:dimensions])[3,]
t(mySVD$v[, 1:dimensions])[1,]
max(t(mySVD$v[, 1:dimensions])[1,])
max(t(mySVD$v[, 1:dimensions])[2,])
t(mySVD$v[, 1:dimensions])[2,]
t(mySVD$v[, 1:dimensions])[3,]
dtm
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
library(readr)
library(tm)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
exdoc <- mydata[1]
exdoc
exdoc <- mydata[1,]
exdoc
names(mydata)
colnames(mydata)
colnames(mydata) <- c("text")
exdoc <- mydata[1,]$text
exdoc
myMatrix
td
textmatrix(exdoc <- "It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.  However, it was sold with falsehoods and is now being mismanaged.  To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market; and Nigel Farage predicted that other countries would follow Britain out of Europe.  It hasn ’t quite turned out like that.  More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when.  The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis.  Like both previous disasters, Brexit reveals three enduring flaws in the UK ’s workings.")
exdoc
textmatrix(exdoc)
c <- VectorSource(exdoc)
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(ft.corpus)
dtm
c("foo", "bar")
c <- VectorSource(c("foo", "bar"))
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
ft.corpus
dtm <- DocumentTermMatrix(ft.corpus)
dtm
exdoc <- c("It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.", "However, it was sold with falsehoods and is now being mismanaged. ", "To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market;", "and Nigel Farage predicted that other countries would follow Britain out of Europe. ", "It hasn ’t quite turned out like that. ", "More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when. ", "The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis. ", "Like both previous disasters, Brexit reveals three enduring flaws in the UK ’s workings.")dtm
exdoc <- c("It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.", "However, it was sold with falsehoods and is now being mismanaged. ", "To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market;", "and Nigel Farage predicted that other countries would follow Britain out of Europe. ", "It hasn ’t quite turned out like that. ", "More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when. ", "The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis. ", "Like both previous disasters, Brexit reveals three enduring flaws in the UK s workings.")
c <- VectorSource(exdoc)
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(ft.corpus)
dtm
as.matrix(dtm)
ftmat <- as.matrix(dtm)
lsa(ftmat, dims = dimcalc_raw())
myMatrix <- ftmat
# replicating the lsa with the svd library
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed)
round(reconstructed) == myMatrix
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
words
myMatrix
rownames(myMatrix)
colnames(myMatrix)
tdm <- TermDocumentMatrix(ft.corpus)
ftmat <- as.matrix(tdm)
myMatrix <- ftmat
# replicating the lsa with the svd library
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed) == myMatrix
# how many dimensions?
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
max(t(mySVD$v[, 1:dimensions])[2,])
max(t(mySVD$v[, 1:dimensions])[1,])
which.max(t(mySVD$v[, 1:dimensions])[1,])
which.max(t(mySVD$v[, 1:dimensions])[1,])
exdoc[which.max(t(mySVD$v[, 1:dimensions])[1,])]
# extract the most prominent sentences
for (i in 1:dimensions) {     exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])] }
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
tdm
ftmat
ftmat[row == "what"]
dim(ftmat)
rownames(ftmat)
rownames(ftmat) == "David"
which(rownames(ftmat) == "david")
which(rownames(ftmat) == "state")
which(rownames(ftmat) == "daniel")
cosine(mySVD[18,], mySVD[19,])
cosine(mySVD$u[18,], mySVD$u[19,])
cosine(mySVD$u[18,], mySVD$u[19,])
dim(mySVD$u)
mySVD$u[18,]
mySVD$u[19,]
mySVD$u[20,]
mySVD$u[27,]
which(rownames(ftmat) == "hannan")
cosine(mySVD$u[18,], mySVD$u[32,])
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
dimensions <- 1
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
# extract the most prominent sentences
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine()
dimensions <- 8
# extract the most prominent sentences
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine()
dimensions <- 4
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cbind(x, c(-1, rep(1, 31)))
cbind(t(mySVD$v[, 1:dimensions]), c(-1, rep(1, 31)))
c(-1, rep(1, 31))
c(-1, rep(1, 31))
c(1, rep(1, 31))
c(-1, rep(1, 31))
cbind(t(mySVD$v[, 1:dimensions]), c(-1, rep(1, 31)))
x <- matrix(rnorm(1024), nrow = 32)
x <- rescale(x, c(-0.5, 0.5))
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
dim(cellcol)
concepts <- t(mySVD$v[, 1:dimensions])
dim(concepts)
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
cbind(x, c(-1, rep(1, 31)))
dim(cbind(x, c(-1, rep(1, 31))))
dim(x)
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))
dim(cellcol)
cellcol <- color.scale(cbind(concepts, c(-1, rep(1, 7))), c(0, 1), 0, c(1, 0))
cbind(concepts, c(-1, rep(1, 7)) )
cbind(concepts, c(-1, rep(1, 7)))
dim(concepts)
cbind(concepts, c(-1, rep(1, 8)))
cbind(concepts, c(-1, rep(1, 7)))
cbind(concepts, c(-1, rep(1, 6)))
c(-1, rep(1, 6)))
c(-1, rep(1, 6))
c(-1, rep(1, 7))
dim(c(-1, rep(1, 7)))
length(c(-1, rep(1, 7)))
?cbind
typeof(x)
f(concepts)
library(readr)
library(tm)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
exdoc <- c("It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.", "However, it was sold with falsehoods and is now being mismanaged. ", "To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market;", "and Nigel Farage predicted that other countries would follow Britain out of Europe. ", "It hasn ’t quite turned out like that. ", "More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when. ", "The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis. ", "Like both previous disasters, Brexit reveals three enduring flaws in the UK s workings.")
c <- VectorSource(exdoc)
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
b <- VectorSource(ss$text)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(ft.corpus)
tdm <- TermDocumentMatrix(ft.corpus)
ftmat <- as.matrix(tdm)
myMatrix <- ftmat
# replicating the lsa with the svd library
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed) == myMatrix
# how many dimensions?
dimensions <- 4
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
concepts <- t(mySVD$v[, 1:dimensions])
color2D.matplot(concepts), main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
# extract the most prominent sentences
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine()
color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
cbind(concepts, c(-1, rep(1, 31)))
cbind(concepts, c(-1, rep(1, 7)))
cbind(concepts, c(-1, rep(1, 4)))
cbind(concepts, c(-1, rep(1, 3)))
cellcol <- color.scale(cbind(x, c(-1, rep(1, d[1]-1))), c(0, 1), 0, c(1, 0))[, 1:d[1]]
cellcol <- color.scale(cbind(concepts, c(-1, rep(1, d[1]-1))), c(0, 1), 0, c(1, 0))[, 1:d[1]]
d <- dim(concepts)
cellcol <- color.scale(cbind(concepts, c(-1, rep(1, d[1]-1))), c(0, 1), 0, c(1, 0))[, 1:d[1]]
color2D.matplot(concepts), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(concepts, cellcolors = cellcol, main = "Document relatedness")
setwd("C:\\Users\\bungum\\Documents")
filename <- "all3.csv"
foo <- read.csv(filename)
setwd("C:\\Users\\bungum\\Documents")
filename <- "all3.csv"
system.time(mydf <- read_csv(filename, stringsAsFactors = FALSE))
system.time(mydf <- read.csv(filename, stringsAsFactors = FALSE))
str(foo)
str(mydf)
str(mydf)
dim(mydf)
ss <- mydf[1:100,]
save(ss, "foo.RData")
save(ss, file = c("foo.RData"))
getwd()
800/5
160*2
320/60
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
names(mydf)
names(mydf)[20:1202]
system.time(full_mldr <- mldr_from_dataframe(mydf, labelIndices = 20:1202))
library(mldr)
system.time(full_mldr <- mldr_from_dataframe(mydf, labelIndices = 20:1202))
install_github('ramnathv/slidify')
install.packages("devtools")
library("devtools")
install_github('ramnathv/slidify')
install_github('ramnathv/slidifyLibraries')
author("mydeck")
library(slidify)
author("mydeck")
slidify('index.Rmd')
--- title: "my presentation" subtitle: author: LB job: framework:io2012 # {html5slides}   highlighter:highlight.js # {highlight.jst}   hitheme:tomorrow #    widgets:[] # {mathjax, quiz, bootstrap}   mode:selfcontained # {standalone}   knit:slidify::knit2slides --- slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
names(mldr)
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
names(mydf)
mydf$replace
mydf[1,15]$replace
mydf[1,15]
strwrap(mydf[1,15])
nchar(mydf[1,15])
nchar(mydf[1:25,15])
strwrap(mydf[8,15])
mydf[8,15]$topics
mydf[8,15]$topic_cat
mydf[8,15]$topics
mydf[8,]$topics
mydf[8,]$topics_cat
mydf[8,]$topic_cat
names(mydf[8,])[1:20]
mydf[8,]$topic
mydf[8,]$money
strwrap(mydf[8,15])
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
library(slidify)
slidify('index.Rmd')
slidify('index.Rmd')
getwd()
setwd("c:\\users\\bungum\\documents\\mydeck")
slidify('index.Rmd')
strwrap(mydf[8,15])
slidify('index.Rmd')
slidify('index.Rmd')
mydf[8,]$money
slidify('index.Rmd')
strwrap(mydf[8,15])
rm(mydf)
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
system.time(full_mldr <- mldr_from_dataframe(mydf[, 20:length(colnames(mydf))], labelIndices = 1:(length(colnames(mydf)) - 20)))
slidify('index.Rmd')
setwd("C:\\Users\\bungum\\Documents\\mydeck")
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
install.packages("evaluate")
slidify('index.Rmd')
library(evaluate)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
evaluate_call
help(evaluate)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
sessionInfo()
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
slidify('index.Rmd')
packageurl <- "http://cran.r-project.org/src/contrib/Archive/knitr/knitr_1.12.tar.gz"
install.packages(packageurl, repos = NULL, type = "source")
install.packages(knitr)
install.packages("knitr")
slidify('index.Rmd')
slidify('index.Rmd')
package_df <- as.data.frame(installed.packages("/Library/Frameworks/R.framework/Versions/2.15/Resources/library"))
package_list <- as.character(package_df$Package)
system.time(install.packages(package_list))
package_list
package_list <- as.character(package_df$Package)
package_list
package_df <- as.data.frame(installed.packages())
package_list <- as.character(package_df$Package)
package_list
system.time(install.packages(package_list))
slidify('index.Rmd')
getwd()
author("mydeck2")
getwd()
slidify('index.Rmd')
slidify('index.Rmd')
require(tm)
b <- VectorSource(mydata$replace)
b <- VectorSource(mydf$replace)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
system.time(djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE)))
10849/3600
djtopic.dtm
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
summary(term_tfidf)[3]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal]
medianVal <- summary(term_tfidf)[3]
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
removeSparseTerms(djreduced.dtm, .99)
?frequency
findFreqTerms(djreduced.dtm, 100)
head(sort(rowSums(as.matrix(dj.reduced.dtm)), decreasing = T), 100)
head(sort(rowSums(as.matrix(djreduced.dtm)), decreasing = T), 100)
?findFreqTerms
head(sort(rowSums(as.matrix(djreduced.dtm)), decreasing = T), 10000)
findFreqTerms(djreduced.dtm, 10000)
findFreqTerms(djreduced.dtm, 20000)
?removeSparseTerms
removeSparseTerms(djreduced.dtm, .5)
removeSparseTerms(djreduced.dtm, .9)
removeSparseTerms(djreduced.dtm, .95)
removeSparseTerms(djreduced.dtm, .975)
removeSparseTerms(djreduced.dtm, .985)
removeSparseTerms(djreduced.dtm, .995)
djreduced.dtm
removeSparseTerms(djreduced.dtm, .999)
djtopic.dtm[, term_tfidf >= medianVal*2]
djtopic.dtm[, term_tfidf >= medianVal*2.5]
djtopic.dtm[, term_tfidf >= medianVal*3]
djtopic.dtm[, term_tfidf >= medianVal*4]
djtopic.dtm[, term_tfidf >= medianVal*5]
djtopic.dtm[, term_tfidf >= medianVal*6]
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*6]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
rowSums(dj.reduced.dt)
rowSums(djreduced.dtm)
djreduced.dtm
?apply
dim(djreduced.dtm)
rowSums(djreduced.dtm)
rowSums(as.matrix(djreduced.dtm))
as.matrix(djreduced.dtm)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*7]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
as.matrix(djreduced.dtm)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*8]
djreduced.dtm
as.matrix(djreduced.dtm)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*10]
as.matrix(djreduced.dtm)
djreduced.dtm
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*12]
djreduced.dtm
as.matrix(djreduced.dtm)
1372718 * 21912
1372718 * 21912*8
1024*1024
1024*1024*1024
1024*1024*1024*224
1372718 * 21912*8
library(text2vec)
install.packages("text2vec")
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$id,              progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab = create_vocabulary(it_train)
vocab
train_tokens = train$review %>%   prep_fun %>%   tok_fun
train_tokens
it_train = itoken(train_tokens,                   ids = train$id, # turn off progressbar because it won't look nice in rmd                   progressbar = T)
vocab = create_vocabulary(it_train)
vocab
it_train = itoken(train$review,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$id,              progressbar = FALSE) vocab1 = create_vocabulary(it_train)
vocab == voca|1
vocab == vocab1
vectorizer = vocab_vectorizer(vocab)
vectorizer
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dtm_train
dim(dtm_train)
library(glmnet) NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],                               family = 'binomial', # L1 penalty                               alpha = 1, # interested in the area under ROC curve                               type.measure = "auc", # 5-fold cross-validation                               nfolds = NFOLDS, # high value is less accurate, but has faster training                               thresh = 1e-3, # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
install.packages("glmnet")
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
library(glmnet)
NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
typeof(train)
train = mydf[1:100,]$replace
train = mydf[1:100,]
names(mydf)
names(mydf)[1:20]
it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab1 = create_vocabulary(it_train)
it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
str(dtm_train)
rownames(dtm_train)
colnames(dtm_train)
dtm_train[1,1]
dtm_train[1,]$turn
dtm_train[1,7912]
dtm_train[,7912]
library(glmnet) NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
train = mydf[1:10000,] # define preprocessing function and tokenization function # create vocabulary with a memory friendly way (lazy function?) prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab = create_vocabulary(it_train)
vocab
vectorizer = vocab_vectorizer(vocab) t1 = Sys.time() dtm_train = create_dtm(it_train, vectorizer) print(difftime(Sys.time(), t1, units = 'sec'))
library(glmnet) NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec')) plot(glmnet_classifier)
dim(dtm_train)
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))
t1 = Sys.time()
dtm_train = create_dtm(it_train, h_vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                              family = 'binomial',                              alpha = 1,                              type.measure = "auc",                              nfolds = 5,                              thresh = 1e-3,                              maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
train = mydf[1:100000,]
it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab = create_vocabulary(it_train)
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L)) t1 = Sys.time() dtm_train = create_dtm(it_train, h_vectorizer) print(difftime(Sys.time(), t1, units = 'sec'))
t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                              family = 'binomial',                              alpha = 1,                              type.measure = "auc",                              nfolds = 5,                              thresh = 1e-3,                              maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec')) plot(glmnet_classifier)
train = mydf[1:100000,]
train = mydf[,]
t1 = Sys.time()
vocab = create_vocabulary(it_train)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(vocab)
length(vocab)
vocab
train = mydf
prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) t1 = Sys.time() vocab = create_vocabulary(it_train) print(difftime(Sys.time(), t1, units = 'sec'))
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))
t1 = Sys.time()
dtm_train = create_dtm(it_train, h_vectorizer)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
library(mldr)
system.time(load("mydf_full_columns_fixed.RData"))
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
library(mldr)
install.packages("XML")
library(mldr)
install.packages("mldr")
library(mldr)
install.packages("colorspace")
library(mldr)
install.packages("digest")
library(mldr)
install.packages("htmltools")
library(mldr)
install.packages("httpuv")
library(mldr)
install.packages("mime")
library(mldr)
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
mylabels
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
names(mydf)
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
colnames(mydf) == "FCL"
sum(colnames(mydf) == "FCL")
system.time(load("fullset_industry_topic_country_in_columns.RData"))
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
colnames(mydf)
sort(colnames(mydf))
train = mydf # define preprocessing function and tokenization function # create vocabulary with a memory friendly way (lazy function?) prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = TRUE) t1 = Sys.time() vocab = create_vocabulary(it_train) print(difftime(Sys.time(), t1, units = 'sec'))
library(text2vec)
library(data.table)
train = mydf # define preprocessing function and tokenization function # create vocabulary with a memory friendly way (lazy function?) prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = TRUE) t1 = Sys.time() vocab = create_vocabulary(it_train) print(difftime(Sys.time(), t1, units = 'sec'))
# feature hashing  h_vectorizer = hash_vectorizer(hash_size = 2^14, ngram = c(1L, 2L)) t1 = Sys.time() dtm_train = create_dtm(it_train, h_vectorizer) print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_matrix)
dim(dtm_train)
system.time(save(dtm_train, "dtm_train_1372718x16384.RData"))
system.time(save("dtm_train_1372718x16384.RData", dtm_train))
system.time(save(dtm_train, file = "dtm_train_1372718x16384.RData"))
ss <- dtm_train[1:1000,]
dim(ss)
library(caret)
install.packages("stringi")
library(caret)
ctrl <- trainControl(method = "cv", savePred = T, classProb = T)
mod <- train(Species ~ ., data = ss[1:800], method = "svmLinear", trControl = ctrl)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
system.time(load("dtm_train_1372718x16384.RData"))
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
library(mldr)
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
colnames(mydf)
colnames(mydf)=="CYC"
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RTWS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
library(doParallel);
library(caret)
#create a list of seed, here change the seed for each resampling
set.seed(123)
#length is = (n_repeats*nresampling)+1
seeds <- vector(mode = "list", length = 11)
#(3 is the number of tuning parameter, mtry for rf, here equal to ncol(iris)-2)
for (i in 1:10)
    seeds[[i]] <- sample.int(n = 1000, 3)
#for the last model
seeds[[11]] <- sample.int(1000, 1)
#control list
myControl <- trainControl(method = 'cv', seeds = seeds, index = createFolds(iris$Species))
#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)
model1 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
model2 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
stopCluster(cl)
all.equal(predict(model1, type = 'prob'), predict(model2, type = 'prob'))
ssmdf <- mydf[1:1000,]
ss <- dtm_train[1:1000,]
ndf <- cbind(ssm$CYC, ss)
ndf <- cbind(ssmdf$CYC, ss)
colnames(ndf)[1:10]
colnames(ndf)
dim(ss)
dim(ssmdf)
dim(ssmdf$cyc)
dim(ssmdf$CYC)
length(ssmdf$CYC)
dim(ndf)
colnames(ndf)[1] <- "CYC"
colnames(ndf)[1]
ndf <- cbind(ssmdf$CYC, as.matrix(ss))
colnames(ndf)[1]
ndf <- data.frame(cbind(ssmdf$CYC, as.matrix(ss)))
colnames(ndf)[1]
colnames(ndf)[1] <- "CYC"
dim(ndf)
ndf$CYC
ifelse(ndf$CYC==T, "YES", "NO")
ndf$CYC <- ifelse(ndf$CYC == T, "YES", "NO")ifelse(ndf$CYC==T, "YES", "NO")
ndf$CYC <- ifelse(ndf$CYC == T, "YES", "NO")
library(doParallel);
library(caret)
#create a list of seed, here change the seed for each resampling
set.seed(123)
#length is = (n_repeats*nresampling)+1
seeds <- vector(mode = "list", length = 11)
#(3 is the number of tuning parameter, mtry for rf, here equal to ncol(iris)-2)
for (i in 1:10)
    seeds[[i]] <- sample.int(n = 1000, 3)
#for the last model
seeds[[11]] <- sample.int(1000, 1)
#control list
myControl <- trainControl(method = 'cv', seeds = seeds, index = createFolds(iris$Species))
#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)
model1 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
model2 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
stopCluster(cl)
all.equal(predict(model1, type = 'prob'), predict(model2, type = 'prob'))
ndfCtrl <- trainControl(method = 'cv', seeds = seeds, index = createFolds(ndf$CYC))
#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)
model3 <- train(CYC ~ ., data=ndf, method = 'rf', trControl = ndfCtrl)
remove(dtm_train)
remove(mydf)
stopCluster(cl)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
stopCluster(cl)
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
model3 <- train(CYC ~ ., data=ndf, method = 'rf', trControl = ndfCtrl)
system.time(load("mydf_full_columns_fixed.RData"))
names(mydf)
mydf[1]$replace
mydf[1]$replace
mydf[1,]$replace
mydf[1,]$replace
mydf[1,15]
strwrap(mydf[1,15],100)
nchar(strwrap(mydf[1,15],100))
substr(mydf[1,15],70, 98)
substr(mydf[1,1],70, 98)
nchar(mydf[1:10,15])
substr(mydf[1,15],5600,5694)
substr(mydf[2,15],5600,5694)
nchar(mydf[1:10,15])
substr(mydf[2,15],4950,5002)
library(stringr)
str_sub(mydf[1:10,15], -2, -1)
str_sub(mydf[1:10,15], -20, -1)
str_sub(mydf[1:10,15], -50, -1)
str_sub(mydf[1:10,15], -40, -1)
str_sub(mydf[1:10,15], -40, -2)
str_sub(mydf[1:1,15], -40, -2)
str_sub(mydf[1:1,15], -40, -20)
?str_sub
str_sub(mydf[1:1,15], -40, -10)
str_sub(mydf[1:1,15], -40, -3)
str_sub(mydf[1:1,15], -40, -4)
str_sub(mydf[1:1,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -5)
foo <- "March 22, 2017 04:06 ET (08:06 GMT)"
as.date(foo)
as.Date(foo)
install.packages("parsedate")
library(parsedate)
parsedate(foo)
?parsedate
library(parsedate)
??parsedate
parse_date(foo)
foo
str_sub(mydf[1:5,15], -40, -5)
bar <- str_sub(mydf[1:5,15], -40, -5)
parse_date(bar)
str(parse_date(bar))
parse_date(str_sub(mydf[1:5,15], -40, -5))
parse_date(str_sub(mydf[1:5,15], -50, -5))
parse_date(str_sub(mydf[1:50,15], -50, -5))
parse_date(str_sub(mydf[1:150,15], -50, -5))
system.time(mydf$date <- parse_date(str_sub(mydf[,15], -50, -5)))
str_sub(mydf[125:150,15], -50, -5)
length(mydf$date)
length(mydf$date)
library(readr)
library(dplyr)
library(tm)
library(LDAvis)
# Function: Visualize LDA model (create JSON object) topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(doc_term)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) } create_dtm_from_isin <- function(isin_selection, dj.select) {     # load the bigram tokenizer     library(NLP)     BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)     # create a dtm     djtopic.dtm <- tm::DocumentTermMatrix(dj.select,     control = list(     #stemming = TRUE,             stopwords = TRUE,             minWordLength = 2,             removeNumbers = TRUE,             removePunctuation = TRUE             )             )     return(djtopic.dtm) } remove_sparse_terms <- function(djtopic.dtm) {     # sum up the dtm rows to gather aggregated counts     term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i],     djtopic.dtm$j, mean) * log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))     median <- summary(term_tfidf)[3]     # only include terms above the median     djreduced.dtm <- djtopic.dtm[, term_tfidf >= median]     summary(slam::col_sums(djreduced.dtm))     return(djreduced.dtm) } attribute_topics_to_docs <- function(dj.model, dj.topics) {     # Creates a dataframe to store the News Number and the most likely topic     doctopics.df <- as.data.frame(dj.topics)     doctopics.df <- dplyr::transmute(doctopics.df,     #NewsId = rownames(doctopics.df),         NewsId = isin_selection,         Topic = dj.topics)     doctopics.df$NewsId <- as.integer(doctopics.df$NewsId)     return(doctopics.df) } label_topics <- function(dj.terms) {     # label topics     topicTerms <- tidyr::gather(dj.terms, Topic)     topicTerms <- cbind(topicTerms, Rank = rep(1:30))     topTerms <- dplyr::filter(topicTerms, Rank < 4)     topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))     topTerms$Topic <- as.numeric(topTerms$Topic)     topicLabel <- data.frame()     for (i in 1:topicNumber) {         z <- dplyr::filter(topTerms, Topic == i)         l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "),             stringsAsFactors = FALSE)         topicLabel <- rbind(topicLabel, l)     }     colnames(topicLabel) <- c("Label")     return(topicLabel) } correlate_topics <- function(isin_selection, dj.model) {     baz <- strsplit(mydf[isin_selection,]$topic, ",")     seltops <- unlist(lapply(baz, `[[`, 1))     # correlate topics     theta <- as.data.frame(topicmodels::posterior(dj.model)$topics)     head(theta[1:5])     phi <- as.data.frame(topicmodels::posterior(dj.model)$terms)     head(phi[1])     x <- as.data.frame(row.names(theta), stringsAsFactors = FALSE)     colnames(x) <- c("NewsId")     x$NewsId <- as.numeric(x$NewsId)     theta2 <- cbind(x, theta)     theta2 <- dplyr::left_join(theta2, FirstCategorybyLesson, by = "NewsId")     ## Returns column means grouped by catergory     theta.mean.by <- by(theta2[, 2:28], theta2$Category, colMeans)     theta.mean <- do.call("rbind", theta.mean.by)     library(corrplot)     c <- cor(theta.mean)     corrplot(c, method = "circle")     return(theta.mean) } most_diagnostic_topics <- function(theta.mean) {     # most diagnostic topics     theta.mean.ratios <- theta.mean     for (ii in 1:nrow(theta.mean)) {         for (jj in 1:ncol(theta.mean)) {             theta.mean.ratios[ii, jj] <-             theta.mean[ii, jj] / sum(theta.mean[ii, - jj])         }     }     topics.by.ratio <- apply(theta.mean.ratios, 1,         function(x) sort(x, decreasing = TRUE, index.return = TRUE)$ix)     # The most diagnostic topics per category are found in the theta 1st row of the index matrix:     topics.most.diagnostic <- topics.by.ratio[1,]     head(topics.most.diagnostic) } find_optimal_topic_number <- function(dj.red.dtm) {     library(Rmpfr)     harmonicMean <- function(logLikelihoods, precision = 2000L) {         llMed <- median(logLikelihoods)         as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,                                        prec = precision) + llMed))))     }     k <- 25     burnin <- 1000     iter <- 1000     keep <- 50     fitted <- topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))     ## assuming that burnin is a multiple of keep     logLiks <- fitted@logLiks[-c(1:(burnin / keep))]     ## This returns the harmomnic mean for k = 25 topics.     harmonicMean(logLiks)     seqk <- seq(2, 100, 1)     burnin <- 1000     iter <- 1000     keep <- 50     system.time(fitted_many <- lapply(seqk, function(k) topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))))     cl <- makePSOCKcluster(8)     setDefaultCluster(cl)     #adder <- function(a, b) a + b     #clusterExport(NULL, c('adder'))     #parLapply(NULL, 1:8, function(z) adder(z, 100))     clusterExport(cl, c('LDA', 'dj.red.dtm', 'burnin', 'iter', 'keep'))     parLapply(cl, seqk, function(k)         LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep)))     # extract logliks from each topic     logLiks_many <- lapply(fitted_many, function(L) L@logLiks[-c(1:(burnin / keep))])     # compute harmonic means     hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))     ldaplot <- ggplot(data.frame(seqk, hm_many), aes(x = seqk, y = hm_many)) + geom_path(lwd = 1.5) +     theme(text = element_text(family = NULL),         axis.title.y = element_text(vjust = 1, size = 16),         axis.title.x = element_text(vjust = -.5, size = 16),         axis.text = element_text(size = 16),         plot.title = element_text(size = 20)) +     xlab('Number of Topics') +     ylab('Harmonic Mean') +      annotate("text", x = 25, y = -80000, label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +     ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of DJ Newswire", atop(italic("How many distinct topics in the abstracts?"), ""))))     return(ldaplot) } toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) { stop <- nchar(x); substring(x, start, stop) }) tokenize_corp <- function(corpus) {     newcorp <- corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Email.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Write.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(MORE TO FOLLOW.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(END\\).*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r") %>%     tm_map(toSpace, "\"")     #tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)")     #tm_map(replacener)     return(newcorp) }
b <- VectorSource(mydf$replace)
#dj.corpus <- Corpus(b, 
readerControl = list(language = "eng", reader = readPlain))
readerControl = list(language = "eng", reader = readPlain))
dj.corpus <- VCorpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
vw <- "DE0007664039"
stl <- "NO0010096985"
ibm <- "US4592001014"
tesla <- "US88160R1014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG)
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 50
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
mydf[1:10,]$date
mydf[1:10,]$date > 2017-03
mydf[1:10,]$date > 2017-03-01
mydf[1:10,]$date > as.Date(2017-03-01)
mydf[1:10,]$date > as.Date("2017-03-01")
str(mydf$date)
mydf[1,]$date > mydf[2,]$date
mydf[1,]$date < mydf[2,]$date
as.Date.date
?as.Date
as.Date("2017-03-1", "%y/%m/%d")
as.Date("2017-03-1", "%Y/%m/%d")
as.Date("2017-03-1", "%yy/%m/%d")
format(Sys.Date(), "%a %b %d")
format(Sys.Date(), "%a %b %d %y %m %d")
format(Sys.Date(), "%a %b %d %y %m %d %Y")
as.Date("2017-03-1", "%Y-%m-%d")
mydf[1:19]$date > as.Date("2017-03-1", "%Y-%m-%d")
mydf[1:19]$date > as.Date("2017-03-01", "%Y-%m-%d")
mydf[1:19]$date 
mydf[1:19,]$date > as.Date("2017-03-01", "%Y-%m-%d")
?as.Date
mydf[1:19,]$date > parse_date("2017-03-01")
str(mydf[1:19,]$date)
as.Date(mydf[1:19,]$date)
as.Date(mydf[1:19,]$date) > "2017-03-01"
system.time(mydf$date <- as.Date(parse_date(str_sub(mydf[, 15], -50, -5))))
system.time(mydf$date <- parse_date(str_sub(mydf[, 15], -50, -5)))
length(mydf$date)
str(mydf$date)
system.time(mydf$date <- parse_date(str_sub(mydf[, 15], -40, -5)))
names(mydf)[1:20]
names(mydf)[1:15]
system.time(mydf$date <- parse_date(str_sub(mydf$replace, -40, -5)))
parse_date(str_sub(mydf[1:100,]$replace, -40, -5))
head(mydf)
system.time(load("mydf_full_columns_fixed.RData"))
parse_date(str_sub(mydf[1:100, 15], -50, -5)))
parse_date(str_sub(mydf[1:100, 15], -50, -5))
str_sub(mydf[1:100, 15], -50, -5)
parse_date(str_sub(mydf[1:100, 15], -50, -5))
parse_date(str_sub(mydf[1:10, 15], -50, -5))
library(parsedate)
parse_date(str_sub(mydf[1:10, 15], -50, -5))
parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T)
parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T)
?parse_date
parsedate::parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T)
as.Date(parsedate::parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T))
system.time(mydf$date <- as.Date(parsedate::parse_date(str_sub(mydf[, 15], -50, -5))))
mydf[1:100,]$date > 2017-03-01
mydf[1:100,]$date < 2017-03-01
mydf[1:100,]$date
str(mydf[1:100,]$date)
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%D")
mydf[1:100,]$date < as.Date("2017-03-01", "%Y-%m-%D")
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%D")
as.Date("2017-03-01", "%Y-%m-%D")
as.Date("2017-03-01", "%Y-%m-%d")
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d")
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d")
which(mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
strwrap(mydf[12,15], 100)
which(mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
which(mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
which(mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-02-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-03-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-01-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-02-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-01-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
topicLabel
?mldr_transform
emo_lp <- mldr_transform(emotions, "LP")
emo_br <- mldr_transform(emotions, "BR")
names(emotions)
¨summary(emotions)
summary(emotions)
emotions$labels
library(mldr)
summary(emotions)
emotions$labels
emo_lp <- mldr_transform(emotions, "LP")
emo_br <- mldr_transform(emotions, "BR")
str(emo_br)
length(emo_br)
summary(emotions)
emotions$labels
str(emo_br[1])
names(emo_br[1])
str(emo_br[1])
length(emo_br[1])
length(emo_br[2])
str(emo_br[2])
str(emo_br[3])
str(emo_lp[3])
str(emo_lp)
?mldr_transform
emo_lp$classLabel
levels(emo_lp$classLabel)
length(levels(emo_lp$classLabel))
summary(emotions)
library(RWeka)summary(emotions)
library(RWeka)
classifier <- IBk(classLabel ~ ., data = emo_lp, control = Weka_control(K = 10))
evaluate_Weka_classifier(classifier, numFolds = 5)
summary(emotions)
summary(emotions)
evaluate_Weka_classifier(classifier, numFolds = 5)
summary(emotions)
summary(emotions)
topicLabel
topicLabel <- label_topics(dj.terms)
library(readr)
library(dplyr)
library(tm)
library(LDAvis)
# Function: Visualize LDA model (create JSON object) topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(doc_term)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) } create_dtm_from_isin <- function(isin_selection, dj.select) {     # load the bigram tokenizer     library(NLP)     BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)     # create a dtm     djtopic.dtm <- tm::DocumentTermMatrix(dj.select,     control = list(     #stemming = TRUE,             stopwords = TRUE,             minWordLength = 2,             removeNumbers = TRUE,             removePunctuation = TRUE             )             )     return(djtopic.dtm) } remove_sparse_terms <- function(djtopic.dtm) {     # sum up the dtm rows to gather aggregated counts     term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i],     djtopic.dtm$j, mean) * log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))     median <- summary(term_tfidf)[3]     # only include terms above the median     djreduced.dtm <- djtopic.dtm[, term_tfidf >= median]     summary(slam::col_sums(djreduced.dtm))     return(djreduced.dtm) } attribute_topics_to_docs <- function(dj.model, dj.topics) {     # Creates a dataframe to store the News Number and the most likely topic     doctopics.df <- as.data.frame(dj.topics)     doctopics.df <- dplyr::transmute(doctopics.df,     #NewsId = rownames(doctopics.df),         NewsId = isin_selection,         Topic = dj.topics)     doctopics.df$NewsId <- as.integer(doctopics.df$NewsId)     return(doctopics.df) } label_topics <- function(dj.terms) {     # label topics     topicTerms <- tidyr::gather(dj.terms, Topic)     topicTerms <- cbind(topicTerms, Rank = rep(1:30))     topTerms <- dplyr::filter(topicTerms, Rank < 4)     topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))     topTerms$Topic <- as.numeric(topTerms$Topic)     topicLabel <- data.frame()     for (i in 1:topicNumber) {         z <- dplyr::filter(topTerms, Topic == i)         l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "),             stringsAsFactors = FALSE)         topicLabel <- rbind(topicLabel, l)     }     colnames(topicLabel) <- c("Label")     return(topicLabel) } correlate_topics <- function(isin_selection, dj.model) {     baz <- strsplit(mydf[isin_selection,]$topic, ",")     seltops <- unlist(lapply(baz, `[[`, 1))     # correlate topics     theta <- as.data.frame(topicmodels::posterior(dj.model)$topics)     head(theta[1:5])     phi <- as.data.frame(topicmodels::posterior(dj.model)$terms)     head(phi[1])     x <- as.data.frame(row.names(theta), stringsAsFactors = FALSE)     colnames(x) <- c("NewsId")     x$NewsId <- as.numeric(x$NewsId)     theta2 <- cbind(x, theta)     theta2 <- dplyr::left_join(theta2, FirstCategorybyLesson, by = "NewsId")     ## Returns column means grouped by catergory     theta.mean.by <- by(theta2[, 2:28], theta2$Category, colMeans)     theta.mean <- do.call("rbind", theta.mean.by)     library(corrplot)     c <- cor(theta.mean)     corrplot(c, method = "circle")     return(theta.mean) } most_diagnostic_topics <- function(theta.mean) {     # most diagnostic topics     theta.mean.ratios <- theta.mean     for (ii in 1:nrow(theta.mean)) {         for (jj in 1:ncol(theta.mean)) {             theta.mean.ratios[ii, jj] <-             theta.mean[ii, jj] / sum(theta.mean[ii, - jj])         }     }     topics.by.ratio <- apply(theta.mean.ratios, 1,         function(x) sort(x, decreasing = TRUE, index.return = TRUE)$ix)     # The most diagnostic topics per category are found in the theta 1st row of the index matrix:     topics.most.diagnostic <- topics.by.ratio[1,]     head(topics.most.diagnostic) } find_optimal_topic_number <- function(dj.red.dtm) {     library(Rmpfr)     harmonicMean <- function(logLikelihoods, precision = 2000L) {         llMed <- median(logLikelihoods)         as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,                                        prec = precision) + llMed))))     }     k <- 25     burnin <- 1000     iter <- 1000     keep <- 50     fitted <- topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))     ## assuming that burnin is a multiple of keep     logLiks <- fitted@logLiks[-c(1:(burnin / keep))]     ## This returns the harmomnic mean for k = 25 topics.     harmonicMean(logLiks)     seqk <- seq(2, 100, 1)     burnin <- 1000     iter <- 1000     keep <- 50     system.time(fitted_many <- lapply(seqk, function(k) topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))))     cl <- makePSOCKcluster(8)     setDefaultCluster(cl)     #adder <- function(a, b) a + b     #clusterExport(NULL, c('adder'))     #parLapply(NULL, 1:8, function(z) adder(z, 100))     clusterExport(cl, c('LDA', 'dj.red.dtm', 'burnin', 'iter', 'keep'))     parLapply(cl, seqk, function(k)         LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep)))     # extract logliks from each topic     logLiks_many <- lapply(fitted_many, function(L) L@logLiks[-c(1:(burnin / keep))])     # compute harmonic means     hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))     ldaplot <- ggplot(data.frame(seqk, hm_many), aes(x = seqk, y = hm_many)) + geom_path(lwd = 1.5) +     theme(text = element_text(family = NULL),         axis.title.y = element_text(vjust = 1, size = 16),         axis.title.x = element_text(vjust = -.5, size = 16),         axis.text = element_text(size = 16),         plot.title = element_text(size = 20)) +     xlab('Number of Topics') +     ylab('Harmonic Mean') +      annotate("text", x = 25, y = -80000, label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +     ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of DJ Newswire", atop(italic("How many distinct topics in the abstracts?"), ""))))     return(ldaplot) } toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) { stop <- nchar(x); substring(x, start, stop) }) tokenize_corp <- function(corpus) {     newcorp <- corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Email.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Write.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(MORE TO FOLLOW.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(END\\).*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r") %>%     tm_map(toSpace, "\"")     #tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)")     #tm_map(replacener)     return(newcorp) }
# import the column in the dataframe as a tm corpus object (presupposes a mydf from the file cvsreader.R b <- VectorSource(mydf$replace) #dj.corpus <- Corpus(b,  #readerControl = list(language = "eng", reader = readPlain)) # Need VCorpus in order exploit bigram tokenization dj.corpus <- VCorpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain)) test.select <- test.corpus[isin_selection] vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
b <- VectorSource(mydf$replace)
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
b <- VectorSource(mydf$replace) #dj.corpus <- Corpus(b,  #readerControl = list(language = "eng", reader = readPlain)) # Need VCorpus in order exploit bigram tokenization dj.corpus <- VCorpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
system.time(mydf$date <- as.Date(parsedate::parse_date(str_sub(mydf[, 15], -50, -5))))
library(stringr)
library(parsedate)
system.time(mydf$date <- as.Date(parsedate::parse_date(str_sub(mydf[, 15], -50, -5))))
vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-01-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2016-12-08", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
phi <- posterior(dj.model)$terms %>% as.matrix
theta <- posterior(dj.model)$topics %>% as.matrix
phi
dim(phi)
dim(theta)
head(theta)
phi
head(theta)
length(mydf)
mydf[1:10,]$date
system.time(save(mydf, file = c("mydf_full_columns_fixed_1346_with_date.RData")))
tokenize_corp <- function(corpus) {     newcorp <- corpus %>% tm_map(toSpace, "[0-9]{4}.*\\\\r\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Email.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Write.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(MORE TO FOLLOW.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(END\\).*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r") %>%     tm_map(toSpace, "\"")     #tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)")     #tm_map(replacener)     return(newcorp) }
library(parallel)
cl <- makePSOCKcluster(7)
setDefaultCluster(cl)
tm_parLapply_engine(cl)
library(tm)
tm_parLapply_engine(cl)
install.packages("tm")
tm_parLapply_engine(cl)
?tm
??tm
tm_parLapply_engine(cl)
?tm_parLapply_engine
??tm_parLapply_engine
??tm
?tm
??tm_parLapply_engine
?tm_parLapply_engine
str(tm)
tm
tm_map
sessionInfo()
install.packages("tm")
install.packages("Rccp")
install.packages('Rcpp')
library(tm)
tm_parLapply_engine
tm
detach_package(tm, TRUE)
detach("package:tm", unload = TRUE)
detach("package:topicmodels", unload = TRUE)
detach("package:tm", unload = TRUE)
install.packages('tm')
library(tm)
install.packages('tm')
library(tm)
