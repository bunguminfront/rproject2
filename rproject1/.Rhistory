library(caret)
install.packages("caret")
library(mlbench)
install.packages("mlbench")
library(mlbench)
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
## The format of the results 
## The output is a set of integers for the rows of Sonar 
## that belong in the training set. 
str(inTrain)
training <- Sonar[inTrain,]
testing <- Sonar[-inTrain,]
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
library(caret)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
## The format of the results 
## The output is a set of integers for the rows of Sonar 
## that belong in the training set. 
str(inTrain)
training <- Sonar[inTrain,]
testing <- Sonar[-inTrain,]
nrow(testing)
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
install.packages("AzureML")
library(tm)
# Create corpus
docs <- Corpus(VectorSource(ds[1:10000,]$Preprocessed.tokenized_text))
install.packages("tm")
library(tm)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(ds[1:10000,]$Preprocessed.tokenized_text))
# Clean corpus
names(ds)
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
install.packages("e1071")
plsFit <- train(Class ~ ., data = training, method = 'pls', preProc = c("center", "scale"))
plot(plsFit)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(ds[1:10000,]$Preprocessed.tokenized_text))
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
install.packages("snowballC")
2
install.packages("SnowballC")
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.95)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, ds$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-train]
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(modeldata$equities == 1)
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
factor(modeldata$equities == 1)
modeldata$equity
docs
names(doccol)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
dim(mat.df)
colnames(mat.df)[ncol(mat.df)] <- "category"
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-train]
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
dim(mat.df)
dim(cl)
dim(training)
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
length(training)
length(testing)
cl <- mat.df[, "category"]
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
names(modeldata)
dim(modeldata)
factor(modeldata$equities == 1)
names(modeldata)
modeldata$equities <- factor(cl == 1)
model1 <- train(equities ~., data=training, method="svmLinear")
model1 <- train(equities ~., data = training, method="svmLinear")
model1 <- train(equities ~., data = modeldata[training,], method="svmLinear")
model1 <- train(equities ~., data = modeldata[training,], method="svmLinear")
plot(model1)
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
accuracy(pred)
table(true = modeldata[testing,]$equities, pred = pred)
model1
plot(pred)
confusionMatrix(pred, modeldata[testing,]$equities)
model1 <- train(equities ~., data = modeldata[training,], trControl = train_control, method="svmLinear")
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
model1 <- train(equities ~., data = modeldata[training,], trControl = train_control, method="svmLinear")
print(model1)
plot(model1)
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.95)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
library(kernlab)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
model1 <- train(equities ~., data = modeldata[training,], trControl = train_control, method="svmLinear")
model1
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
tm_map(reuters, content_transformer(tolower))
tm_map(docs, content_transformer(tolower))
tm_map(docs, content_transformer(gsub))
tm_map(docs, content_transformer(gsub("FOO", "BAR")))
gsub("FOO", "BAR", "FOOBAR")
tm_map(docs, content_transformer(gsub))
tm_map(docs, content_transformer(function(x, pattern) gsub(pattern, " ", x)))
tm_map(docs, content_transformer(function(x, pattern) gsub("foobar", " ", x)))
bar <- "I am just a poor boy"
substr(bar,1 , 25)
substr(bar,10 , 25)
chophead <- content_transformer(function(x, start, stop) substr(x, start, stop))
docs <- tm_map(docs, chophead, 10, 25)
inspect(docs[[1]])
inspect(docs[1])
inspect(docs[1:2])
inspect(docs[[2]])
lapply(ovid[1:2], as.character)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start, stop) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start, stop) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, length(x)))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
bar
head(bar,10)
tail(bar, 10)
chophead <- content_transformer(function(x, start) substr(x, start, 25))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, 25))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) substr(x, start, 100))
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start) {substr(x, start, 100)} )
docs <- tm_map(docs, chophead, 10)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {substr(x, start, 100)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
chophead <- content_transformer(function(x, start) {stop <- length(x); substr(x, start, 100)} )
docs <- tm_map(docs, chophead, 10)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- length(x); substr(x, start, stop)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- length(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
bar[25:length(bar)]
bar
length(bar)
length(bar[1])
length(bar[[1]])
nchar(bar)
bar[1:5]
bar[[1]]
bar[[1]][1:5]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 10)
lapply(docs[1:2], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:2], as.character)
docs <- tm_map(docs, toSpace, "\\( \\w+.+@+\\w+.* GMT \\)")
lapply(docs[1:10], as.character)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.+@+\\w+.* GMT \\)")
lapply(docs[1:10], as.character)
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
lapply(docs[1:10], as.character)
baz <- " ( kate . geenty @ wsj.com ) "
baz
gsub("\\( \\w+.+@+\\w+.* GMT \\)", baz)
gsub("\\( \\w+.+@+\\w+.* GMT \\)")
gsub("\\( \\w+.+@+\\w+.* GMT \\)", "BAR", baz)
gsub("\\( \\w+ \. +@+\\w+.* GMT \\)", "BAR", baz)
gsub("\\( \\w+ \\. +@+\\w+.* GMT \\)", "BAR", baz)
gsub("\\( \\w+ \\. +@+\\w+.* \\)", "BAR", baz)
gsub("\\( \\w+ \\. +@+\\w+.* ", "BAR", baz)
gsub("\\( \\w+ \\. ", "BAR", baz)
gsub("\\( \\w+ \\. \\w+", "BAR", baz)
gsub("\\( \\w+ \\. \\w+ @", "BAR", baz)
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baz)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+ \\. \\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+ \\. \\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
lapply(docs[1:10], as.character)
baf <- "( yvonne.lee @ wsj.com )"
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baz)
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baf)
baz
baf
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baf)
gsub("\\( \\w+ \\. \\w+ @.*", "BAR", baf)
gsub("\\( \\w+ \\. \\w+", "BAR", baf)
gsub("\\( \\w+ \\.", "BAR", baf)
gsub("\\( \\w+", "BAR", baf)
gsub("\\( \\w+ \\.", "BAR", baf)
bar
baz
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baf)
gsub("\\( \\w+ \\. \\w+ @.* \\)", "BAR", baz)
gsub("\\( \\w+.*\\w+ @.* \\)", "BAR", baz)
gsub("\\( \\w+.*\\w+ @.* \\)", "BAR", baf)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "FOO!", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
lapply(docs[1:10], as.character)
dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.95) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
lapply(docs[1:10], as.character)
# Clean corpus
#docs <- tm_map(docs, content_transformer(tolower))
#docs <- tm_map(docs, removeNumbers)
#docs <- tm_map(docs, removeWords, stopwords("english"))
#docs <- tm_map(docs, removePunctuation)
#docs <- tm_map(docs, stripWhitespace)
#docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.95)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
library(kernlab)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.95) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
library(kernlab)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) #docs <- tm_map(docs, chophead, 25) #docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") #docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") #docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") #docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") #docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
docs <- tm_map(docs, toSomth, "\\d+\\w", "NUMCHAR")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 25) docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR") #lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.95)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.91)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.2)
dtm.sparse
dtm.ng
removeSparseTerms(dtm.ng, 0.2)
removeSparseTerms(dtm.ng, 0.99)
removeSparseTerms(dtm.ng, 0.995)
dtm.spars<-removeSparseTerms(dtm.ng, 0.995)
dtm.spars
dtm.spars
dtm.sparse<-removeSparseTerms(dtm.ng, 0.995)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
colnames(mat.df)
mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
doccol <- ds[1:50000,] mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>% mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text)) library(NLP) BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE) library(tm) # Create corpus docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 25) docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR") #lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer)) dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
library("AzureML") ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net") ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV") names(ds)
library("AzureML") ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net") ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV") names(ds)
doccol <- ds[1:5000,]
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
#lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
library(caret)
library(mlbench)
# Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
doccol <- ds[1:10000,] #mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>% #mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text)) library(NLP) BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE) library(tm) # Create corpus docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 25) docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR") #lapply(docs[1:10], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer)) dtm <- DocumentTermMatrix(docs) # Remove sparse entries dtm.sparse <- removeSparseTerms(dtm, 0.97) dtm.sparse # Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1) model1 <- train(equities ~ .,     data = modeldata[training,],     trControl = train_control,     method="svmLinear") pred <- predict(model1, modeldata[testing, ], probability = FALSE) table(true = modeldata[testing,]$equities, pred = pred) confusionMatrix(pred, modeldata[testing,]$equities)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
library(RWeka)
install.packages("RWeka")
library(RWeka)
Sys.getenv("R_ARCH")
Sys.getenv("R_ARCH")
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
#lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
inspect(dtm[1:10,1:10])
inspect(dtm.sparse[1:10,1:10])
inspect(dtm.sparse[1,1:50])
inspect(dtm.sparse[1,1:100])
inspect(dtm.sparse[1,])
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 25)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
#lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.98)
inspect(dtm.sparse[1,1:100])
inspect(dtm.sparse[1,])
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
lapply(docs[1:10], as.character)
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:10], as.character)
# Clean corpus
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))lapply(docs[1:10], as.character)
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
lapply(docs[1:10], as.character)
docs <- tm_map(docs, chophead, 25)
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- tm_map(docs, chophead, 30)
lapply(docs[1:10], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- tm_map(docs, chophead, 33)
lapply(docs[1:10], as.character)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
lapply(docs[1:10], as.character)
lapply(docs[1:100], as.character)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
lapply(docs[90], as.character)
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
lapply(docs[90], as.character)
inspect(dtm.sparse[1,])
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.98)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
inspect(dtm.sparse[1,])
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.ng
dtm.sparse <- removeSparseTerms(dtm.ng, 0.996)
dtm.sparse
inspect(dtm.sparse[1,])
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm, 0.995)
inspect(dtm.sparse[1,])
# Create corpus docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} ) docs <- tm_map(docs, chophead, 33) docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)") docs <- tm_map(docs, toSpace, "More at.* GMT \\)") docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)") docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)") docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") docs <- tm_map(docs, toSpace, "Write to.* GMT \\)") docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)") docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character) # Clean corpus docs <- tm_map(docs, content_transformer(tolower)) docs <- tm_map(docs, removeNumbers) docs <- tm_map(docs, removeWords, stopwords("english")) docs <- tm_map(docs, removePunctuation) docs <- tm_map(docs, stripWhitespace) docs <- tm_map(docs, stemDocument, language = "english") # Create dtm dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
inspect(dtm.sparse[1,])
dtm.sparse
# Transform dtm to matrix to data frame - df is easier to work with mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .75)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1) library(kernlab) train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
    method="svmLinear"))
model1 <- train(equities ~ ., data = modeldata[training,], trControl = train_control, method = "svmLinear")
)
)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities)
confusionMatrix(pred, modeldata[testing,]$equities, mode="prec_recall")
2313/(2313+61)
2313/(2313+48)
confusionMatrix(modeldata[testing,]$equities, pred,mode="prec_recall")
confusionMatrix(pred, modeldata[testing,]$equities, mode="prec_recall", positive="TRUE")
names(valds)
docs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, ds$equities)
# Change name of new column to "category"
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, ds$equities)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
dim(mat.df)
dim(doccol)
mat.df <- cbind(mat.df, doccol$equities)
mat.df <- cbind(mat.df, doccol$equities)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
inspect(dtm.sparse[1,])
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
mat.df <- cbind(mat.df, valds$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl$equities == 1)
head(cl)
modeldata$equities <- factor(cl == 1)
length(cl)
valpred <- predict(model1, modeldata, probability = FALSE)
length(modeldata)
length(mat.df)
dim(modeldata)
''
foo <- Terms(dt)
foo <- Terms(dtm.sparse)
foo
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
inspect(dtm.sparse[1,])
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
trainVB
trainVocav
trainVocab
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE) # Column bind category (known classification) mat.df <- cbind(mat.df, doccol$equities) mat.df <- cbind(mat.df, valds$equities) # Change name of new column to "category" colnames(mat.df)[ncol(mat.df)] <- "category" # Split data by rownumber into two equal portions training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50)) testing <- (1:nrow(mat.df))[-training] # Isolate classifier cl <- mat.df[, "category"] # Create model data and remove "category" modeldata <- mat.df[, !colnames(mat.df) %in% "category"] modeldata$equities <- factor(cl == 1)
length(modeldata)
length(cl)
dim(modeldata)
length(modeldata$equities)
modeldata$equities <- factor(cl == 1)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, method="svmLinear"))
PP <- c('center', 'scale')
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, method="svmLinear"))
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
names(doccol)
names(valdocs)
names(valds)
mat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
mat.df <- cbind(mat.df, valds$equities)
colnames(mat.df)[ncol(mat.df)] <- "category"
cl <- mat.df[, "category"]
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
pred <- predict(model1, modeldata, probability = FALSE)
length(pred)
confusionMatrix(pred, modeldata$equities, mode = "prec_recall", positive = "TRUE")
predict(model1, modeldata[testing,], probability = TRUE)
predict(model1, modeldata[testing,], probability = TRUE)$probabilities
pred <- predict(model1, modeldata[testing,], probability = TRUE)
pred$levels
trControl <- trainControl(classProbs = TRUE)
ctrl <- trainControl(classProbs = TRUE)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl, method="svmLinear"))
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
mat.df <- cbind(mat.df, doccol$equities)
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl, method="svmLinear"))
modeldata$equities
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl, method="svmLinear")
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP,  method="svmLinear")
str(training)
str(modeldata)
str(cl)
str(modeldata$equities)
modeldata$equities <- cl
str(modeldata$equities)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
library(caret) library(mlbench) library("AzureML") ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net") ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV") names(ds) valds <- download.datasets(dataset = ws,name = "val_peder_csv") # subset dataset for processing doccol <- ds[1:1000,]
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.995)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
dtm.ng
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.95)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- cl
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- mat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata$equities <- factor(cl == 1)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata$equities <- factor(cl == 1)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata[equities = 1]
modeldata[equities == 1]
modeldata[modeldata$equities == 1]
modeldata[modeldata$equities = 1]
modeldata[modeldata$equities == 1231]
modeldata$equities
modeldata[modeldata$equities == TRUE]
modeldata[modeldata$equities == TRUE,]
modeldata[modeldata$equities == FALSE,]
dim(modeldata[modeldata$equities == FALSE,])
dim(modeldata[modeldata$equities == TRUE,])
modeldata[modeldata$equities == TRUE,]$equities
modeldata[modeldata$equities == TRUE,]$equities <- "P"
modeldata[modeldata$equities == TRUE,]$equities <- P
modeldata[modeldata$equities == TRUE,]$equities <- c("P")
modeldata$equities <- cl
modeldata[modeldata$equities == TRUE,]$equities
modeldata[modeldata$equities == TRUE,]$equities <- c("P")
modeldata[modeldata$equities == FALSE,]$equities <- c("F")
modeldata[modeldata$equities == FALSE,]$equities 
modeldata[modeldata$equities == TRUE,]$equities 
modeldata[modeldata$equities == 1,]$equities 
modeldata[modeldata$equities == 0,]$equities 
modeldata$equities <- cl
modeldata$equities 
modeldata[modeldata$equities == 0,]$equities 
modeldata[modeldata$equities == 1,]$equities 
modeldata[modeldata$equities == 0,]$equities <- c("F")
modeldata[!modeldata$equities == 0,]$equities <- c("P")
modeldata[modeldata$equities == 0,]$equities 
modeldata$equities 
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities 
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities 
modeldata[modeldata$equities == 1,]$equities  <- "JA"
modeldata$equities 
str(modeldata$equities)
typeof(modeldata$equities)
as.factor(modeldata$equities)
modeldata$equities <- as.factor(modeldata$equities)
modeldata$equities 
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
table(true = modeldata[testing,]$equities, pred = pred)
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
pred <- predict(model1, valmodeldata, probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
table(true = valmodeldata$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
length(valpred)
length(valmodeldata)
length(valmat.df)
valmat.df
dim(valmat.df)
valmodeldata[modeldata$equities == 0,]$equities <- "NEI"modeldata[modeldata$equities == 1,]$equities  <- "JA"
valmodeldata[modeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
as.factor(valmodeldata$equities)
valmodeldata$equities <- cl
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[modeldata$equities == 0,]$equities <- "NEI"
valmodeldata[modeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "TRUE")
pred
predict(model1, valmodeldata, probability = TRUE)
predict(model1, valmodeldata, probability = TRUE)
head(attr(pred, "probabilities"))
ctrl <- trainControl(classProbs = TRUE, probability = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
attr(pred)
pred
str(pred)
attr(pred)
attr(pred, "Probabilities")
attr(pred, "probabilities")
dtm.sparse <- removeSparseTerms(dtm.ng, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm.ng, 0.98)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE, probability = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(valmodeldata$equities)
modeldata$equities <- as.factor(modeldata$equities)
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
head(attr(pred, "probabilities"))
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = FALSE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
doccol <- ds[1:5000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear")
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "TRUE")
head(attr(pred, "probabilities"))
valpred <- predict(model1, valmodeldata, probability = FALSE)
table(true = valmodeldata$equities, pred = valpred)
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(classProbs = TRUE)
system.time(model1 <- train(equities ~ .,data = modeldata[training,],preProcess = PP, trControl = ctrl,  method="svmLinear"))
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
#mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text))
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
ctrl <- trainControl(method="None")
valmodeldata[1:10]
valmodeldata[1,]
dim(valmodeldata[1,])
dim(valmodeldata[1,])
dim(valmodeldata[1,])
typeof(valmodeldata[1,])
typeof(valmodeldata)
valmat.df[,]
valmat.df[1,]
valmat.df[1,]
typeof(valmat.df[1,])
typeof(valmat.df)
typeof(valdtm)
typeof(valdtm)
valdtm
valdtm[1]
valdtm[1,]
inspect(valdtm)
inspect(valdtm)[1,]
valmat.df[1,]
names(valmat.df[1,])
valmat.df[1,]
valmat.df[1,!= 0]
colSums(valmat.df[1,])
rowSums(valmat.df[1,])
rowSums(valmat.df[2,])
valdtm
rowSums(valmat.df)
valmat.df[1,]
valmat.df[2,]
row(valmat.df)
row(valmat.df[1,])[which(!X == 0)]
row(valmat.df[1,])[which(!valmat.df == 0)]
row(valmat.df[1,])[which(!valmat.df[1,] == 0)]
col(valmat.df[1,])[which(!valmat.df[1,] == 0)]
valmat.df[1,]
valmat.df[1,516]
names(valmat.df)
col(valmat.df[1,])[which(!valmat.df[1,] == 0)]
col(valmat.df[1:10,])[which(!valmat.df[1:10,] == 0)]
col(valmat.df[1:100,])[which(!valmat.df[1:100,] == 0)]
rowSums(valmat.df)
col(valmat.df[281,])[which(!valmat.df[281,] == 0)]
col(valmat.df[282,])[which(!valmat.df[282,] == 0)]
col(valmat.df[2,])[which(!valmat.df[2,] == 0)]
rowSums(valmat.df)
valdocs
inspelibrary(qdap)valdocs
library(qdap)
install.packages("qdap")
library(qdap)
    with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
valdocs
valdocs.df
valmat.df
valmat.df[1,]
valmat.df[2,]
mat.df[2,]
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
dim(modeldata)
system.time(model1 <- train(equities ~ .,data = modeldata[training,], trControl = ctrl,  method="svmLinear2"))
head(modeldata[training,])
ctrl <- trainControl(method="none")
system.time(model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear2"))
model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear2")
model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear")
model1 <- train(equities ~ ., data = modeldata[1:100,],  method = "svmLinear")
warnings()
typeof(modeldata$equities)
modeldata$equities = as.factor(modeldata$equities)
typeof(modeldata$equities)
levels(modeldata$equities)
model1 <- train(equities ~ ., data = modeldata[1:100,],  method = "svmLinear")
warnings/()
warnings()
model1 <- train(equities ~ ., data = modeldata[1:100,],  method = "svmLinear2")
warnings()
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
#mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text))
library(NLP)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm.ng <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
PP <- c('center', 'scale')
ctrl <- trainControl(method="none")
system.time(model1 <- train(equities ~ ., data = modeldata[1:100,], trControl = ctrl, method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[1:100,], method = "svmLinear2"))
warnings()
model1
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainContrl(method="none"), method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2"))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneLength=0))
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneLength=0)
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear", tuneLength=0)
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
pred
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear", tuneLength=0)
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear", tuneLength=2)
model1
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneLength=2)
model1 <- train(equities ~ ., data = modeldata[training,], trControl=trainControl(method="none"), method = "svmLinear2", tuneGrid = NULL)
model1 <- train(equities ~ ., data = modeldata[training,] method = "svmLinear2")
model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2")
model1 <- svmLinear(equities ~ ., data = modeldata[training,], method = "svmLinear2")
svm
model1 <- svm(equities ~ ., data = modeldata[training,])
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model1 <- svm(equities ~ ., data = modeldata[training,], kernel="linear")
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2")
model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear")
model1 <- train(equities ~ ., data = modeldata[training,], trainControl=ctrl, method = "svmLinear")
model1 <- train(equities ~ ., data = modeldata[training,], trControl = trainControl(method = "none"), method = "svmLinear", tuneLength = 0)
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
pred
table(true = modeldata[testing,]$equities, pred = pred)
names(valmodeldata)
names(valmodeldata)
names(modeldata)
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
library(magrittr)
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
valdocs
valmat.df
valmat.df[1,]
valmodeldata$equities
valdocs
valdocs[[1]]
valdocs[[1]]$content
valdocs[[1]]$content
strWrap(valdocs[[1]]$content)
library(qdap)
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
model1 <- train(equities ~ ., data = modeldata[training,], trControl = trainControl(method = "none"), method = "svmLinear", tuneLength = 3)
model1
model1 <- train(equities ~ ., data = modeldata[training,],  method = "svmLinear", tuneLength = 3)
model1
model1$finalModel
pred <- predict(model1, modeldata[testing,], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
pred <- predict(model1$finalModel, modeldata[testing,], probability = TRUE)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")pred <- predict(model1$finalModel, modeldata[testing,], probability = TRUE)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
pred <- predict(model1, modeldata[testing,], probability = TRUE)
pred
model_none <- train(equities ~ ., data = modeldata[training,], trControl = trainControl(method = "none"), method = "svmLinear", tuneLength = 0)
pred <- predict(model_none, modeldata[testing,], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
pred <- predict(model1, modeldata[testing,], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model1
model_none
model1
model1$fin
model1$finalModel
predict(model1$finalModel, modeldata[testing,])
system.time(model1 <- train(equities ~ ., data = modeldata[training,],  method = "svmLinear2", tuneLength = 3))
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "JA")
head(attr(pred, "probabilities"))
valpred <- predict(model1, valmodeldata, type="pred")
valpred <- predict(model1, valmodeldata, type="prob")
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trainControl(repeats = 2),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trainControl(repeats = 2, method = "none"),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trainControl(method = "none"),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trControl = trainControl(method = "none", repeats = 2),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2", trControl = trainControl(repeats = 2),  tuneLength = 3))
system.time(model1 <- train(equities ~ ., data = modeldata[training,], method = "svmLinear2"))
model1
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
valpred <- predict(model1, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
pred <- predict(model1, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
}
}
as.data.frame(valdocs)[1,] %>%     with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
}
       with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
    as.data.frame(valdocs)[1,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo <- function(doc)     as.data.frame(valdocs)[1,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo(1)
displayDoc <- function(doc)     as.data.frame(doc %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") }))) )
foo <- function(doc)     as.data.frame(doc)[1,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo <- function(docs, num)     as.data.frame(docs)[num,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
foo(valdocs, 1)
foo(docs, 1)
foo(docs, 2)
foo(docs, 3)
foo(docs, 4)
stopwords('english')
c(stopwords('english'), "gmt")
type(valmat.df)
typeof(valmat.df)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
typeof(valmat.df)
names(valmat.df)
names(modeldata)
names(valmat.df)[1:10]
foo <- names(valmat.df)[1:10]
modeldata[1:2,foo]
modeldata[1:2,-foo]
modeldata[1:2,!foo]
colnames(valmat.df)[1:10]
modeldata[1:2,!colnames(valmat.df)[1:10]]
modeldata[1:2,colnames(valmat.df)[1:10]]
modeldata[1:2,-colnames(valmat.df)[1:10]]
modeldata[-colnames(valmat.df)[1:10]]
modeldata[foo]
head(modeldata[foo])
head(modeldata[!foo])
head(modeldata[-foo])
head(modeldata[c("numchar")])
head(modeldata[,c("numchar")])
head(modeldata[,numchar])
names(modeldata)
head(modeldata[,"year numchar"])
head(modeldata[,!"year numchar"])
head(modeldata[,c("year numchar")])
head(modeldata[,!c("year numchar")])
head(modeldata[,-c("year numchar")])
head(modeldata[,colnames(valdata.df)])
head(modeldata[,colnames(valmat.df)])
head(modeldata[,colnames(valmat.df)[1:10]])
head(modeldata[,!colnames(valmat.df)[1:10]])
head(modeldata[,-colnames(valmat.df)[1:10]])
colnames(valmat.df)[1:10]]
colnames(valmat.df)[1:10]
colnames(valmat.df)[1:5]] -  colnames(valmat.df)[1:5]]
colnames(valmat.df)[1:10]] -  colnames(valmat.df)[1:5]]
head(modeldata[,!=colnames(valmat.df)[1:10]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:10]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:10]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:200]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:300]])
head(modeldata[,!names(modeldata) %in% colnames(valmat.df)[1:500]])
require(spacyr)
install.packages(spacyr)
install.packages(openNLP)
install.packages("spacyr")
install.packages("openNLP")
library(openNLP)
mystr <- "this is very nice"
tagPOS(mystr)
Maxent_POS_Tag_Annotator(mystr)
install.packages(openNLPdata)
install.packages("openNLPdata")
install.packages("openNLPmodels.en")
install.packages("openNLPmodels")
foo(docs, 1)
displayData <- function(docs, num)     as.data.frame(docs)[num,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
displayData(docs, 1)
library(magrittr)
displayData(docs, 1)
displayData(valddocs, 1)
displayData(valdocs, 1)
docs[1,]''
docs
docs$content
docs$content[1,]
docs$content[[1]]
docs$content[[1,]]
docs$content[[1]]
str(docs$content[[1]])
docs$content[[1]]$content
docs$content[[2]]$content
strWrap(docs$content[[2]]$content)
library(qdap)
strWrap(docs$content[[2]]$content)
valdtm
str(valdtm)
m <- as.matrix(valdtm)
v <- sort(rowSums(m), decreasing = TRUE)
head(v, N)
head(v, 10)
head(v, 100)
library(tm)
findFreqTerms(valdocs, 20)
findFreqTerms(valdtm, 20)
findFreqTerms(dfm.sparse, 20)
findFreqTerms(dtm.sparse, 20)
findFreqTerms(dtm.sparse, 1)
findFreqTerms(dtm, 1)
m <- as.matrix(dtm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
N <- 10
head(v, N)
head(m)
head(v)
v
dim(v)
length(v)
length(m)
dim(m)
m[v]
head(m[v])
v
head(v, N)
colnames(head(v, N))
col(head(v, N))
head(v, N)
head(v, N)
names(head(v, N))
m[names(head(v, N))]
m[,names(head(v, N))]
head(v, N)
m[1:10,1398]
m
m <- as.matrix(dfm.sparse)
N <- 10
m <- as.matrix(dfm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
Reset
N <- 10
m <- as.matrix(dfm.sparse)
library(caret)
library(mlbench)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(ws = ws, node_id = "0b4683ab-89b8-4b40-b372-b3bab93255d8-1829108", experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7", port_name = "Results dataset", data_type_id = "GenericCSV")
names(ds)
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
#mutate(tokenized_text = sub("\\( \\w+.+@+\\w+.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Subscribe to WSJ.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("Insider Data Source.* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( END .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = sub("\\( MORE TO FOLLOW .* GMT \\)", "", tokenized_text)) %>%
#mutate(tokenized_text = gsub("\\d+\\w", "NUMCHAR", tokenized_text))
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpus
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, c(stopwords("english"), "gmt"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm.ng, 0.99)
dtm.sparse
inspect(dtm.sparse[1,])
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
dtm.sparse
N <- 10
m <- as.matrix(dfm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
N <- 10
m <- as.matrix(dtm.sparse)
v <- sort(rowSums(m), decreasing = TRUE)
head(v, N)
m
head(v, N)
m[1:10,4774]
dim(m)
m[1:10,1:10]
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
N <- 10
m <- as.matrix(valdtm)
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
N <- 10
m <- as.matrix(dtm.sparse)
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
svm
library(e1071)
svm
?svm
model2 <- svm(equities ~ ., data=training, kernel = "linear")
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)model2 <- svm(equities ~ ., data=training, kernel = "linear")
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear")
model2
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear", cost=1)
model2
levels(modeldata$equities)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear", cost=1, probability = TRUE)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
model2 <- svm(equities ~ ., data=modeldata[training,], kernel = "linear", cost=10, probability = TRUE)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, modeldata[testing,]$equities, mode = "prec_recall", positive = "JA")
pred
pred[1:10,]
pred[1,]
pred[1]
length(pred)
pred[3999]
attr(pred, "probabilities")
attr(pred, "probabilities")[3999]
doccol[3999]
doccol[3999,]
table(true = modeldata[testing,]$equities, pred = pred)
valpred <- predict(model2, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model2, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "JA")
trainVocab
model2$coefs
length(trainVocab)
model2
str(model2)
str(model2)
trainVocab
)
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
head(ds)
names(ds)
dim(ds)
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- tm_map(docs, chophead, 33)
docs <- tm_map(docs, toSpace, "Subscribe to WSJ.* GMT \\)")
docs <- tm_map(docs, toSpace, "More at.* GMT \\)")
docs <- tm_map(docs, toSpace, "Visit http.* GMT \\)")
docs <- tm_map(docs, toSpace, "opyright 2.* GMT \\)")
docs <- tm_map(docs, toSpace, "Insider Data Source.* GMT \\)")
docs <- tm_map(docs, toSpace, "Write to.* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( END .* GMT \\)")
docs <- tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
docs <- tm_map(docs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(docs[90], as.character)
# Clean corpus
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, c(stopwords("english"), "gmt"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm <- DocumentTermMatrix(docs)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
inspect(dtm.sparse[1,])
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
ctrl <- trainControl(method="none")
model2 <- svm(equities ~., data= modeldata[training,], kernel = "linear", cost=5)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data = modeldata[training,], kernel = "linear", cost = 1)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data = modeldata[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, modeldata[testing, ], probability = TRUE)
table(true = modeldata[testing,]$equities, pred = pred)
valpred <- predict(model2, valmodeldata, probability = TRUE)
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
valpred <- predict(model2, valmodeldata, probability = TRUE)
valmodeldata
dim(valmodeldata)
predict(model2, valmodeldata, probability = TRUE)
dim(modeldata)
trainVocab <- Terms(dtm.sparse)
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
valdocs <- tm_map(valdocs, chophead, 33)
valdocs <- tm_map(valdocs, toSpace, "Subscribe to WSJ.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "More at.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Visit http.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "opyright 2.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Insider Data Source.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "Write to.* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( END .* GMT \\)")
valdocs <- tm_map(valdocs, toSpace, "\\( \\w+.*\\w+ @.* \\)")
valdocs <- tm_map(valdocs, toSmth, "\\d+\\w", "NUMCHAR")
lapply(valdocs[90], as.character)
# Clean corpus
valdocs <- tm_map(valdocs, content_transformer(tolower))
valdocs <- tm_map(valdocs, removeNumbers)
valdocs <- tm_map(valdocs, removeWords, stopwords("english"))
valdocs <- tm_map(valdocs, removePunctuation)
valdocs <- tm_map(valdocs, stripWhitespace)
valdocs <- tm_map(valdocs, stemDocument, language = "english")
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
valmat.df <- as.data.frame(data.matrix(valdtm), stringsAsfactors = FALSE)
valmat.df <- cbind(valmat.df, valds$equities)
colnames(valmat.df)[ncol(valmat.df)] <- "category"
cl <- valmat.df[, "category"]
valmodeldata <- valmat.df[, !colnames(valmat.df) %in% "category"]
valmodeldata$equities <- cl
valmodeldata[valmodeldata$equities == 0,]$equities <- "NEI"
valmodeldata[valmodeldata$equities == 1,]$equities <- "JA"
valmodeldata$equities <- as.factor(valmodeldata$equities)
dim(valmodeldata)
valpred <- predict(model2, valmodeldata, probability = TRUE)
table(true = valmodeldata$equities, pred = valpred)
confusionMatrix(valpred, valmodeldata$equities, mode = "prec_recall", positive = "JA")
trainVocab
}
filter_corp <- function(corpus) {     docs <- tm_map(docs, chophead, 33)     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33)     return (docs) }
filter_corp(docs)
library(magrittr)
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%      tm_map(toSpace, "Subscribe to WSJ.* GMT \\)")     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "") %>%     tm_map(docs, content_transformer(tolower)) %>%     tm_map(docs, removeNumbers) %>%     tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(docs, removePunctuation) %>%     tm_map(docs, stripWhitespace) %>%     tm_map(docs, stemDocument, language = "english")     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "") %>%     tm_map(docs, content_transformer(tolower)) %>%     tm_map(docs, removeNumbers) %>%     tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(docs, removePunctuation) %>%     tm_map(docs, stripWhitespace) %>%     tm_map(docs, stemDocument, language = "english")     return (docs) }
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "") %>%     tm_map(docs, content_transformer(tolower)) %>%     tm_map(docs, removeNumbers) %>%     #tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     #tm_map(docs, removePunctuation) %>%     #tm_map(docs, stripWhitespace) %>%     #tm_map(docs, stemDocument, language = "english")     return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "")      #tm_map(docs, content_transformer(tolower)) %>%     #tm_map(docs, removeNumbers) %>%     #tm_map(docs, removeWords, c(stopwords("english"), "gmt")) %>%     #tm_map(docs, removePunctuation) %>%     #tm_map(docs, stripWhitespace) %>%     #tm_map(docs, stemDocument, language = "english")     return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)") %>%     tm_map(docs, toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(docs, toSpace, "Write to.* GMT \\)") %>%     tm_map(docs, toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( END .* GMT \\)") %>%     tm_map(docs, toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(docs, toSmth, "\\d+\\w", "")      return (docs) } filter_corp(docs)
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)")      return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(docs, toSpace, "More at.* GMT \\)") %>%     tm_map(docs, toSpace, "Visit http.* GMT \\)") %>%     tm_map(docs, toSpace, "opyright 2.* GMT \\)")      return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "")      return (docs) } filter_corp(docs)
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (docs) } filter_corp(docs)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(mat.df, doccol$equities)
names(nmat)
names(nmat.df)
colnames(mat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
colnames(nmat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
# Column bind category (known classification)
mat.df <- cbind(mat.df, doccol$equities)
# Change name of new column to "category"
colnames(mat.df)[ncol(mat.df)] <- "category"
# Split data by rownumber into two equal portions
training <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
testing <- (1:nrow(mat.df))[-training]
# Isolate classifier
cl <- mat.df[, "category"]
# Create model data and remove "category"
modeldata <- mat.df[, !colnames(mat.df) %in% "category"]
modeldata$equities <- factor(cl == 1)
modeldata$equities <- cl
modeldata[modeldata$equities == 0,]$equities <- "NEI"
modeldata[modeldata$equities == 1,]$equities <- "JA"
modeldata$equities <- as.factor(modeldata$equities)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(mat.df, doccol$equities)
colnames(mat.df)[ncol(mat.df)] <- "equities"
namescolnames(nmat.df)[ncol(mat.df)] <- "equities"
colnames(nmat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(nmat.df, doccol$equities)
colnames(nmat.df)[ncol(mat.df)] <- "equities"
names(nmat.df)
names(modeldata)
names(modeldata) == names(nmat.df)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     return (nmat.df) }
prepare_for_classifier(dtm.sparse, doccol)
foo <- prepare_for_classifier(dtm.sparse, doccol)
dim(foo)
dim(modeldata)
model2 <- svm(equities ~ ., data = foo[training,], kernel = "linear", cost = 1, probability = TRUE)
levels(foo$equities)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, doccol)
levels(foo$equities)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NEI"     nmat.df[nmat.df$equities == 1,]$equities <- "JA"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NEI"     nmat.df[nmat.df$equities == 1,]$equities <- "JA"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, doccol)
levels(foo$equities)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, doccol)
levels(foo$equities)
model2 <- svm(equities ~ ., data = foo[training,], kernel = "linear", cost = 1, probability = TRUE)
asdasdasdasdasdasdasdsdafasdfasdfasdf
library(caret)
library(mlbench)
library(magrittr)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
doccol <- ds[1:10000,]
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
library(tm)
# Create corpora objects
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
filter_corp <- function(corpus) {     docs %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (docs) } docs <- filter_corp(docs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm <- DocumentTermMatrix(docs)
docs <- filter_corp(docs)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
doccol <- ds[1:10000,]
head(ds)
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
valdocs <- filter_corp(valdocs)
valdocs <- filter_corp(valdocs)
filter_corp <- function(corpus) {     corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (docs) }
valdocs <- filter_corp(valdocs)
valdocs
dtm <- DocumentTermMatrix(valdocs)
dtm
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
N <- 10
m <- as.matrix(dtm.sparse)
v <- sort(colSums(m), decreasing = TRUE)
head(v, N)
foo <- prepare_for_classifier(dtm.sparse, valdocs)
foo <- prepare_for_classifier(dtm.sparse, valdocs)
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(mat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, valdocs)
valdocs
prepare_for_classifier <- function(dtm, doccol) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, doccol$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, valdocs)
nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
nmat.df <- cbind(nmat.df, doccol$equities)
nmat.df <- cbind(nmat.df, valdocs$equities)
valdocs$equities
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
foo <- prepare_for_classifier(dtm.sparse, valds)
foo
model2 <- svm(equities ~ ., data = foo, kernel = "linear", cost = 1, probability = TRUE)
library(e1071)
model2 <- svm(equities ~ ., data = foo, kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, foo, probability = TRUE)
pred
table(true = modeldata[testing,]$equities, pred = pred)
confusionMatrix(pred, foo$equities, mode = "prec_recall", positive = "YES")
training <- sample(nrow(val_set_for_classifier), ceiling(nrow(val_set_for_classifier) * .50))
val_set_for_classifier <- prepare_for_classifier(dtm.sparse, valds)
training <- sample(nrow(val_set_for_classifier), ceiling(nrow(val_set_for_classifier) * .50))
testing <- (1:nrow(val_set_for_classifier))[-training]
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "JA")
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "JA")
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
doccol <- ds[1:10000,]
ds <- download.datasets(dataset = ws, name = "10k_EN_R_script")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
ds <- download.datasets(dataset = ws, name = "10k_EN_R_script")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
doccol <- ds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
docs <- filter_corp(docs)
valdocs <- filter_corp(valdocs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
trainVocab
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm, valds)
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "JA")
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
dtm <- DocumentTermMatrix(valdocs)
valdtm_create <- dtm
dtm
filter_corp <- function(corpus) {     corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (corpus) }
valdocs <- filter_corp(valdocs)
valddtm_create <- DocumentTermMatrix(valdocs)
valdtm_createVocab <- Terms(valdtm_create)
valddtm_create
valdocs
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
valdocs
valdocs <- filter_corp(valdocs)
valdocs
valddtm_create <- DocumentTermMatrix(valdocs)
valdtm_createVocab <- Terms(valdtm_create)
valddtm_create
valdtm_create <- DocumentTermMatrix(valdocs)
valdtm_createVocab <- Terms(valdtm_create)
valddtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valddtm.sparse
train_dtm <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm
valdtm_createVocab <- Terms(valdtm.sparse)
valddtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valdtm_createVocab <- Terms(valdtm.sparse)
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
valddtm.sparse
valdtm_createVocab <- Terms(valdtm.sparse)
valddtm.sparsevaldtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valdtm_createVocab <- Terms(valdtm.sparse)
length(valdtm_createVocab)
valdtm.sparse <- removeSparseTerms(valdtm_create, 0.97)
valdtm.sparse
valdtm_createVocab <- Terms(valdtm.sparse)
length(valdtm_createVocab)
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
val_set_for_classifier <- prepare_for_classifier(valdtm_create, valds)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
dim(val_set_for_classifier)
dim(train_set_for_classifier)
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
dim(train_set_for_classifier)
dim(val_set_for_classifier)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
valdtm.sparse <- removeSparseTerms(valdtm_create, 0.98)
valdtm_createVocab <- Terms(valdtm.sparse)
valdtm.sparse
valdtm.sparse <- removeSparseTerms(valdtm_create, 0.95)
valdtm.sparse
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 1, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
names(train_set_for_classifier)
dim(train_set_for_classifier)
dim(val_set_for_classifier)
model2
dim(val_set_for_classifier)
dim(train_set_for_classifier)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
dim(train_set_for_classifier)
train_dtm_from_val
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
length(valdtm_createVocab)
valdtm.sparse
valdtm_createVocab <- Terms(valdtm.sparse)
length(valdtm_createVocab)
train_dtm_from_val <- DocumentTermMatrix(docs, list(dictionary = valdtm_createVocab))
train_dtm_from_val
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
model2
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
valds
docs
library(caret)
library(mlbench)
library(magrittr)
library("AzureML")
ws <- workspace(id = "e12f59ece68c40559b1b83e53ba52030", auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==", api_endpoint = "https://europewest.studioapi.azureml.net")
ds <- download.datasets(dataset = ws, name = "10k_EN_R_script")
valds <- download.datasets(dataset = ws,name = "val_peder_csv")
# subset dataset for processing
doccol <- ds[1:10000,]
library(tm)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x))
toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x))
chophead <- content_transformer(function(x, start) {stop <- nchar(x); substring(x, start, stop)} )
filter_corp <- function(corpus) {     corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "Subscribe to WSJ.* GMT \\)") %>%     tm_map(toSpace, "More at.* GMT \\)") %>%     tm_map(toSpace, "Visit http.* GMT \\)") %>%     tm_map(toSpace, "opyright 2.* GMT \\)") %>%     tm_map(toSpace, "Insider Data Source.* GMT \\)") %>%     tm_map(toSpace, "Write to.* GMT \\)") %>%     tm_map(toSpace, "\\( MORE TO FOLLOW .* GMT \\)") %>%     tm_map(toSpace, "\\( END .* GMT \\)") %>%     tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)") %>%     tm_map(toSmth, "\\d+\\w", "") %>%     tm_map(content_transformer(tolower)) %>%     tm_map(removeNumbers) %>%     tm_map(removeWords, c(stopwords("english"), "gmt")) %>%     tm_map(removePunctuation) %>%     tm_map(stripWhitespace) %>%     tm_map(stemDocument, language = "english")     return (corpus) }
docs <- filter_corp(docs)
valdocs <- filter_corp(valdocs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
trainVocab <- Terms(dtm.sparse)
valdtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(train_dtm_from_val, doccol)
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
val_set_for_classifier <- prepare_for_classifier(valdtm.sparse, valds)
val_set_for_classifier <- prepare_for_classifier(valdtm, valds)
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
dim(training)
dim(train_set_for_classifier)
dim(val_set_for_classifier)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
library(e1071)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
table(true = val_set_for_classifier$equities, pred = valpred)
foo <- function(bar) { levels(docs$bar) }
foo("equities")
levels(docs$equities)
levels(dtm.sparse$equities)
length(dtm.sparse$equities)
length(doccol$equities)
foo <- function(bar) { length(docs$bar) }
foo("equities")
foo <- function(bar) { length(doccol$bar) }
foo("equities")
length(doccol$equities)
foo(equities)
length(doccol$equities)
names(doccol)
colnames(doccol)
names(doccol)
foo(c("equities"))
source("~/visual studio 2017/Projects/rproject1/rproject1/script.R", echo = TRUE, encoding = "Windows-1252")
prepare_for_classifier <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$commentary)     colnames(nmat.df)[ncol(nmat.df)] <- c("commentary")     nmat.df[nmat.df$commentary == 0,]$commentary <- "NO"     nmat.df[nmat.df$commentary == 1,]$commentary <- "YES"     nmat.df$equities <- as.factor(nmat.df$commentary)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
dtm.sparse
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$commentary)     colnames(nmat.df)[ncol(nmat.df)] <- c("commentary")     nmat.df[nmat.df$commentary == 0,]$commentary <- "NO"     nmat.df[nmat.df$commentary == 1,]$commentary <- "YES"     nmat.df$equities <- as.factor(nmat.df$commentary)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE     )
nmat.df <- cbind(nmat.df, docs$commentary)
nmat.df <- cbind(nmat.df, doccol$commentary)
names(doccol)
prepare_for_classifier <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$equities)     colnames(nmat.df)[ncol(nmat.df)] <- c("equities")     nmat.df[nmat.df$equities == 0,]$equities <- "NO"     nmat.df[nmat.df$equities == 1,]$equities <- "YES"     nmat.df$equities <- as.factor(nmat.df$equities)     return (nmat.df) }
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
ctrl <- trainControl(method="cv")
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2"))
ctrl <- trainControl(method="cv", allowParallel = TRUE)
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2"))
model1
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
grid <- expand.grid(sigma = c(.01, .015, 0.2),                     C = c(0.75, 0.9, 1, 1.1, 1.25) )
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
grid <- expand.grid(sigma = c(.01, .015, 0.2),                     cost = c(0.75, 0.9, 1, 1.1, 1.25) )
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
grid <- expand.grid(cost = c(0.75, 0.9, 1, 1.1, 1.25))
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
allds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
allds <- download.datasets(dataset = ws, name = "10_en_preprocess_allcat")
ds <- download.intermediate.dataset(  ws = ws,  node_id = "8f087393-3474-4a94-8bb0-e9f979d044e8-80484",  experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.fe87bfbfb85140858d69aaee28f44eb7",  port_name = "Results dataset",  data_type_id = "GenericCSV")
allds <- ds
names(allds)
foofunc <- function(col) { typeof(docs$col) }
foofuc("equities")
foofunc("equities")
typeof(docs$equities)
typeof(ds$equities)
length(ds$equities)
foofunc <- function(col) { length(ds$col) }
foofunc("equities")
foofunc <- function(col) { length(ds$eval(col)) }
foofunc("equities")
length(ds$equities)
names(ds)
colnames(ds)
foofunc <- function(col) { length(ds[[col]]) }
foofunc("equities")
train_set_for_classifier
dim(train_set_for_classifier)
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs[[tag]])     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier(dtm.sparse, doccol)
train_set_for_classifier_new <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
dim(train_set_for_classifier)
dim(train_set_for_classifier_new)
model2 <- svm(equities ~ ., data = train_set_for_classifier_new[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model1, train_set_for_classifier_new[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm, valds, "equities")
names(valds)
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
grid <- expand.grid(cost = c(1, 1.5, 2))
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
dim(train_set_for_classifier)
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
valdocs <- Corpus(VectorSource(valds$Preprocessed.tokenized_text))
docs <- filter_corp(docs)
valdocs <- filter_corp(valdocs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.97)
dtm.sparse
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
doccol <- allds[1:10000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- filter_corp(docs)
dtm <- DocumentTermMatrix(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm_from_train_dtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
dim(training)
length(training)
dim(allds)
doccol
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
pred <- predict(model2, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
valpred <- predict(model2, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
doccol <- allds[1:20000,]
docs <- Corpus(VectorSource(doccol$Preprocessed.tokenized_text))
docs <- filter_corp(docs)
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm_from_train_dtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
dtm.sparse
dtm
docs
dtm <- DocumentTermMatrix(docs)
dtm.sparse
dtm
dtm.sparse <- removeSparseTerms(dtm, 0.98)
dtm.sparse
trainVocab <- Terms(dtm.sparse)
valdtm_from_train_dtm <- DocumentTermMatrix(valdocs, list(dictionary = trainVocab))
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
training <- sample(nrow(train_set_for_classifier), ceiling(nrow(train_set_for_classifier) * .50))
testing <- (1:nrow(train_set_for_classifier))[-training]
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
ctrl <- trainControl(method="cv", allowParallel = TRUE)
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
valpred <- predict(model1, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
model1
ctrl <- trainControl(method="cv", allowParallel = TRUE, repeats = 2)
model1
table(true = val_set_for_classifier$equities, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$equities, mode = "prec_recall", positive = "YES")
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "outlook")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "outlook")
model2 <- svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
dim(train_set_for_classifier)
names(train_set_for_classifier)
dim(train_set_for_classifier)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "equities")
dim(train_set_for_classifier)
names(train_set_for_classifier)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, "equities")
names(train_set_for_classifier)
train_set_for_classifier$equities
names(train_set_for_classifier)
names(train_set_for_classifier$equities)
train_set_for_classifier$equities
names(train_set_for_classifier$equities)
colnames(train_set_for_classifier$equities)
colnames(train_set_for_classifier)
training_tag <- "outlook"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
model2 <- svm(training_tag ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
training_tag <- "equities"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
model2 <- svm(training_tag ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
names(train_set_for_classifier)
length(names(train_set_for_classifier))
length(colnames(train_set_for_classifier))
colnames(train_set_for_classifier)
svm(equities ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
ctrl <- trainControl(method="cv", allowParallel = TRUE, repeats = 2)
system.time(model1 <- train(equities ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
training_tag <- "outlook"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier$outlook
doccol$outlook
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
train_set_for_classifier$outlook
doccol$outlook
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
train_set_for_classifier$outlook
doccol$outlook
train_set_for_classifier$outlook
train_set_for_classifier$outlookprepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier$outlook
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
training_tag <- "outlook"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier$outlook
dtm.sparse
nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
nmat.df <- as.data.frame(data.matrix(dtm.sparse), stringsAsfactors = FALSE)
nmat.df <- cbind(nmat.df, docs$outlook)
nmat.df <- cbind(nmat.df, doccol$outlook)
names(nmat.df)
colnames(nmat.df)[ncol(nmat.df)] <- "outlook"
names(nmat.df)
nmat.df$outlook
doccol$outlook
nmat.df <- cbind(nmat.df, docs$outlook)
nmat.df <- cbind(nmat.df, doccol$outlook)
names(nmat.df)
nmat.df$doccol$outlook
nmat.df[,746]
colnames(nmat.df)[ncol(nmat.df)] <- "outlook"
nmat.df[,746]
nmat.df$outlook
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook <- as.factor(nmat.df$outlook)     return (nmat.df) }
training_tag <- "outlook_tag"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
model1
plot(model1)
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
names(train_set_for_classifier)
table(true = train_set_for_classifier[testing,]$outlook_tag, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$outlook_tag, mode = "prec_recall", positive = "YES")
levels(train_set_for_classifie$outlook_tag)
levels(train_set_for_classifier$outlook_tag)
names(train_set_for_classifier$outlook_tag)
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
training_tag <- "outlook_tag"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
names(train_set_for_classifier)
model1
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
levels$outlook_tag
levels(train_set_for_classifier$outlook_tag)
levels(train_set_for_classifier$outlook_taprepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }g)
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
training_tag <- "outlook_tag"
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
names(train_set_for_classifier)
levels(train_set_for_classifier$outlook_tag)
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook == 0,]$outlook_tag <- "NO"     nmat.df[nmat.df$outlook == 1,]$outlook_tag <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
levels(train_set_for_classifier$outlook_tag)
prepare_for_classifier_new <- function(dtm, docs) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- "outlook_tag"     nmat.df[nmat.df$outlook_tag == 0,]$outlook_tag <- "NO"     nmat.df[nmat.df$outlook_tag == 1,]$outlook_tag <- "YES"     nmat.df$outlook_tag <- as.factor(nmat.df$outlook_tag)     return (nmat.df) }
levels(train_set_for_classifier$outlook_tag)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
levels(train_set_for_classifier$outlook_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
pred <- predict(model1, train_set_for_classifier[testing,], probability = TRUE)
table(true = train_set_for_classifier[testing,]$equities, pred = pred)
dim(train_set_for_classifier[testing,])
length(pred)
table(true = train_set_for_classifier[testing,]$outlook_tag, pred = pred)
confusionMatrix(pred, train_set_for_classifier[testing,]$equities, mode = "prec_recall", positive = "YES")
confusionMatrix(pred, train_set_for_classifier[testing,]$outlook_tag, mode = "prec_recall", positive = "YES")
valpred <- predict(model1, val_set_for_classifier, probability = TRUE)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds)
valpred <- predict(model1, val_set_for_classifier, probability = TRUE)
table(true = val_set_for_classifier$outlook_tag, pred = valpred)
confusionMatrix(valpred, val_set_for_classifier$outlook_tag, mode = "prec_recall", positive = "YES")
table(true = train_set_for_classifier[testing,]$outlook_tag, pred = pred)
table(true = val_set_for_classifier$outlook_tag, pred = valpred)
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs$outlook)     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
val_set_for_classifier2 <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, "outlook_tag")
val_set_for_classifier == val_set_for_classifier2
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds)
valpred <- predict(model1, val_set_for_classifier2, probability = TRUE)
predict(model1, val_set_for_classifier, probability = TRUE) == predict(model1, val_set_for_classifier2, probability = TRUE)
names(ds)
training_tag <- "FXFI"
prepare_for_classifier_new <- function(dtm, docs, tag) {      nmat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)     nmat.df <- cbind(nmat.df, docs[[tag]])     colnames(nmat.df)[ncol(nmat.df)] <- tag     nmat.df[nmat.df[[tag]] == 0,][[tag]] <- "NO"     nmat.df[nmat.df[[tag]] == 1,][[tag]] <- "YES"     nmat.df[[tag]] <- as.factor(nmat.df[[tag]])     return (nmat.df) }
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol)
train_set_for_classifier <- prepare_for_classifier_new(dtm.sparse, doccol, training_tag)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds)
val_set_for_classifier <- prepare_for_classifier_new(valdtm_from_train_dtm, valds, training_tag)
system.time(model1 <- train(outlook_tag ~ ., data = train_set_for_classifier[training,], trControl = ctrl, method = "svmLinear2", tuneGrid = grid))
model2 <- svm(training_tag ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
model2 <- svm(string(training_tag) ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
model2 <- svm(FXFI ~ ., data = train_set_for_classifier[training,], kernel = "linear", cost = 2, probability = TRUE)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "7ea3268f-ebe0-4ce9-b942-029d2811d2f5-58393",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.03f3f2e2f38e4777b1fed27d00b87aae",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) head(ds)
colSums(ds)
dim(ds)
valds
displayData <- function(docs, num)     as.data.frame(docs)[num,] %>% with(., invisible(sapply(text, function(x) { strWrap(x); cat("\n\n") })))
displayData(valdocs, 1)
valdocs[1,]
valds[1,]
displayData(valds, 1)
library(qdap)
displayData(valds, 1)
displayData(valds, 2)
valds[1]$content
valdocs$content
valdocs$content[[1]]$content
strWrap(valdocs$content[[1]]$content)
strWrap(valdocs$content[[1]]$content)
head(ds)
head(ds, 50)
strWrap(valdocs$content[[43]]$content)
strWrap(valdocs$content[[46]]$content)
head(ds, 50)
ds
strWrap(valdocs$content[[296]]$content)
ds
names(ds)
ds[Scored.Labels == 1]
ds[ds$Scored.Labels == 1]
ds$Scored.Labels
ds[ds$Scored.Labels == 1]
ds[ds$Scored.Labels == 1,]
ds[ds$Scored.Labels == 1,1]
ds[ds$Scored.Labels == 1,]
strWrap(valdocs$content[[294]]$content)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
library(mldr)
install.packages(mldr)
install.packages("mldr")
names(ds)
length(names(ds))
c(seq(1:4))
c(seq(1:4), seq(5:6))
names(ds)
names(ds)[10]
names(ds)[15]
ds[1:10, c(15, seq(1:14))]
ds[1:10, c(15, seq(1:14), seq(16:17))]
c(15, seq(1:14), seq(16:17))
c(15, seq(1:14), 16, 17)
ds[1:10, c(15, seq(1:14), 16,17)]
mydf <- ds[, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(my_chosen_data, c(seq(2:17)))
library(mldr)
mymldr <- mldr_from_dataframe(my_chosen_data, c(seq(2:17)))
mymldr <- mldr_from_dataframe(mydf, c(seq(2:17)))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) library(mldr)
mydf <- ds[1:10, c(15, seq(1:14), 16, 17)]
names(mydf)
mydf[2:17]
mymldr <- mldr_from_dataframe(mydf, c(seq(2:17)))
plot(mymldr)
mymldr
mymldr <- mldr_from_dataframe(mydf, c(seq(3:17)))
mymldr
mymldr <- mldr_from_dataframe(mydf, c(seq(4:17)))
mymldr
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(seq(4:17)))
mymldr
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(4:17))
mymldr
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(4,5))
mymldr
mldr_from_dataframe(mydf, labelIndices = c(4,5))
mldr_from_dataframe(mydf, labelIndices = c(4,5), name = "FOO!")
summary(mldr_from_dataframe(mydf, labelIndices = c(4,5), name = "FOO!"))
summary(mldr_from_dataframe(mydf, labelIndices = c(2:17), name = "FOO!"))
mydf <- ds[1:1000, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
plot(mymldr, type = "LSB")
plot(mymldr, type = "AT")
plot(mymldr, type = "LSH")
my_predictions <- mydf[, seq(2, length(names(mydf)), 2)]
mydf <- ds[1:10000, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
mydf <- ds[1:100000, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
plot(mymldr, type = "LSB")
plot(mymldr, type = "AT")
mydf <- ds[, c(15, seq(1:14), 16, 17)]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
plot(mymldr, type = "LH")
plot(mymldr, type = "LB")
plot(mymldr, type = "CH")
plot(mymldr, type = "LSB")
plot(mymldr, type = "AT")
plot(mymldr, type = "LSH")
resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
my_predictions <- mydf[, seq(2, length(names(resds)), 2)]
names(resds)
seq(2, length(names(resds)), 2)
length(names(resds))
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
res <- mldr_evaluate(mymldr, my_predictions)
names(mydf)
head(mydf)
head(my_predictions)
typeof(my_predictions)
typeof(my_predictions$Scored.Labels)
as.matrix(my_predictions)
asm<-as.matrix(my_predictions)
res <- mldr_evaluate(mymldr, asm)
trueLabels <- mldr$dataset[, mldr$labels$index]
trueLabels <- mymldr$dataset[, mymldr$labels$index]
trueLabels
names(trueLabels)
length(trueLabels)
length(my_predictions)
names(my_predictions)
names(resds)
ds[1:10, order(names(ds))]
mydf[1:10, order(names(mydf))]
mydf[1:10, order(names(mydf))]
ds[1:10, order(names(ds))]
names(ds)
names(ds$newsid)
ds[1:10, -c(15)]
ds[1:10, -c("newsid")]
names(ds)
ds[1:10, c("newsid")]
ds[1:10, !c("newsid")]
ds[1:10, !=c("newsid")]
library(dplyr)
ds[1:10,] %>% select(newsid)
ds[1:10, -which(names(ds) == "newsid")]
order(ds[1:10, -which(names(ds) == "newsid")])
ds[1:10, -which(names(ds) == "newsid")]
ds[1:10, order(names(ds))]
order(names(ds[1:10, -which(names(ds) == "newsid")]))
order(names(ds[1:10, -which(names(ds) == "newsid")]))
ds[1:10,order(names(ds[1:10, -which(names(ds) == "newsid")]))]
length(order(names(ds[1:10, -which(names(ds) == "newsid")])))
order(names(ds[1:10, -which(names(ds) == "newsid")]))
lapply(order(names(ds[1:10, -which(names(ds) == "newsid")])), +1)
order(names(ds[1:10, - which(names(ds) == "newsid")]))
order(names(ds[1:10, - which(names(ds) == "newsid")])) + 1
order(names(ds[1:10, - which(names(ds) == "newsid")])) + 1
order(names(ds[1:10, - which(names(ds) == "newsid")]))
order(names(ds[1:10,]) )
order(names(ds[1:10,]))
ds[1:10,order(names(ds[1:10,]))]
mydf[1:10, order(names(ds[1:10,]))]
mydf <- ds[1:10, order(names(ds[1:10,]))]
which(colnames(ds) == c("newsid"))
goo <- which(colnames(ds) == c("newsid"))
mydf[,seq(1,goo)]
mydf
goo <- which(colnames(ds) == c("newsid"))
goo <- which(colnames(mydf) == c("newsid"))
mydf[,seq(1,goo)]
mydf[,seq(1,goo-1)]
seq(1,15)
seq(1,15)+seq(16:17)
seq(1:15)+seq(16:17)
c(seq(1:15),seq(16:17))
c(seq(1:15),seq(16:17)+15)
c(seq(1:goo-1),seq(16:17)+15)
goo
c(seq(1:goo-1),seq(goo:17)+15)
c(seq(1:goo-1),seq(goo:length(names(ds)))+15)
c(seq(1:goo-1),seq(goo:length(names(ds)))+goo)
c(seq(1:goo-1),seq(goo:length(names(mydf)))+goo)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
order(names(ds[1:10,]))
order(names(ds[1:10, ]))
mydf <- ds[1:10, order(names(ds[1:10,]))]
mydf
which(colnames(mydf) == c("newsid"))
newsid_col <- which(colnames(mydf) == c("newsid"))
c(seq(1:newsid_col))
c(seq(1:newsid_col-1), seq(newsid_col,length(colnames(mydf))) )
c(seq(1:newsid_col-1), seq(newsid_col,length(colnames(mydf))))
c(seq(1:newsid_col-1), seq(newsid_col+1,length(colnames(mydf))))
c(seq(1:newsid_col-1)))
c(seq(1:newsid_col-1))
seq(newsid_col + 1, length(colnames(mydf)) )
seq(newsid_col + 1, length(colnames(mydf)))
colnames(mydf)
newsid_col
c(seq(1:newsid_col-1)))
c(seq(1:newsid_col-1))
c(seq(1:newsid_col))
c(seq(1:(newsid_col-1)))
c(seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))) )
c(seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
mydf <- ds[1:10, order(names(ds[1:10,]))]
mydf
mydf <- ds[1:10000, order(names(ds[1:10,]))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
library(mldr)
mydf <- ds[1:10000, order(names(ds[1:10,]))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
labels(mymldr)
mymldr$labels
summary(mymldr)
c(seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
c(newsid_co, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))
mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
head(mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))])
mydf<- ds[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))])
mydf<- ds[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf
mydf <- ds[1:10000, order(names(ds[1:10,]))]
mydf<- ds[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf
mydf[1:10, c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
newsid_col
mydf[,12]
colnames(mydf)[12]
colnames(mydf)
newsid_col <- which(colnames(mydf) == c("newsd"))
newsid_col
newsid_col <- which(colnames(mydf) == c("newsid"))
newsid_col
newsid_col <- which(colnames(ds) == c("newsid"))
ds[1:10,c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf <- ds[1:10000, order(names(ds[1:10,]))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf[1:10,c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf
mydf[1:10,c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
mymldr$labels
names(resds)
names(resds)
colnames(resds)
which(colnames(resds) != "Scored")
which(colnames(resds) != "Scored*")
resds %>% select(contains"Scored")
resds %>% select(contains("Scored"))
library(magrittr)
resds %>% select(contains("Scored"))
library(dplyr)
resds %>% select(contains("Scored"))
head(resds %>% select(contains("Scored")))
head(resds %>% select(!contains("Scored")))
head(resds %>% select(-contains("Scored")))
resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
seq(1:length(resds))
seq(2, 1:length(resds))
seq(1:length(resds))
seq(1:length(resds), 2)
seq(1:length(resds))
seq(2, length(names(resds)), 2)
seq(1,length(resds))
seq(1,length(resds),2 )
seq(2,length(resds),2 )
my_predictions <- resds[, seq(2, length(names(resds)), 2)]seq(2,length(resds),2 )
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
names(my_predictions)
my_predictions <- resds[, order(names(my_predictions))]
names(my_predictions)
my_predictions <- resds[, seq(2, length(names(resds)), 2)]names(my_predictions)
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
names(my_predictions)
names(mydf)
names(my_predictions)
names(mydf)
names(resds)
length(names(resds))
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
names(my_predictions)
mydf <- ds[1:10000, order(names(ds[1:10,]))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
names(mydf)
names(my_predictions)
mydf <- ds[, order(names(ds[1:10,]))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:17))
summary(mymldr)
plot(mymldr, type = "LC")
my_predictions <- resds[, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
length(names(my_predictions))
length(names(mydf))
names(mydf)
mymldr <- mldr_from_dataframe(mydf, c(2:length(mydf)))
summary(mymldr)
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
res
plot(res)
res
dim(my_predictions)
mydf <- ds[1:1000, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
plot(mymldr, type = "LC")
my_predictions <- resds[1:1000, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
head(my_predictions)
str(my_predictions)
res
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
subset_size <- 10000;
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
res
subset_size <- 100;
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
res
warnings()
my_predictions
max(my_predictions)
max(my_predictions$FCL.labels)
sum(my_predictions$FCL.labels)
sum(my_predictions$BSC.labels)
sum(my_predictions$IDU.labels)
warnings()
predictions <- as.matrix(emotions$dataset[, emotions$labels$index]) # and introduce some noise (alternatively get the predictions from some classifier) predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE) # then evaluate predictive performance res <- mldr_evaluate(emotions, predictions) str(res) plot(res$ROC, main = "ROC curve for emotions")
emotions
dim(emotions)
dim(predictions)
typeof(emotions)
typeof(predictions)
typeof(my_predictions)
head(my_predictions)
head(as.matrix(my_predictions))
head(predictions)
typeof(predictions)
typeof(my_predictions)
res <- mldr_evaluate(emotions, predictions)
str(res)
plot(res$ROC, main = "ROC curve for emotions")
predictions <- as.matrix(emotions$dataset[, emotions$labels$index])
res <- mldr_evaluate(emotions, predictions)
str(res)
plot(res$ROC, main = "ROC curve for emotions")
predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE)
res <- mldr_evaluate(emotions, predictions)
str(res)
library(mldr) predictions <- as.matrix(emotions$dataset[, emotions$labels$index]) # and introduce some noise (alternatively get the predictions from some classifier) predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE) # then evaluate predictive performance res <- mldr_evaluate(emotions, predictions) str(res) plot(res$ROC, main = "ROC curve for emotions")
library(mldr) predictions <- as.matrix(emotions$dataset[, emotions$labels$index]) # and introduce some noise (alternatively get the predictions from some classifier) predictions[sample(1:593, 100), sample(1:6, 100, replace = TRUE)] <- sample(0:1, 100, replace = TRUE) # then evaluate predictive performance res <- mldr_evaluate(emotions, predictions) str(res) plot(res$ROC, main = "ROC curve for emotions")
sessionInfo
sessionInfo()
str(res)
str(res)library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds) resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" )
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "59233455-00b1-437e-9052-d1350e968eea-1236753",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
mydf <- ds[1:subset_size, order(colnames(ds))] newsid_col <- which(colnames(mydf) == c("newsid")) mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))] mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf)))) summary(mymldr)
library(mldr)
subset_size <- 100;
mydf <- ds[1:subset_size, order(colnames(ds))]
newsid_col <- which(colnames(mydf) == c("newsid"))
mydf <- mydf[c(newsid_col, seq(1:(newsid_col - 1)), seq(newsid_col + 1, length(colnames(mydf))))]
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
summary(mymldr)
resds <- download.intermediate.dataset(   ws = ws,   node_id = "470e2bcd-fcba-4498-81aa-29f309f6a05a-12901",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.74b01b36f2644d5fa3db68d8c05272a6",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(resds)
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
warnings()
res
mymldr@labels
mymldr
str(mymldr)
mymldr <- mldr_from_dataframe(mydf, labelIndices = c(2:length(names(mydf))), name = "foo")
mymldr$labels
mymldr <- mldr_from_dataframe(mydf, c(2:length(names(mydf))))
mymldr$labels
my_predictions <- resds[1:subset_size, seq(2, length(names(resds)), 2)]
my_predictions <- my_predictions[, order(names(my_predictions))]
res <- mldr_evaluate(mymldr, my_predictions)
res
warnings()
is.na(my_predictions)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
library(readr)
install.packages("readr")
exp <- read_csv("~/temp/Experiment created on _31_._07_._2017 - 506153734175476c4f62416c57734963.faa6ba63383c4086ba587abf26b85814.v1-default-1643 - Results data.csv")
library(readr)
exp <- read_csv("~/temp/Experiment created on _31_._07_._2017 - 506153734175476c4f62416c57734963.faa6ba63383c4086ba587abf26b85814.v1-default-1643 - Results data.csv")
exp <- read_csv("c:\\users\\lars bungum\\documents\exp.csv")
exp <- read_csv("c:\\users\\lars bungum\\documents\\exp.csv")
getwd()
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-209632",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) dim(ds)
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-209632",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-216896",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
names(ds)
dim(ds)
head(ds)
names(ds)
ds[1:2]
ds[1:2,]
ds[1:3,]
names(ds)
dim(ds)
library(tm)
b <- VectorSource(ds$dj_clean)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
# Clean corpus
dj.corpus
dj.corpus.clean <- tm_map(docs, content_transformer(tolower)) %>% tm_map(docs, removeNumbers) %>% tm_map(docs, removeWords, stopwords("english")) %>% tm_map(docs, removePunctuation) %>% tm_map(docs, stripWhitespace) %>% tm_map(docs, stemDocument, language = "english")
# Create dtm
library(magrittr)
# Clean corpus
dj.corpus.clean <- tm_map(docs, content_transformer(tolower)) %>% tm_map(docs, removeNumbers) %>% tm_map(docs, removeWords, stopwords("english")) %>% tm_map(docs, removePunctuation) %>% tm_map(docs, stripWhitespace) %>% tm_map(docs, stemDocument, language = "english")
dj.corpus.clean <- tm_map(docs, content_transformer(tolower)) %>% tm_map(removeNumbers) %>% tm_map(removeWords, stopwords("english")) %>% tm_map(removePunctuation) %>% tm_map(stripWhitespace) %>% tm_map(stemDocument, language = "english")
dj.corpus.clean <- tm_map(dj.corpus, content_transformer(tolower)) %>% tm_map(removeNumbers) %>% tm_map(removeWords, stopwords("english")) %>% tm_map(removePunctuation) %>% tm_map(stripWhitespace) %>% tm_map(stemDocument, language = "english")
# Create dtm
dj.corpus.clean
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus.clean, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.01917]
summary(slam::col_sums(djreduced.dtm))
# Run model
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
library(topicmodels)
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
install.packages("topicmodels")
library(topicmodels)
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.020380]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.010380]
djreduced.dtm
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.10380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.20380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.30380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.50380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.70380]
djreduced.dtm
system.time(dj.model <- topicmodels::LDA(djreduced.dtm, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
rowTotals <- apply(djreduced.dtm, 1, sum)
head(rawTotals)
head(rowTotals)
dtm.new <- djreduced.dtm[rowTotals > 0,]
dtm.new
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.02380]
summary(slam::col_sums(djreduced.dtm))
rowTotals <- apply(djreduced.dtm, 1, sum)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.52380]
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
dtm.new
system.time(dj.model <- topicmodels::LDA(dtm.new, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
dj.topics <- topicmodels::topics(llis.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
# label topics
topicTerms <- tidyr::gather(dj.terms, Topic)
topicTerms <- cbind(topicTerms, Rank = rep(1:30))
topTerms <- dplyr::filter(topicTerms, Rank < 4)
topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))
topTerms$Topic <- as.numeric(topTerms$Topic)
topicLabel <- data.frame()
for (i in 1:27) {     z <- dplyr::filter(topTerms, Topic == i)     l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "), stringsAsFactors = FALSE)     topicLabel <- rbind(topicLabel, l) }
colnames(topicLabel) <- c("Label")
topicLabel
# Visualize LDA model topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(djreduced.dtm)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) }
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
library(LDAvis)
install.packages("LDAvis")
library(LDAvis)
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
serVis(dj.json)
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, dtm.new)
djcorp.new <- dj.corpus[rowTotals > 0,]
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) ds <- download.intermediate.dataset(   ws = ws,   node_id = "d69d0e83-2a6c-4790-83a2-b0673f017e06-10253",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(ds)
names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.22369dfa75d34c2282f7e993c2d46dcc",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(ds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
names(ds)
ds[order(dsignrank$Topic.1),]
ds[order(ds$Topic.1),]
tail(ds[order(ds[, 3]), c(1, 3)])
tail(ds[order(ds[, 3]), c(1, 2)])
tail(ds[order(ds[, 1]), c(1, 2)])
tail(ds[order(ds[, 2]), c(1, 2)])
tail(ds[order(ds[, 3]), c(1, 3)])
tail(ds[order(ds[, 4]), c(1, 4)])
tail(ds[order(ds[, 5]), c(1, 5)])
tail(ds[order(ds[, 6]), c(1, 6)])
tail(ds[order(ds[, 7]), c(1, 7)])
tail(ds[order(ds[, 7]), c(1, 8)])
tail(ds[order(ds[, 7]), c(1, 6)])
tail(ds[order(ds[, 7]), c(1, 7)])
tail(ds[order(ds[, 8]), c(1, 8)])
tail(ds[order(ds[, 9]), c(1, 9)])
tail(ds[order(ds[, 10]), c(1, 10)])
dim(ds)
t(ds)
phi <- t(ds)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
filename <- "thetaDS_LDA_Prep.csv"
foo <- read.csv(filename)
filename <- "c:\\users\\bungun\\documents\\thetaDS_LDA_Prep.csv"
foo <- read.csv(filename)
filename <- "c:\\users\\bungum\\documents\\thetaDS_LDA_Prep.csv"
foo <- read.csv(filename)
library(readr)
read_csv(filename)
thethads <- read_csv(filename)
dim(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
filename <- "c:\\users\\bungum\\documents\\thetaTSV.tsv"
thetads <- read.csv(filename, header = T, sep = "\t")
names(thethads)
names(thethads)[1:10]
names(phi)
dim(phi)
phi[1:10,1:10]
phi[1,1]
head(thetads)
thetads[1:10,1:10]
phi[1:10,1:10]
phi.df <- as.data.frame(phi)
names(phi.df)
dim(phi.df)
dim(phi)
colnames(phi)
attributes(phi)
phi[1:10,1:10]
phi[1,1:10]
phi.df[1,1:10]
colnames(phi.df) <- phi.df[1,]
colnames(phi.df)
phi.df[1,1:10]
phi[1,1:10]
foo <- phi[1,]
typeof(foo)
colnames(phi.df)
dim(colnames(phi.df))
length(colnames(phi.df))
length(foo)
colnames(phi.df) <- foo
colnames(phi.df)
phi.df[1,1:10]
bar <- phi.df[-1,1:10]
dim(bar)
bar <- phi.df[-1,]
bar
dim(bar)
colnames(bar)
phi.done.df <- bar
dim(phi.done.df)
dim(theta)
dim(thetads)
dim(ds)
head(ds)
ds[1,1:10]
dim(ds)
thetads <- t(ds[,2:501])
dim(thetads)
dim(phi.dine.df)
dim(phi.done.df)
dim(vocab)
vocab <- foo
length(foo)
foo[1:10]
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-27360",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
filename_TF <- "c:\\users\\bungum\\documents\\TF_LDA.tsv"
TF_LDA <- read.csv(filename_tf, header = T, sep = "\t")
TF_LDA <- read.csv(filename_TF, header = T, sep = "\t")
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109411",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(vocabds)
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
dim(phids)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109411",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(vocabds)
ds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) head(ds)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(phids)
dim(thetads)
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
phi.df <- as.data.frame(phids)
vocab <- phi[1,]
vocab[1:10]
length(vocab)
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
phi.df <- as.data.frame(phids)
# the vocabulary is in the first row
vocab <- phi[1,]
length(vocab)
dim(phids)
phi.df <- as.data.frame(phids)
vocab <- phids[1,]
length(vocab)
names(phids)
dim(phids)
dim(t(phids))
phi <- t(phids)
vocab <- phi[1,]
length(vocab)
colnames(phi.df) <- vocab
phi.df <- as.data.frame(phi)
# the vocabulary is in the first row
vocab <- phi[1,]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
# get the first row out of the data frame
phi.done.df <- phi.df[-1,]
thetads <- t(ds[, 2:31])
dim(vocabds)
names(vocabds)
names(vocabds)[1:10]
vocabds[1:10,1:10]
vocabds[1:5,1:5]
vocabds[2:5,1:5]
vocabds[1:5,2:5]
foo <- as.data.frame(vocabs)
foo <- as.data.frame(vocabds)
foo <- foo[,-c(1)]
dim(foo)
colnames(foo) <- vocab
names(foo)
foo <- foo[,-c(1)]vocab.df <-  vocab.df[,c(1)]
vocab.df <-  vocab.df[,c(1)]
vocab.df <- as.data.frame(vocabs)
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <-  vocab.df[,c(1)]
colnames(vocab.df) <- vocab
vocab.df <- vocab.df[, c(1)]
dim(vocab.df)
vocab.df <- as.data.frame(vocabds)
dim(vocab.df)
vocab.df <- vocab.df[, -c(1)]
dim(vocab.df)
colnames(vocab.df) <- vocab
head(vocab.df)
colSums(vocab.df)
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
                           Freq = colSums(vocab.df))
head(freq_matrix)
names(thetads)[1:2]
names(phids)[1:2]
phids[1:10,1]
thetads[1,1]
ds[1,1]
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
dim(lengthds)
typeof(lengthds)
vector(lengthds)
lengthds$length.Preprocessed.dj_clean.  
lengthds$length.Preprocessed.dj_clean. <- as.numeric(lengthds$length.Preprocessed.dj_clean.)
vector(lengthds)
vector(lengthds$length.Preprocessed.dj_clean.)
as.matrix(lengthds$length.Preprocessed.dj_clean.)
foo <- as.matrix(lengthds$length.Preprocessed.dj_clean.)
foo
typeof(foo)
typeof(as.integer(foo))
length:mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))foo <- as.matrix(lengthds$length.Preprocessed.dj_clean.)
length_mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))
typeof(length_mat)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.done.df)
length(vocab)
dim(thetads)
thetads <- t(ds[, 2:31])
dim(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-1187",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(thetads)
thetads <- t(thetads)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(thetads)
dim(phi.done.df)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(thetads)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
                                    term.frequency = freq_matrix$Freq)
rowSums(phi.done.df)
str(phi.done.df)
typeof(phi.done.df[1,])
typeof(phi.done.df[1,1])
typeof(phi.done.df[1,])
typeof(phi.done.df[2,])
names(phi.done.df-d)
names(phi.done.df)
dim(phi.done.df)
typeof(phi.done.df[,1])
typeof(phi.done.df[,2])
typeof(phi.done.df[,3])
typeof(phi.done.df[,4])
sum(hi.done.df[,4])
sum(phi.done.df[,4])
sum(as.integer(phi.done.df[,4]))
sum(as.numeric(phi.done.df[,4]))
phi.done.df[,4])
phi.done.df[,4])
phi.done.df[,4]
rowSum(phi.done.df[,4])
rowSums(phi.done.df[,4])
sum(phi.done.df[,4])
str(phi.done.df[,4])
str(as.float(phi.done.df[,4]))
as.numeric(as.float(phi.done.df[,4]))
as.numeric(phi.done.df[,4])
phi.done.df[,4]
phi.done.df[1,4]
names(phi.done.df)
phi.done.df[1,4]
foo <- as.matrix(phi.done.df)
dim(foo)
names(foo)
head(foo)
colnames(foo)
rowSums(foo)
typeof(foo)
rowSums(as.numeric(foo))
bar <- as.numeric(foo))
bar <- as.numeric(foo)
dim(bar)
names(bar)
bar[1:10,1:10]
dim(bar)
bar <- as.numeric(foo)
dim(bar)
as.numeric(foo)
foo <- as.matrix(phi.done.df)
tyoeof(foo)
typeof(foo)
typeof(foo[,1])
sum(foo[,1])
sum(as.numeric(foo[,1]))
as.numeric(foo[,1])
sum(as.numeric(foo[,1]))
sum(as.numeric(phi.done.df[,1]))
as.numeric(phi.done.df[,1])
phi_mat <- as.matrix(phi.done.df)foo <- as.matrix(phi.done.df)
phi_mat <- as.matrix(phi.done.df)
phi_mat <- sapply(phi_mat, as.numeric)
rowSums(phi_mat)
rowSums(as.data.frame(phi_mat))
dim(as.data.frame(phi_mat))
dim(phi_mat)
length(phi_mat)
phi_mat <- as.matrix(phi.done.df)
dim(phi_mat)
phi_mat <- sapply(phi_mat, as.numeric)
dim(phi_mat)
phi_mat <- as.matrix(phi.done.df)
dim(sapply(phi_mat, as.numeric))
dim(lapply(phi_mat, as.numeric))
dim(apply(phi_mat, as.numeric))
df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
lapply(phi.done.df, function(x) as.numeric(as.character(x)))
phi.done.num.df <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
rowSums(phi.done.num.df)
dim(phi.done.num.df)
length(phi.done.num.df)
phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
dim(phi.done.df)
rowSums(phi.done.df)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
phi <- t(phids)
#convert the phi you read in as a data frame
phi.df <- as.data.frame(phi)
# the vocabulary is in the first row
vocab <- phi[1,]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
# get the first row out of the data frame
phi.done.df <- phi.df[-1,]
# Convert all columns from factors to numeric
phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
# make the theta dataset out of he transpose of the vocabulary and the 500 last rows (minus the feature ID).  NB the topic number is hard coded in her
thetads <- t(thetads)
# Convert the vocabds into a data.frame
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))
# Convert phi to matrix
phi_mat <- as.matrix(phi.done.df)
phi_mat <- sapply(phi_mat, as.numeric)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(dj.json)
dim(thetads)
dim(phi.done.df)
thetads <- t(thetads)
dim(thetads)
json_lda <- LDAvis::createJSON(phi = phi.done.df, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
head(phi.done.df)
phi.done.df[1,]
sum(phi.done.df[1,])
rowS
rowSums(phi.done.df)
rowSums(phi.done.df)==1
rowSums(phi.done.df)[1]
rowSums(phi.done.df)[1]==1
rowSums(phi.done.df)[2]==1
rowSums(phi.done.df)[2]
rowSums(phi.done.df)[2]==1
typeof(rowSums(phi.done.df)[2])
dim(phids)
colSums(phids[2,:31])
colSums(phids[,2:31])
rowSums(phi)
rowSums(phi[2,31,])
dim(phi)
typeof(phi[2,31,])
typeof(phi[2:31,])
typeof(phids[,2:31])
rowSums(phi[2,31,])
colSums(phids[,2:31])
typeof(phids[,2:31])
typeof(phids[,2:31])
colSums(phids[,2:31])
typeof(phids[,2:31])
typeof(phids[1,2:31])
typeof(phids[1,2])
colSums(phids[,2:31])
rowSums(phi)
rowSums(phi[2:30,1])
rowSums(phi[2:30,])
rowSums(as.numeric(phi[2:30,]))
dim(phids)
foo <- phids[,2:31]
colSums(foo)
rowSums(t(foo))
phi <- t(phids[,2:31])
json_lda <- LDAvis::createJSON(phi = phi, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(dj.json)
dim(thetads)
rowSums(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
dim(thetads)
rowSums(thetads)
typeof(thetads)
typeof(thetads[,1])
typeof(thetads[,2])
sum(thetads[,2])
sum(thetads[1,])
sum(thetads[2,])
sum(thetads[2,])==1
sum(thetads[2,])
thetads[2,]
dim(thetads)
dim(thetads[2,])
length(thetads[2,])
colSums(thetads)
rowSums(thetads)
length(thetads[2,])
length(thetads[,2])
sum(thetads[,2])
dim(thetads)
sum(thetads[1,])
sum(thetads[2,])
rowSums(thetads[2,])
rowSums(thetads)
typeof(thetads)
typeof(thetads[1,2])
typeof(thetads[1,])
sum(thetads[1,])
colSums(phi_transpose_nums)
phi_transpose_nums <- t(phids[,2:31])
colSums(phi_transpose_nums)
rowSums(phi_transpose_nums)
rowSums(phi_transpose_nums)
rowSums(thetads)
colSums(thetads)
rowSums(thetads)
rowSums(phi_transpose_nums)
rowSums(phi_transpose_nums) == 1
trace(createJSON, edit = T)
all.equal(rowSums(phi_transpose_nums), rep(1, K), check.attributes = FALSE)
trace(createJSON, edit = T)
dt <- dim(theta)
K = dt[2]
dt <- dim(thetads)
K = dt[2]
K
phi.test <- all.equal(rowSums(phi), rep(1, K), check.attributes = FALSE)
theta.test <- all.equal(rowSums(theta), rep(1, dt[1]), check.attributes = FALSE)
phi.test <- all.equal(rowSums(phi_transpose_nums), rep(1, K), check.attributes = FALSE)
theta.test <- all.equal(rowSums(thetads), rep(1, dt[1]), check.attributes = FALSE)
phi.test
theta.test
trace(createJSON, edit = T)
trace(createJSON, edit = T)
trace(createJSON, edit = T)
rowSums(thetads)
rowSums(thetads)[1:10]
rowSums(thetads)[1:1]
equal(rowSums(thetads)[1:1],1)
all.equal(rowSums(thetads)[1:1],1)
all.equal(rowSums(thetads)[2],1)
all.equal(rowSums(thetads)[3],1)
all.equal(rowSums(phi_transpose_sums)[3],1)
all.equal(rowSums(phi_transpose_nums)[3],1)
all.equal(rowSums(phi_transpose_nums)[1],1)
rowSums(phi_transpose_nums)[1]
rowSums(phi_transpose_nums)[2]
rowSums(phi_transpose_nums)[3]
phi_transpose_nums[1,]
sum(phi_transpose_nums[1,])
all.equal(rowSums(phi_transpose_nums)[1],1)
all.equal(rowSums(phi_transpose_nums)[1],1) == T
phi.test
all.equal(rowSums(phi_transpose_nums)[1],1, chec.attributes= F) == T
all.equal(rowSums(phi_transpose_nums)[1],1, check.attributes= F) == T
all.equal(rowSums(thetads)[1],1, check.attributes= F) == T
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
all.equal(rowSums(thetads)[3],1, check.attributes= F) == T
all.equal(rowSums(thetads)[4],1, check.attributes= F) == T
theta.test
theta.test == T
phi.test == T
thetads[1,]
sum(thetads[1,])
length(thetads[1,])
sum(thetads[1,])
1-sum(thetads[1,])
thethads[1,30] <- thetads[1,30]+(1-sum(thetads[1,]))
thetads[1,]
sum(thetads[1,])
thetads[1,30]+(1-sum(thetads[1,]))
thethads[1,30] <- (thetads[1,30]+(1-sum(thetads[1,])))
thethads[1,30]
sum(thetads[1,])
1-sum(thetads[1,])
thethads[1,30] <- (thetads[1,30]+(1-sum(thetads[1,])))
1-sum(thetads[1,])
thethads[1,30]
thethads[1,30] <- (thetads[1,30]+(1-sum(thetads[1,])))
thethads[1,30]
1-sum(thetads[1,])
(thetads[1,30]+(1-sum(thetads[1,])))
thethads[1,30]
1-sum(thetads[1,])
thetads[1, 30] + 1.156e-07
thetads[1, 30]
thetads[1, 30] + 1.156e-07
thetads[1, 30]
thetads[1, 30] <- thetads[1, 30] + 1.156e-07
thetads[1, 30]
1-sum(thetads[1,])
all.equal(rowSums(thetads)[1],1, check.attributes= F) == T
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
1-sum(thetads[2,])
1-sum(thetads[2,])[[1]]
1-sum(thetads[2,])[1]
thetads[2,30] + (1-sum(thetads[2,])[1])
thetads[2,30]
(1-sum(thetads[2,])[1])
thetads[2, 30] + 4.13e-08
thetads[2,30] + (1-sum(thetads[2,])[1])
thetads[2,30] + (1-sum(thetads[2,]))
thetads[2,30] + (1-sum(thetads[2,]))
thetads[2,30] 
thetads[2,30] + (1-sum(thetads[2,]))
thetads[2,30] <- thetads[2,30] + (1-sum(thetads[2,]))
thetads[2, 30]
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
dim(thetads)
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(thetads)
    R <- dt[1]
    for (r in 1:R) {
        df[2, 30] <- df[2, 30] + (1 - sum(df[2,]))
    }
}
impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[2, 30] <- df[2, 30] + (1 - sum(df[2,]))
    }
    return df
}
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[2, 30] <- df[2, 30] + (1 - sum(df[2,]))
    }
    return (df)
}
foo <- impute_df_to_sum_to_one(thetads)
dim(foo)
theta.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
theta.test
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[R, 30] <- df[R, 30] + (1 - sum(df[R,]))
    }
    return (df)
}
theta.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
theta.test
foo <- impute_df_to_sum_to_one(thetads)
theta.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
theta.test
dt <- dim(df)
R <- dt[1]
R
dt
dt <- dim(thetads)
R <- dt[1]
R
thetads[2,30] + (1-sum(thetads[2,]))
(1-sum(thetads[2,]))
(1-sum(thetads[3,]))
(1-sum(thetads[3,]))
thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3, 30]
thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3,30] <- thetads[3, 30]+(1-sum(thetads[3,]))
thetads[3,30] 
foo <- impute_df_to_sum_to_one(thetads)
all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
all.equal(rowSums(thetads)[2],1, check.attributes= F) == T
all.equal(rowSums(foo)[2],1, check.attributes= F) == T
all.equal(rowSums(foo)[3],1, check.attributes= F) == T
all.equal(rowSums(foo)[4],1, check.attributes= F) == T
thetads[4,30] <- thetads[4, 30]+(1-sum(thetads[4,]))
all.equal(rowSums(thetads)[4],1, check.attributes= F) == T
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    for (r in 1:R) {
        df[R, 30] <- df[R, 30] + (1 - sum(df[R,]))
        all.equal(rowSums(df)[R], 1, check.attributes = F) == T
    }
    return (df)
}
foo <- impute_df_to_sum_to_one(thetads)
foo
foo == thetads
impute_df_to_sum_to_one <- function(df) {
    dt <- dim(df)
    R <- dt[1]
    df_out <- df
    for (r in 1:R) {
        df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))
        all.equal(rowSums(df)[R], 1, check.attributes = F) == T
    }
    return (df_out)
}
foo <- impute_df_to_sum_to_one(thetads)
foo == thetads
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
foo <- imp
foo <- impute_df_to_sum_to_one(thetads)
foo == thetads
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(R)         df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
impute_df_to_sum_to_one(thetads[1:19,])
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[R, 30] <- df[R, 30] + (1 - sum(df[R,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
f <- impute_df_to_sum_to_one(thetads[1:19,])
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         #all.equal(rowSums(df)[R], 1, check.attributes = F) == T     }     return (df_out) }
f <- impute_df_to_sum_to_one(thetads[1:19,])
f <- impute_df_to_sum_to_one(thetads)
f == thetads
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         print(df_out)     }     return (df_out) }
foo <- impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         print(temp)     }     return (df_out) }foo <- impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         print(temp)     }     return (df_out) }
foo <- impute_df_to_sum_to_one(thetads)
dt <- dim(thetads)
K = dt[2]
theta.test <- all.equal(rowSums(thetads), rep(1, dt[1]), check.attributes = FALSE)
theta.test
foo.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
foo.test
foo.test
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
phi_transpose_nums <- t(phids[,2:31])
colnames(phi.df) <- vocab
vocab <- phi[1,]
rownames(phids)
colnames(phids)
phi.df <- as.data.frame(phi_transpose_nums)
colnames(phi.df) <- vocab
names(phids)
phids[1,]
phids[,1]
vocab <- phids[,1]
colnames(phi.df) <- vocab
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.matrix(as.integer(lengthds$length.Preprocessed.dj_clean.))
# Convert phi to matrix
phi_mat <- as.matrix(phi.done.df)
phi_mat <- sapply(phi_mat, as.numeric)
json_lda <- LDAvis::createJSON(phi = phi_transpose_nums, theta = thetads,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(dj.json)
dt <- dim(thetads)
K = dt[2]
phi.test <- all.equal(rowSums(phi_transpose_nums), rep(1, K), check.attributes = FALSE)
theta.test <- all.equal(rowSums(thetads), rep(1, dt[1]), check.attributes = FALSE)
foo.test <- all.equal(rowSums(foo), rep(1, dt[1]), check.attributes = FALSE)
foo.test
json_lda <- LDAvis::createJSON(phi = phi_transpose_nums, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(freq_matrix)
dim(doc.length)
dim(length_mat)
length(vocab)
trace("createJSON", edit = TRUE)
library(LDAvis)
trace("createJSON", edit = TRUE)
vocab_df[order(saliency, decreasing = TRUE)][1:R]
vocab
vocab.df[order(saliency, decreasing = TRUE)][1:R]
trace("createJSON", edit = TRUE)
dim(phi_transpose_nums)
dim(foo)
dim(freq_matrix)
length(length_mat)
str(length_mat)
json_lda <- LDAvis::createJSON(phi = phi_transpose_nums, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
trace("createJSON", edit = TRUE)
topic.frequency <- colSums(foo * length_mat)
topic.proportion <- topic.frequency / sum(topic.frequency)
o <- order(topic.proportion, decreasing = TRUE)
phi <- phi_transpose_nums[o,]
theta <- thetads[,o]
topic.frequency <- topic.frequency[o]
topic.proportion <- topic.proportion[o]
term.topic.frequency <- phi * topic.frequency
term.frequency <- colSums(term.topic.frequency)
stopifnot(all(term.frequency > 0))
term.proportion <- term.frequency / sum(term.frequency)
phi <- t(phi)
topic.given.term <- phi / rowSums(phi)
kernel <- topic.given.term * log(sweep(topic.given.term, MARGIN = 2, topic.proportion, `/`))
distinctiveness <- rowSums(kernel)
saliency <- term.proportion * distinctiveness
default.terms <- vocab[order(saliency, decreasing = TRUE)][1:R]
default.terms <- vocab[order(saliency, decreasing = TRUE)][1:30]
default.terms
    counts <- as.integer(term.frequency[match(default.terms,
        vocab)])
    Rs <- rev(seq_len(R))
Rs <- rev(seq_len(30))
    default <- data.frame(Term = default.terms, logprob = Rs,
        loglift = Rs, Freq = counts, Total = counts, Category = "Default",
        stringsAsFactors = FALSE)
    topic_seq <- rep(seq_len(K), each = R)
topic_seq <- rep(seq_len(K), each = 30)
    category <- paste0("Topic", topic_seq)
    lift <- phi / term.proportion
    find_relevance <- function(i) {
        relevance <- i * log(phi) + (1 - i) * log(lift)
        idx <- apply(relevance, 2, function(x) order(x, decreasing = TRUE)[seq_len(R)])
        indices <- cbind(c(idx), topic_seq)
        data.frame(Term = vocab[idx], Category = category, logprob = round(log(phi[indices]),
            4), loglift = round(log(lift[indices]), 4), stringsAsFactors = FALSE)
    }
    lambda.seq <- seq(0, 1, by = lambda.step)
lambda.step = 0.01, mds.method = jsPCA, cluster
lambda.step = 0.01, mds.method = jsPCA
lambda.step = 0.01
    lambda.seq <- seq(0, 1, by = lambda.step)
    if (missing(cluster)) {
        tinfo <- lapply(as.list(lambda.seq), find_relevance)
    }
    else {
        tinfo <- parallel::parLapply(cluster, as.list(lambda.seq),
            find_relevance)
    }
    tinfo <- unique(do.call("rbind", tinfo))
cluster
    if (missing(cluster)) {
        tinfo <- lapply(as.list(lambda.seq), find_relevance)
    }
default
    if (missing(cluster)) {
        tinfo <- lapply(as.list(lambda.seq), find_relevance)
    }
cluster = list()
tinfo <- lapply(as.list(lambda.seq), find_relevance)
    find_relevance <- function(i) {
        relevance <- i * log(phi) + (1 - i) * log(lift)
        idx <- apply(relevance, 2, function(x) order(x, decreasing = TRUE)[seq_len(30)])
        indices <- cbind(c(idx), topic_seq)
        data.frame(Term = vocab[idx], Category = category, logprob = round(log(phi[indices]),
            4), loglift = round(log(lift[indices]), 4), stringsAsFactors = FALSE)
    }
tinfo <- lapply(as.list(lambda.seq), find_relevance)
    tinfo <- unique(do.call("rbind", tinfo))
    tinfo$Total <- term.frequency[match(tinfo$Term, vocab)]
    rownames(term.topic.frequency) <- paste0("Topic", seq_len(K))
    colnames(term.topic.frequency) <- vocab
    tinfo$Freq <- term.topic.frequency[as.matrix(tinfo[c("Category",         "Term")])]
    tinfo <- rbind(default, tinfo)
    ut <- sort(unique(tinfo$Term))
    m <- sort(match(ut, vocab))
    tmp <- term.topic.frequency[, m]
    r <- row(tmp)[tmp >= 0.5]
    c <- col(tmp)[tmp >= 0.5]
    dd <- data.frame(Term = vocab[m][c], Topic = r, Freq = round(tmp[cbind(r,         c)]), stringsAsFactors = FALSE)
    dd[, "Freq"] <- dd[, "Freq"] / term.frequency[match(dd[, "Term"],         vocab)]
    token.table <- dd[order(dd[, 1], dd[, 2]),]
    RJSONIO::toJSON(list(mdsDat = mds.df, tinfo = tinfo, token.table = token.table,         R = R, lambda.step = lambda.step, plot.opts = plot.opts,         topic.order = o))
}
    mds.res <- mds.method(phi)
mds.method = jsPCA
    mds.res <- mds.method(phi)
str(phi)
dim(phi)
phi.df <- as.data.frame(phi)
dim(phi.df)
names(phi.df)
dim(phi)
dim(phids)
dim(phi)
dim(phi.df)
t(phi.df)
dim(t(phi.df))
names(t(phi.df))
dim(as.data.frame((t(phi.df))))
names(as.data.frame((t(phi.df))))
phi.df <- as.data.frame((t(phi.df))))
phi.df <- as.data.frame((t(phi.df)))
colnames(phi.df) <- vocab
jsPCA(phi.df)
dim(phi_transpose_nums)
dim(ph.df)
dim(phi.df)
str(phi_transpose_nums)
jsPCA(phi_transpose_nums)
jsPCA(phi.df)
jsPCA(phi.df)json_lda <- LDAvis::createJSON(phi = phi.df, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = foo,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.df)
typeof(phi.df[1,])
typeof(phi.df[1,1])
jsPCA(phi.df[,1:1000])
jsPCA(phi.df[,1:10000])
system.time(jsPCA(phi.df[,]))
system.time(foo <- jsPCA(phi.df[,]))
dim(foo)
theta_imputed <- impute_df_to_sum_to_one(thetads)
phi=phi.df
theta = theta_imputed
doc.length = length.mat
doc.length = length_mat
dim(vocab)
length(vocab)
    dp <- dim(phi)
    dt <- dim(theta)
    N <- sum(doc.length)
    W <- length(vocab)
    D <- length(doc.length)
    K <- dt[2]
    if (dp[1] != K)
        stop("Number of rows of phi does not match \n      number of columns of theta; both should be equal to the number of topics \n      in the model.")
    if (D != dt[1])
        stop("Length of doc.length not equal \n      to the number of rows in theta; both should be equal to the number of \n      documents in the data.")
    if (dp[2] != W)
        stop("Number of terms in vocabulary does \n      not match the number of columns of phi (where each row of phi is a\n      probability distribution of terms for a given topic).")
    if (length(term.frequency) != W)
        stop("Length of term.frequency \n      not equal to the number of terms in the vocabulary.")
    if (any(nchar(vocab) == 0))
        stop("One or more terms in the vocabulary\n      has zero characters -- all terms must have at least one character.")
    phi.test <- all.equal(rowSums(phi), rep(1, K), check.attributes = FALSE)
    theta.test <- all.equal(rowSums(theta), rep(1, dt[1]), check.attributes = FALSE)
    if (!isTRUE(phi.test))
        stop("Rows of phi don't all sum to 1.")
    if (!isTRUE(theta.test))
        stop("Rows of theta don't all sum to 1.")
    topic.frequency <- colSums(theta * doc.length)
    topic.proportion <- topic.frequency / sum(topic.frequency)
    o <- order(topic.proportion, decreasing = TRUE)
    phi <- phi[o,]
    theta <- theta[, o]
    topic.frequency <- topic.frequency[o]
    topic.proportion <- topic.proportion[o]
    mds.res <- mds.method(phi)
    if (is.matrix(mds.res)) {         colnames(mds.res) <- c("x", "y")     }
    else if (is.data.frame(mds.res)) {
        names(mds.res) <- c("x", "y")
    }
    else {
        warning("Result of mds.method should be a matrix or data.frame.")
mds.method <- jsPCA
R <- 30
    mds.res <- mds.method(phi)
    if (is.matrix(mds.res)) {         colnames(mds.res) <- c("x", "y")     }
    else if (is.data.frame(mds.res)) {
        names(mds.res) <- c("x", "y")
    }
colnames(mds.res)
    mds.df <- data.frame(mds.res, topics = seq_len(K), Freq = topic.proportion *         100, cluster = 1, stringsAsFactors = FALSE)
    term.topic.frequency <- phi * topic.frequency
    term.frequency <- colSums(term.topic.frequency)
    stopifnot(all(term.frequency > 0))
    term.proportion <- term.frequency / sum(term.frequency)
    phi <- t(phi)
    topic.given.term <- phi / rowSums(phi)
    kernel <- topic.given.term * log(sweep(topic.given.term,         MARGIN = 2, topic.proportion, `/`))
    distinctiveness <- rowSums(kernel)
    saliency <- term.proportion * distinctiveness
    default.terms <- vocab[order(saliency, decreasing = TRUE)][1:R]
    counts <- as.integer(term.frequency[match(default.terms,         vocab)])
    Rs <- rev(seq_len(R))
    default <- data.frame(Term = default.terms, logprob = Rs,         loglift = Rs, Freq = counts, Total = counts, Category = "Default",         stringsAsFactors = FALSE)
    topic_seq <- rep(seq_len(K), each = R)
    category <- paste0("Topic", topic_seq)
    lift <- phi / term.proportion
    find_relevance <- function(i) {         relevance <- i * log(phi) + (1 - i) * log(lift)         idx <- apply(relevance, 2, function(x) order(x, decreasing = TRUE)[seq_len(30)])         indices <- cbind(c(idx), topic_seq)         data.frame(Term = vocab[idx], Category = category, logprob = round(log(phi[indices]),             4), loglift = round(log(lift[indices]), 4), stringsAsFactors = FALSE)     }
    lambda.seq <- seq(0, 1, by = lambda.step)
    if (missing(cluster)) {         tinfo <- lapply(as.list(lambda.seq), find_relevance)     }
    else {
        tinfo <- parallel::parLapply(cluster, as.list(lambda.seq),             find_relevance)
    }
    tinfo <- unique(do.call("rbind", tinfo))
    tinfo$Total <- term.frequency[match(tinfo$Term, vocab)]
    rownames(term.topic.frequency) <- paste0("Topic", seq_len(K))
    colnames(term.topic.frequency) <- vocab
missing(cluster)
tinfo <- lapply(as.list(lambda.seq), find_relevance)
    tinfo <- rbind(default, tinfo)
    ut <- sort(unique(tinfo$Term))
names(tfinfo)
names(tinfo)
names(tinfo$Term)
tinfo$Term
    ut <- sort(unique(tinfo$Term))
    tinfo$Total <- term.frequency[match(tinfo$Term, vocab)]
    rownames(term.topic.frequency) <- paste0("Topic", seq_len(K))
    colnames(term.topic.frequency) <- vocab
    tinfo$Freq <- term.topic.frequency[as.matrix(tinfo[c("Category",         "Term")])]
    tinfo$Freq <- term.topic.frequency[as.matrix(tinfo[c("Category","Term")])]
term.topic.frequency
names(term.topic.frequency)
colnames(term.topic.frequency)
dim(term.topic.frequency)
colnames(term.topic.frequency)
rownames(term.topic.frequency)
as.matrix(tinfo[c("Category", "Term")])
as.matrix(tinfo[c("Category", "Term")])
term.topic.frequency[as.matrix(tinfo[c("Category", "Term")])]
term.topic.frequency[tinfo[c("Category", "Term")]]
term.topic.frequency[as.matrix(tinfo[c("Category", "Term")])]
str(phi.df)
str(theta_imputed)
rownames(theta_imputed)
colames(theta_imputed)
colnames(theta_imputed)
colnames(theta_imputed) <- seq(1:30)
str(theta_imputed)
str(length_mat)
typeof(length_mat)
str(list(length_mat))
bar <- (as.integer(lengthds$length.Preprocessed.dj_clean.))
str(bar)
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
str(freq_matrix$Freq)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.df)
dim(theta_imputed)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab.df,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
    tinfo <- rbind(default, tinfo)
Rs
length(Rs)
length(counts)
length(default.terms)
str(vocab.df)
str(vocab)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
dim(theta)
topic.frequency
serVis(json_lda)
topic.proportion
topic.proportion*100
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
library("AzureML") ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
phi_transpose_nums <- t(phids[,2:31])
#convert the phi you read in as a data frame
phi.df <- as.data.frame(phi_transpose_nums)
# the vocabulary is in the first row
vocab <- phids[,1]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
theta_imputed <- impute_df_to_sum_to_one(thetads)
library(LDAvis) json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
topic.frequency <- colSums(theta_imputed * doc.length)
topic.frequency <- colSums(theta_imputed * length_mat)
topic.proportion <- topic.frequency / sum(topic.frequency)
topic.proportion
o <- order(topic.proportion, decreasing = TRUE)
o
phi <- phi[o,]
phi <- phi.df[o,]
phi
theta <- theta_imputed[, o]
topic.frequency[o]
    topic.frequency <- topic.frequency[o]
    topic.proportion <- topic.proportion[o]
topic.frequency
topic.proportion*100
sum(topic.proportion)
colSums(theta_imputed)
dim(theta_imputed)
rowSums(theta_imputed)
colSums(theta_imputed)
colSums(theta_imputed*length_mat)
    term.topic.frequency <- phi * topic.frequency
    term.frequency <- colSums(term.topic.frequency)
term.frequency
dim(phi)
rowSums(phi)
rowSums(phi.df)
rowSums(t(phi.df))
t(phi)/rowSums(t(phi.df))
dim(t(phi)/rowSums(t(phi.df)))
topic.given.term <- (t(phi) / rowSums(t(phi.df)))
    kernel <- topic.given.term * log(sweep(topic.given.term,         MARGIN = 2, topic.proportion, `/`))
    distinctiveness <- rowSums(kernel)
    saliency <- term.proportion * distinctiveness
    term.proportion <- term.frequency / sum(term.frequency)
    distinctiveness <- rowSums(kernel)
    saliency <- term.proportion * distinctiveness
    default.terms <- vocab[order(saliency, decreasing = TRUE)][1:R]
default.terms <- vocab[order(saliency, decreasing = TRUE)][1:30]
dim(default.terms)
head(default.terms)
    counts <- as.integer(term.frequency[match(default.terms,         vocab)])
counts
dim(vocab.df)
colnames(vocab.df)
colnames(vocab.df)[1:10]
vocab[1:20]
vocab.df <- as.data.frame(vocabds)
vocab.df <- vocab.df[, -c(1)]
colnames(vocab.df)[1:10]
vocab.df[1:10,1]
vocab.df[1:10,2]
colSums(vocab.df)
colSums(vocab.df)[1:10]
vocab[1:10]
vocab=="uzu"
vocab[9681]
colSums(vocab.df)[9681]
head(freq_matrix)
freq_matrix[9681]
freq_matrix[9681,]
max(freq_matrix,2)
max(freq_matrix,3)
max(freq_matrix$Freq)
freq_matrix$Freq==20442
freq_matrix[19450,]
freq_matrix[19451,]
freq_matrix[19449,]
freq_matrix[19150,]
dim(phi)
dim(phi*topic.frequency)
dim(topic.frequency)
length(topic.frequency)
topic.frequency
dim(phi)
dim(term.topic.frequency)
term.topic.frequency[1:10,1:10]
term.topic.frequency[1:10,9680:9682]
term.topic.frequency[1:10,9680:9690]
phi[1:10,9680:9690]
dim(phids)
phids[9681,]
sum(phids[9681,])
sum(phids[9681,2:31])
sum(phids[9682,2:31])
sum(phids[9683,2:31])
phids[9681,]
dim(phi.df)
phi.df[,9681]
sum(phi.df[,9681])
sum(phi.df[1,])
sum(phi.df[,9681])
ws <- workspace(   id = "e12f59ece68c40559b1b83e53ba52030",   auth = "sNb9YyIk9NBphMIF8gYbXqP+Ts0SKTRxKFZ3Cai5N1l01N7Mqp71qsDxBmmdwXq1rFh8G5kvG/y4nyZbox228w==",   api_endpoint = "https://europewest.studioapi.azureml.net" ) phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
names(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
phi_transpose_nums <- t(phids[,2:11])
phi.df <- as.data.frame(phi_transpose_nums)
vocab <- phids[,1]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
thetads <- t(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
dim(thetads)
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
dim(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, 30] <- df[r, 30] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
theta_imputed <- impute_df_to_sum_to_one(t(thetads))
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
dim(phi.df)
dim(theta_imputed)
R
dim(thetads)
dim(thetads)[1]
theta_imputed <- impute_df_to_sum_to_one(thetads)
dim(thetads)[2]
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, 30] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, K] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) }
theta_imputed <- impute_df_to_sum_to_one(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads) vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds) lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
# Transpose to get the right format for the 
phi_transpose_nums <- t(phids[,2:11])
#convert the phi you read in as a data frame
phi.df <- as.data.frame(phi_transpose_nums)
# the vocabulary is in the first row
vocab <- phids[,1]
# give the dataframe this vocabulary as names
colnames(phi.df) <- vocab
# get the first row out of the data frame
#phi.done.df <- phi.df[-1,]
# Convert all columns from factors to numeric
#phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x)))
# make the theta dataset out of he transpose of the vocabulary and the 500 last rows (minus the feature ID).  NB the topic number is hard coded in her
#thetads <- t(ds[, 2:31])
#thetads <- t(thetads)
# Convert the vocabds into a data.frame
vocab.df <- as.data.frame(vocabds)
# remove the first column
vocab.df <- vocab.df[, -c(1)]
# realign column names with the above
colnames(vocab.df) <- vocab
# compute the total frequencies for each term in the corpus
freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df))
# compute document lengths
length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, K] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return (df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
dim(thetads)
thetads[1:100,1]
thetads[1:100,]
thetads[1:40,]
thetads[1:20,]
thetads[1:10,]
thetads[,1]
phids <- download.intermediate.dataset(   ws = ws,   node_id = "d5120482-e51d-4ca6-a241-af3fdc2f1185-902",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" )
thetads <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-92",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(thetads)
vocabds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-109414",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericTSV" ) names(vocabds)
lengthds <- download.intermediate.dataset(   ws = ws,   node_id = "adf8c0f7-96bf-4a8d-aa1c-635ebf23325b-200803",   experiment = "e12f59ece68c40559b1b83e53ba52030.f-id.4027c176d1ec46c3a093a009115c817d",   port_name = "Results dataset",   data_type_id = "GenericCSV" ) names(lengthds)
# Transpose to get the right format for the  phi_transpose_nums <- t(phids[,2:11]) #convert the phi you read in as a data frame phi.df <- as.data.frame(phi_transpose_nums) # the vocabulary is in the first row vocab <- phids[,1] # give the dataframe this vocabulary as names colnames(phi.df) <- vocab # get the first row out of the data frame #phi.done.df <- phi.df[-1,] # Convert all columns from factors to numeric #phi.done.df[] <- lapply(phi.done.df, function(x) as.numeric(as.character(x))) # make the theta dataset out of he transpose of the vocabulary and the 500 last rows (minus the feature ID).  NB the topic number is hard coded in her #thetads <- t(ds[, 2:31]) #thetads <- t(thetads) # Convert the vocabds into a data.frame vocab.df <- as.data.frame(vocabds) # remove the first column vocab.df <- vocab.df[, -c(1)] # realign column names with the above colnames(vocab.df) <- vocab # compute the total frequencies for each term in the corpus freq_matrix <- data.frame(ST = vocab,                            Freq = colSums(vocab.df)) # compute document lengths length_mat <- as.integer(lengthds$length.Preprocessed.dj_clean.)
impute_df_to_sum_to_one <- function(df) {     dt <- dim(df)     R <- dt[1]     K <- dt[2]     df_out <- df     for (r in 1:R) {         #print(r)         df_out[r, K] <- df[r, K] + (1 - sum(df[r,]))         temp <- all.equal(rowSums(df_out)[r], 1, check.attributes = F) == T         #print(temp)     }     return(df_out) } theta_imputed <- impute_df_to_sum_to_one(thetads)
json_lda <- LDAvis::createJSON(phi = phi.df, theta = theta_imputed,                                     vocab = vocab,                                     doc.length = length_mat,                                     term.frequency = freq_matrix$Freq)
serVis(json_lda)
head(thetads)
colSums(thetads)
rowSums(thetads)
rowSums(theta_imputed)
head(thetads)
head(phi.df)
vocab == "communication"
phi.df[612,]
phi.df[,612]
names(phi.df)[612]
names(phi.df)[613]
names(phi.df)[611]
phi.df[,611]
sum(phi.df[,611])
sum(phi.df[1,])
sum(phi.df[,611])
sum(phi.df[,610])
phi = phi.df
theta = thetads
doc.length = length_mat
term.frequency = freq_matrix$Freq
    dp <- dim(phi)
    dt <- dim(theta)
    N <- sum(doc.length)
    W <- length(vocab)
    D <- length(doc.length)
    K <- dt[2]
    if (dp[1] != K)
        stop("Number of rows of phi does not match \n      number of columns of theta; both should be equal to the number of topics \n      in the model.")
dim(thetads)
dim(phi.df)
theta = theta_imputed
sum(phi.df[,611])
    dp <- dim(phi)
    dt <- dim(theta)
    N <- sum(doc.length)
    W <- length(vocab)
    D <- length(doc.length)
    K <- dt[2]
    if (dp[1] != K)
        stop("Number of rows of phi does not match \n      number of columns of theta; both should be equal to the number of topics \n      in the model.")
    if (D != dt[1])
        stop("Length of doc.length not equal \n      to the number of rows in theta; both should be equal to the number of \n      documents in the data.")
    if (dp[2] != W)
        stop("Number of terms in vocabulary does \n      not match the number of columns of phi (where each row of phi is a\n      probability distribution of terms for a given topic).")
    if (length(term.frequency) != W)
        stop("Length of term.frequency \n      not equal to the number of terms in the vocabulary.")
    if (any(nchar(vocab) == 0))
        stop("One or more terms in the vocabulary\n      has zero characters -- all terms must have at least one character.")
    phi.test <- all.equal(rowSums(phi), rep(1, K), check.attributes = FALSE)
    theta.test <- all.equal(rowSums(theta), rep(1, dt[1]), check.attributes = FALSE)
    if (!isTRUE(phi.test))
        stop("Rows of phi don't all sum to 1.")
    if (!isTRUE(theta.test))
        stop("Rows of theta don't all sum to 1.")
    topic.frequency <- colSums(theta * doc.length)
    topic.proportion <- topic.frequency / sum(topic.frequency)
doc.length
doc.length[1:10]
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
library(readr)
mydata <- read_csv(filename)
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus.clean, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.52380]
summary(slam::col_sums(djreduced.dtm))
# find non-zero rows
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
djcorp.new <- dj.corpus[rowTotals > 0,]
b <- VectorSource(mydata$`Preprocessed dj_clean`)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus.clean, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
b <- VectorSource(mydata$`Preprocessed dj_clean`)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
require(tm)
library(topicmodels)
library(readr)
b <- VectorSource(mydata$`Preprocessed dj_clean`)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE))
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= 0.050350]
summary(slam::col_sums(djreduced.dtm))
# find non-zero rows
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
djcorp.new <- dj.corpus[rowTotals > 0,]
djcorp.new
dtm.new
system.time(dj.model <- topicmodels::LDA(dtm.new, 27, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
dj.topics <- topicmodels::topics(llis.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
# label topics
topicTerms <- tidyr::gather(dj.terms, Topic)
topicTerms <- cbind(topicTerms, Rank = rep(1:30))
topTerms <- dplyr::filter(topicTerms, Rank < 4)
topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))
topTerms$Topic <- as.numeric(topTerms$Topic)
topicLabel <- data.frame()
for (i in 1:27) {     z <- dplyr::filter(topTerms, Topic == i)     l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "), stringsAsFactors = FALSE)     topicLabel <- rbind(topicLabel, l) }
colnames(topicLabel) <- c("Label")
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(djreduced.dtm)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) }
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, djreduced.dtm)
dj.model
str(dj.model)
  djreduced.dtm
dj.corpus
phi <- posterior(dj.model)$terms %>% as.matrix
theta <- posterior(dj.model)$topics %>% as.matrix
dim(phi)
dim(theta)
vocab <- colnames(phi)
length(vocab)
doc_length <- vector()
for (i in 1:length(dj.corpus)) {     temp <- paste(dj.corpus[[i]]$content, collapse = ' ')     doc_length <- c(doc_length, stri_count(temp, regex = '\\S+')) }
doc_length
bar <- slam::col_sums(djreduced.dtm)
freq_matrix <- data.frame(ST = names(bar), Freq = bar)
#freq_matrix <- as.data.frame(slam::col_sums(djreduced.dtm))
dim(freq_matrix)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)
length(doc.length)
length(doc_length)
dim(theta)
dj.corpus
theta <- posterior(dj.model)$topics %>% as.matrix
dim(theta)
  djreduced.dtm
dim(phi)
str(dj.model)
dj.corpus
dj.json <- topicmodels_json_ldavis(dj.model, dj.corpus, dtm.new)
dtm.new
dj.json <- topicmodels_json_ldavis(dj.model, dj.corp.new, dtm.new)
dj.json <- topicmodels_json_ldavis(dj.model, djcorp.new, dtm.new)
djcorp.new <- dj.corpus[rowTotals > 0,]
rowTotals
str(rowTotals)
length(rowTotals)
djcorp.new <- dj.corpus[rowTotals > 0,]
dj.corpus[1,1]
dj.corpus[1]
djcorp.new <- dj.corpus[rowTotals > 0]
djcorp.new
dj.json <- topicmodels_json_ldavis(dj.model, djcorp.new, dtm.new)
serVis(dj.json)
sessionInfo()
library(readr)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
library(readr)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
dim(mydata)
ss <- myddata[1:1000]''
ss <- mydata[1:1000]
ss <- mydata[1:1000,]
dim(ss)
names(ss)
colnames(ss) <- c("text")
b <- VectorSource(ss$text)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
library(tm)
b <- VectorSource(ss$text)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
          readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(docs)
dtm <- DocumentTermMatrix(dj.corpus)
# Remove sparse entries
dtm.sparse <- removeSparseTerms(dtm, 0.93)
dtm.sparse
library(lsa)
install.packages("lsa")
library(lsa)
# create some files 
td = tempfile()
dir.create(td)
write(c("dog", "cat", "mouse"), file = paste(td, "D1", sep = "/"))
write(c("hamster", "mouse", "sushi"), file = paste(td, "D2", sep = "/"))
write(c("dog", "monster", "monster"), file = paste(td, "D3", sep = "/"))
write(c("dog", "mouse", "dog"), file = paste(td, "D4", sep = "/"))
# read files into a document-term matrix 
myMatrix = textmatrix(td, minWordLength=1)
myMatrx
myMatrix
round(as.textmatrix(myLSAspace),2) # should give the original
myLSAspace = lsa(myMatrix, dims=dimcalc_raw())
round(as.textmatrix(myLSAspace),2) # should give the original
myLSAspace = lsa(myMatrix, dims=dimcalc_share())
myNewMatrix = as.textmatrix(myLSAspace)
myNewMatrix # should look be different!
# compare two terms with the cosine measure 
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
# compare two documents with pearson 
cor(myNewMatrix[,1], myNewMatrix[,2], method="pearson")
# clean up 
install.packages("svd")
library(svd)
myMatrix
svd(myMatrix)
foo <- svd(myMatrix)
str(foo)
foo$d*foo$u*foo$v
foo$u*foo$v
foo$u%*%foo$v
foo$d%*%foo$u%*%foo$v
foo$d%*%foo$u
foo$u%*%foo$v
dim(foo$u)
dim(foo$d)
dim(foo$v)
foo$d
diag$d
diag
diag(foo$d,1,4)
foo$d
?diag
diag(123)
diag(4)
diag(4)*foo$d
diag_d<-diag(4)*foo$d
foo$d%*%foo$u%*%foo$v
foo$u%*%foo$d%*%foo$v
foo$u %*% diag_d
foo$u %*% diag_d %*5 foo$v
foo$u %*% diag_d %*% foo$v
myMatrix
foo$d
foo$u %*% diag_d %*% foo$v
myMatrix
foo$v %*% diag_d %*% foo$u
foo$u %*% diag_d %*% foo$v
foo$u %*% diag_d
(foo$u %*% diag_d)%*% foo$v
myMatrix
(foo$u %*% diag_d)%*% t(foo$v)
(foo$u %*% diag_d)%*% t(foo$v)
myMatrix
round(foo$u %*% diag_d)%*% t(foo$v)
round((foo$u %*% diag_d)%*% t(foo$v))
myMatrix
dim(foo$u)
dim(foo$v)
d
foo$d
myMatrix
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
cosine(myMatrix["dog",], myMatrix["cat",])
foo$d
cosine(myMatrix["dog",], myMatrix["cat",])
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
cosine(myNewMatrix["dog",], myNewMatrix["sushi",])
cosine(myMatrix["dog",], myMatrix["sushi",])
library(svd)
mySVD <- svd(myMatrix)
mySVD$u %*% mySVD$d %*% mySVD$v
mySVD$u %*% mySVD$d %*% t(mySVD$v)
reconstructed <- mySVD$u %*% mySVD$d %*% t(mySVD$v)
reconstructed
mySVD$u
mySVD$d
diag(mySVD$d)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
reconstructed
round(reconstructed)
mySVD$u
mySVD$u[,1:2]
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% mySVD$v[1:2,1:2]
diag(mySVD$d[1:2])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[1:2,])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[1:2,])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[1:2,])
mySVD$u[, 1:2]
mySVD$v[1:2,]
mySVD$v[,1:2]
t(mySVD$v[,1:2])
mySVD$u[, 1:2] %*% diag(mySVD$d[1:2]) %*% t(mySVD$v[,1:2])
myNewMatrix
mySVD$u[, 1:2]
mySVD$v[, 1:2]
mySVD$v[,1:2]
t(mySVD$v[,1:2])
mySVD$v[,1:2]
library(graphics)
matplot((-4:5) ^ 2, main = "Quadratic") # almost identical to plot(*)
x <- 0:50 / 50 matplot(x, outer(x, 1:8, function(x, k) sin(k * pi * x)),         ylim = c(-2, 2), type = "plobcsSh",         main = "matplot(,type = \"plobcsSh\" )")
matplot(x, outer(x, 1:4, function(x, k) sin(k * pi * x)),         pch = letters[1:4], type = c("b", "p", "o"))
mySVD$v[,1:2]
x <- matrix(rnorm(1024), nrow = 32)
# simulate a correlation matrix with values -0.5 to 0.5 x <- rescale(x, c(-0.5, 0.5))
# add a column with the extreme values (-1,1) to calculate 
# the colors, then drop the extra column in the result 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
library(plotrix)
x <- matrix(rnorm(1024), nrow = 32)
# simulate a correlation matrix with values -0.5 to 0.5 x <- rescale(x, c(-0.5, 0.5))
# add a column with the extreme values (-1,1) to calculate 
# the colors, then drop the extra column in the result 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
# do the legend call separately to get the full range 
color.legend(0, -4, 10, -3, legend = c(-1, -0.5, 0, 0.5, 1), rect.col = color.scale(c(-1, -0.5, 0, 0.5, 1), c(0, 1), 0, c(1, 0)), align = "rb")
x <- matrix(rnorm(100), nrow = 10)
# generate colors that show negative values in red to brown 
# and positive in blue-green to green 
cellcol <- matrix(rep("#000000", 100), nrow = 10)
cellcol[x < 0] <- color.scale(x[x < 0], c(1, 0.8), c(0, 0.8), 0)
cellcol[x > 0] <- color.scale(x[x > 0], 0, c(0.8, 1), c(0.8, 0))
# now do hexagons without borders 
color2D.matplot(x, cellcolors = cellcol, xlab = "Columns", ylab = "Rows", do.hex = TRUE, main = "2D matrix plot (hexagons)", border = NA)
# for this one, we have to do the color legend separately 
# because of the two part color scaling 
legval <- seq(min(x), max(x), length.out = 6)
legcol <- rep("#000000", 6)
legcol[legval < 0] <- color.scale(legval[legval < 0], c(1, 0.8), c(0, 0.8), 0)
legcol[legval > 0] <- color.scale(legval[legval > 0], 0, c(0.8, 1), c(0.8, 0) color.legend(0, -1.8, 3, -1.4, round(c(min(x), 0, max(x)), 1), rect.col = legcol)
legcol[legval < 0] <- color.scale(legval[legval < 0], c(1, 0.8), c(0, 0.8), 0)
legcol[legval > 0] <- color.scale(legval[legval > 0], 0, c(0.8, 1), c(0.8, 0) )
color.legend(0, -1.8, 3, -1.4, round(c(min(x), 0, max(x)), 1), rect.col = legcol)
# do a color only association plot 
xt <- table(sample(1:10, 100, TRUE), sample(1:10, 100, TRUE))
observed <- xt[, rev(1:dim(xt)[2])]
expected <- outer(rowSums(observed), colSums(observed), "*") / sum(xt)
deviates <- (observed - expected) / sqrt(expected)
cellcol <- matrix(rep("#000000", 100), nrow = 10)
cellcol[deviates < 0] < color.scale(deviates[deviates < 0], c(1, 0.8), c(0, 0.5), 0)
cellcol[deviates > 0] < color.scale(deviates[deviates > 0], 0, c(0.7, 0.8), c(0.5, 0))
color2D.matplot(x = round(deviates, 2), cellcolors = cellcol, show.values = TRUE, main = "Association plot")
# Hinton diagram 
border.col <- color.scale(x, extremes = 2:3)
color2D.matplot(x, extremes = c(2, 3), main = "Hinton diagram (green +, red -)", Hinton = TRUE, border = border.col)
# waffle plot of percentages with two contributing elements 
waffle.col <- fill.corner(c(rep("red", 18), rep("blue", 45)), 10, 10)
color2D.matplot(matrix(1:100,nrow=10),cellcolors=waffle.col,yrev=FALSE, border="lightgray",xlab="",ylab="",main="Waffle plot",axes=FALSE)
color2D.matplot(x, cellcolors = cellcol, xlab = "Columns", ylab = "Rows", do.hex = TRUE, main = "2D matrix plot (hexagons)", border = NA)
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
x <- matrix(rnorm(1024), nrow = 32)
# simulate a correlation matrix with values -0.5 to 0.5 x <- rescale(x, c(-0.5, 0.5))
# add a column with the extreme values (-1,1) to calculate 
# the colors, then drop the extra column in the result 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(x, cellcolors = cellcol, main = "Blue to red correlations")
mySVD$v
color2D.matplot(mySVD$v)
color2D.matplot(mySVD$v, cellcolors = cellcol)
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol)
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[,1:2]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[,1:2]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(mySVD$v[,1:2], cellcolors = cellcol, main = "Document relatedness")color2D.matplot(mySVD$v[, 1:3], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(mySVD$v[, 1:3], cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:3]), cellcolors = cellcol, main = "Document relatedness")
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
mySVD$u[, 1:dimensions]
wm <- mySVD$u[, 1:dimensions]
plot(wm[,1], wm[,2])
plot(wm[,1], wm[,2], col = wm[,3])
plot(wm[,1], wm[,2])
wm[,3]
plot(wm[,1], wm[,2], col = wm[,3], pchi="x")
plot(wm[,1], wm[,2], col = wm[,3])
plot(wm[,1], wm[,2], col = wm[,3])
?plot
plot(wm[,1], wm[,2], col = wm[,3], type = p)
plot(wm[,1], wm[,2], col = wm[,3], type = "p")
plot(wm[,1], wm[,2],  type = "p")
library(ggplot2)
qplot(data = wm, x = wm[,1], y=wm[,2])
wmdf <- as.data.frame(wm)
names(wmdf)
qplot(data = wmdf, x = V1, y=V2)
qplot(data = wmdf, x = V1, y=V2, color = V3)
qplot(data = wmdf, x = V1, y = V2, color = V3)
ggplot(data = wmdf, x = V1, y = V2, color = V3)
ggplot
plotwords <- ggplot(data = wmdf, x = V1, y = V2, color = V3)
plotwords
rainbow <- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)
ggplot(mtcars, aes(mpg, wt)) + geom_point() + annotation_raster(rainbow, 15, 20, 3, 4)
plotwords <- ggplot(data = wmdf, x = V1, y = V2, color = V3)
geom_point()
plotwords + geom_point()
plotwords <- ggplot(data = wmdf, x = V1, y = V2, color = V3)
plotwords + geom_point()
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point()
rainbow <- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)
ggplot(mtcars, aes(mpg, wt)) + geom_point() + annotation_raster(rainbow, 15, 20, 3, 4)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point()
myMatrix
colnames(myMatrix)
rownames(myMatrix)
words <- rownames(myMatrix)
plotwords + geom_point() + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point() + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point() + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords +  geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_point()
geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
wmdf$V3
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V2)
plotwords <- plotwords  + geom_point()
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V2)
plotwords <- plotwords  + geom_point()
color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V2)
plotwords <- plotwords  + geom_point()
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
wmdf
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp
bp <- ggplot(ToothGrowth, aes(x = dose, y = len, fill = dose)) +   geom_boxplot()
bp
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp
sp + scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9"))
sp + scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9"))
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9"))
bp
td = tempfile()
dir.create(td)
write(c("dog", "cat", "mouse"), file = paste(td, "D1", sep = "/"))
write(c("hamster", "mouse", "sushi"), file = paste(td, "D2", sep = "/"))
write(c("dog", "monster", "monster"), file = paste(td, "D3", sep = "/"))
write(c("dog", "mouse", "dog"), file = paste(td, "D4", sep = "/"))
# read files into a document-term matrix 
myMatrix = textmatrix(td, minWordLength=1)
library(lsa)
myMatrix = textmatrix(td, minWordLength=1)
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed)
# how many dimensions?
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
library(ggplot2)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords <- plotwords  + geom_point()
plotwords
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_brewer(palette = "Dark2")
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
words <- rownames(wmdf)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
words
words <- rownames(myMatrx)
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2), color = V3)
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
rainbow <- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)
ggplot(mtcars, aes(mpg, wt)) + geom_point() + annotation_raster(rainbow, 15, 20, 3, 4)
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_brewer(palette = "Dark2")
typeof(wmdf)
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_continuous(values = c("#999999", "#E69F00", "#56B4E9"))
sp <- ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point()
sp + scale_color_brewer(palette = "Dark2")
sp + scale_color_continuous(palette = "Dark2")
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
t(mySVD$v[, 1:dimensions])
diag(mySVD$d[, 1:dimensions])
mySVD$d[1:dimensions]
mySVD$v[, 1:dimensions]
t(mySVD$v[, 1:dimensions])
max(t(mySVD$v[, 1:dimensions]))
max(t(mySVD$v[, 1:dimensions])[1,])
max(t(mySVD$v[, 1:dimensions])[2,])
max(t(mySVD$v[, 1:dimensions])[3,])
max(t(mySVD$v[, 1:dimensions])[2,])
max(t(mySVD$v[, 1:dimensions])[3,])
max(t(mySVD$v[, 1:dimensions])[3,])
argmax(t(mySVD$v[, 1:dimensions])[1,])
max(t(mySVD$v[, 1:dimensions])[3,])
t(mySVD$v[, 1:dimensions])[3,]
myMatrix
t(mySVD$v[, 1:dimensions])[3,]
t(mySVD$v[, 1:dimensions])[1,]
max(t(mySVD$v[, 1:dimensions])[1,])
max(t(mySVD$v[, 1:dimensions])[2,])
t(mySVD$v[, 1:dimensions])[2,]
t(mySVD$v[, 1:dimensions])[3,]
dtm
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
library(readr)
library(tm)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
mydata <- read_csv(filename)
exdoc <- mydata[1]
exdoc
exdoc <- mydata[1,]
exdoc
names(mydata)
colnames(mydata)
colnames(mydata) <- c("text")
exdoc <- mydata[1,]$text
exdoc
myMatrix
td
textmatrix(exdoc <- "It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.  However, it was sold with falsehoods and is now being mismanaged.  To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market; and Nigel Farage predicted that other countries would follow Britain out of Europe.  It hasn ’t quite turned out like that.  More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when.  The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis.  Like both previous disasters, Brexit reveals three enduring flaws in the UK ’s workings.")
exdoc
textmatrix(exdoc)
c <- VectorSource(exdoc)
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(ft.corpus)
dtm
c("foo", "bar")
c <- VectorSource(c("foo", "bar"))
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
ft.corpus
dtm <- DocumentTermMatrix(ft.corpus)
dtm
exdoc <- c("It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.", "However, it was sold with falsehoods and is now being mismanaged. ", "To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market;", "and Nigel Farage predicted that other countries would follow Britain out of Europe. ", "It hasn ’t quite turned out like that. ", "More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when. ", "The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis. ", "Like both previous disasters, Brexit reveals three enduring flaws in the UK ’s workings.")dtm
exdoc <- c("It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.", "However, it was sold with falsehoods and is now being mismanaged. ", "To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market;", "and Nigel Farage predicted that other countries would follow Britain out of Europe. ", "It hasn ’t quite turned out like that. ", "More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when. ", "The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis. ", "Like both previous disasters, Brexit reveals three enduring flaws in the UK s workings.")
c <- VectorSource(exdoc)
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(ft.corpus)
dtm
as.matrix(dtm)
ftmat <- as.matrix(dtm)
lsa(ftmat, dims = dimcalc_raw())
myMatrix <- ftmat
# replicating the lsa with the svd library
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed)
round(reconstructed) == myMatrix
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
words
myMatrix
rownames(myMatrix)
colnames(myMatrix)
tdm <- TermDocumentMatrix(ft.corpus)
ftmat <- as.matrix(tdm)
myMatrix <- ftmat
# replicating the lsa with the svd library
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed) == myMatrix
# how many dimensions?
dimensions <- 3
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
max(t(mySVD$v[, 1:dimensions])[2,])
max(t(mySVD$v[, 1:dimensions])[1,])
which.max(t(mySVD$v[, 1:dimensions])[1,])
which.max(t(mySVD$v[, 1:dimensions])[1,])
exdoc[which.max(t(mySVD$v[, 1:dimensions])[1,])]
# extract the most prominent sentences
for (i in 1:dimensions) {     exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])] }
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine(myNewMatrix["dog",], myNewMatrix["cat",])
tdm
ftmat
ftmat[row == "what"]
dim(ftmat)
rownames(ftmat)
rownames(ftmat) == "David"
which(rownames(ftmat) == "david")
which(rownames(ftmat) == "state")
which(rownames(ftmat) == "daniel")
cosine(mySVD[18,], mySVD[19,])
cosine(mySVD$u[18,], mySVD$u[19,])
cosine(mySVD$u[18,], mySVD$u[19,])
dim(mySVD$u)
mySVD$u[18,]
mySVD$u[19,]
mySVD$u[20,]
mySVD$u[27,]
which(rownames(ftmat) == "hannan")
cosine(mySVD$u[18,], mySVD$u[32,])
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
dimensions <- 1
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
# extract the most prominent sentences
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine()
dimensions <- 8
# extract the most prominent sentences
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine()
dimensions <- 4
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(t(mySVD$v[, 1:dimensions]), main = "Document relatedness")
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cbind(x, c(-1, rep(1, 31)))
cbind(t(mySVD$v[, 1:dimensions]), c(-1, rep(1, 31)))
c(-1, rep(1, 31))
c(-1, rep(1, 31))
c(1, rep(1, 31))
c(-1, rep(1, 31))
cbind(t(mySVD$v[, 1:dimensions]), c(-1, rep(1, 31)))
x <- matrix(rnorm(1024), nrow = 32)
x <- rescale(x, c(-0.5, 0.5))
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
dim(cellcol)
concepts <- t(mySVD$v[, 1:dimensions])
dim(concepts)
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
cbind(x, c(-1, rep(1, 31)))
dim(cbind(x, c(-1, rep(1, 31))))
dim(x)
cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))
dim(cellcol)
cellcol <- color.scale(cbind(concepts, c(-1, rep(1, 7))), c(0, 1), 0, c(1, 0))
cbind(concepts, c(-1, rep(1, 7)) )
cbind(concepts, c(-1, rep(1, 7)))
dim(concepts)
cbind(concepts, c(-1, rep(1, 8)))
cbind(concepts, c(-1, rep(1, 7)))
cbind(concepts, c(-1, rep(1, 6)))
c(-1, rep(1, 6)))
c(-1, rep(1, 6))
c(-1, rep(1, 7))
dim(c(-1, rep(1, 7)))
length(c(-1, rep(1, 7)))
?cbind
typeof(x)
f(concepts)
library(readr)
library(tm)
setwd("C:\\Users\\bungum\\Documents")
filename <- "preprocessed50000.csv"
exdoc <- c("It ’s possible that Brexit — treated strictly as an intellectual concept — is a brilliant idea.", "However, it was sold with falsehoods and is now being mismanaged. ", "To cite just a few Brexiter politicians:David Davis sketched a deal with the EU as simply a matter of a quick visit to Berlin; Daniel Hannan said that obviously the UK wouldn ’t leave the European single market;", "and Nigel Farage predicted that other countries would follow Britain out of Europe. ", "It hasn ’t quite turned out like that. ", "More than a year after the referendum, the cabinet still can ’t agree on what kind of Brexit it wants, or when. ", "The British state is steaming towards its third disaster in 15 years, after the Iraq war and the financial crisis. ", "Like both previous disasters, Brexit reveals three enduring flaws in the UK s workings.")
c <- VectorSource(exdoc)
ft.corpus <- Corpus(c, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
b <- VectorSource(ss$text)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
dtm <- DocumentTermMatrix(ft.corpus)
tdm <- TermDocumentMatrix(ft.corpus)
ftmat <- as.matrix(tdm)
myMatrix <- ftmat
# replicating the lsa with the svd library
library(svd)
mySVD <- svd(myMatrix)
reconstructed <- mySVD$u %*% diag(mySVD$d) %*% t(mySVD$v)
round(reconstructed) == myMatrix
# how many dimensions?
dimensions <- 4
mySVD$u[, 1:dimensions] %*% diag(mySVD$d[1:dimensions]) %*% t(mySVD$v[,1:dimensions])
#plotting the decomposed matrix as colors
library(plotrix)
#cellcol <- color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
#color2D.matplot(t(mySVD$v[, 1:dimensions]), cellcolors = cellcol, main = "Document relatedness")
concepts <- t(mySVD$v[, 1:dimensions])
color2D.matplot(concepts), main = "Document relatedness")
library(ggplot2)
wmdf <- as.data.frame(mySVD$u[, 1:dimensions])
words <- rownames(myMatrix)
plotwords <- ggplot(data = wmdf, aes(V1, V2, color = V3))
plotwords <- plotwords  + geom_point()
plotwords
plotwords + geom_text(aes(V1, V2, label = words, group = NULL), data = wmdf) + geom_point() 
# extract the most prominent sentences
for (i in 1:dimensions) {     print(exdoc[which.max(t(mySVD$v[, 1:dimensions])[i,])]) }
cosine()
color.scale(cbind(x, c(-1, rep(1, 31))), c(0, 1), 0, c(1, 0))[, 1:32]
cbind(concepts, c(-1, rep(1, 31)))
cbind(concepts, c(-1, rep(1, 7)))
cbind(concepts, c(-1, rep(1, 4)))
cbind(concepts, c(-1, rep(1, 3)))
cellcol <- color.scale(cbind(x, c(-1, rep(1, d[1]-1))), c(0, 1), 0, c(1, 0))[, 1:d[1]]
cellcol <- color.scale(cbind(concepts, c(-1, rep(1, d[1]-1))), c(0, 1), 0, c(1, 0))[, 1:d[1]]
d <- dim(concepts)
cellcol <- color.scale(cbind(concepts, c(-1, rep(1, d[1]-1))), c(0, 1), 0, c(1, 0))[, 1:d[1]]
color2D.matplot(concepts), cellcolors = cellcol, main = "Document relatedness")
color2D.matplot(concepts, cellcolors = cellcol, main = "Document relatedness")
setwd("C:\\Users\\bungum\\Documents")
filename <- "all3.csv"
foo <- read.csv(filename)
setwd("C:\\Users\\bungum\\Documents")
filename <- "all3.csv"
system.time(mydf <- read_csv(filename, stringsAsFactors = FALSE))
system.time(mydf <- read.csv(filename, stringsAsFactors = FALSE))
str(foo)
str(mydf)
str(mydf)
dim(mydf)
ss <- mydf[1:100,]
save(ss, "foo.RData")
save(ss, file = c("foo.RData"))
getwd()
800/5
160*2
320/60
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
names(mydf)
names(mydf)[20:1202]
system.time(full_mldr <- mldr_from_dataframe(mydf, labelIndices = 20:1202))
library(mldr)
system.time(full_mldr <- mldr_from_dataframe(mydf, labelIndices = 20:1202))
install_github('ramnathv/slidify')
install.packages("devtools")
library("devtools")
install_github('ramnathv/slidify')
install_github('ramnathv/slidifyLibraries')
author("mydeck")
library(slidify)
author("mydeck")
slidify('index.Rmd')
--- title: "my presentation" subtitle: author: LB job: framework:io2012 # {html5slides}   highlighter:highlight.js # {highlight.jst}   hitheme:tomorrow #    widgets:[] # {mathjax, quiz, bootstrap}   mode:selfcontained # {standalone}   knit:slidify::knit2slides --- slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
names(mldr)
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
names(mydf)
mydf$replace
mydf[1,15]$replace
mydf[1,15]
strwrap(mydf[1,15])
nchar(mydf[1,15])
nchar(mydf[1:25,15])
strwrap(mydf[8,15])
mydf[8,15]$topics
mydf[8,15]$topic_cat
mydf[8,15]$topics
mydf[8,]$topics
mydf[8,]$topics_cat
mydf[8,]$topic_cat
names(mydf[8,])[1:20]
mydf[8,]$topic
mydf[8,]$money
strwrap(mydf[8,15])
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
library(slidify)
slidify('index.Rmd')
slidify('index.Rmd')
getwd()
setwd("c:\\users\\bungum\\documents\\mydeck")
slidify('index.Rmd')
strwrap(mydf[8,15])
slidify('index.Rmd')
slidify('index.Rmd')
mydf[8,]$money
slidify('index.Rmd')
strwrap(mydf[8,15])
rm(mydf)
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
system.time(full_mldr <- mldr_from_dataframe(mydf[, 20:length(colnames(mydf))], labelIndices = 1:(length(colnames(mydf)) - 20)))
slidify('index.Rmd')
setwd("C:\\Users\\bungum\\Documents\\mydeck")
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
install.packages("evaluate")
slidify('index.Rmd')
library(evaluate)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
evaluate_call
help(evaluate)
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
sessionInfo()
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
slidify('index.Rmd')
packageurl <- "http://cran.r-project.org/src/contrib/Archive/knitr/knitr_1.12.tar.gz"
install.packages(packageurl, repos = NULL, type = "source")
install.packages(knitr)
install.packages("knitr")
slidify('index.Rmd')
slidify('index.Rmd')
package_df <- as.data.frame(installed.packages("/Library/Frameworks/R.framework/Versions/2.15/Resources/library"))
package_list <- as.character(package_df$Package)
system.time(install.packages(package_list))
package_list
package_list <- as.character(package_df$Package)
package_list
package_df <- as.data.frame(installed.packages())
package_list <- as.character(package_df$Package)
package_list
system.time(install.packages(package_list))
slidify('index.Rmd')
getwd()
author("mydeck2")
getwd()
slidify('index.Rmd')
slidify('index.Rmd')
require(tm)
b <- VectorSource(mydata$replace)
b <- VectorSource(mydf$replace)
dj.corpus <- Corpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
system.time(djtopic.dtm <- tm::DocumentTermMatrix(dj.corpus, control = list(stemming = TRUE,     stopwords = TRUE,     minWordLength = 2,     removeNumbers = TRUE,     removePunctuation = TRUE)))
10849/3600
djtopic.dtm
term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i], djtopic.dtm$j, mean) *   log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))
summary(term_tfidf)
summary(term_tfidf)[3]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal]
medianVal <- summary(term_tfidf)[3]
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
removeSparseTerms(djreduced.dtm, .99)
?frequency
findFreqTerms(djreduced.dtm, 100)
head(sort(rowSums(as.matrix(dj.reduced.dtm)), decreasing = T), 100)
head(sort(rowSums(as.matrix(djreduced.dtm)), decreasing = T), 100)
?findFreqTerms
head(sort(rowSums(as.matrix(djreduced.dtm)), decreasing = T), 10000)
findFreqTerms(djreduced.dtm, 10000)
findFreqTerms(djreduced.dtm, 20000)
?removeSparseTerms
removeSparseTerms(djreduced.dtm, .5)
removeSparseTerms(djreduced.dtm, .9)
removeSparseTerms(djreduced.dtm, .95)
removeSparseTerms(djreduced.dtm, .975)
removeSparseTerms(djreduced.dtm, .985)
removeSparseTerms(djreduced.dtm, .995)
djreduced.dtm
removeSparseTerms(djreduced.dtm, .999)
djtopic.dtm[, term_tfidf >= medianVal*2]
djtopic.dtm[, term_tfidf >= medianVal*2.5]
djtopic.dtm[, term_tfidf >= medianVal*3]
djtopic.dtm[, term_tfidf >= medianVal*4]
djtopic.dtm[, term_tfidf >= medianVal*5]
djtopic.dtm[, term_tfidf >= medianVal*6]
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*6]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
rowTotals <- apply(djreduced.dtm, 1, sum)
dtm.new <- djreduced.dtm[rowTotals > 0,]
rowSums(dj.reduced.dt)
rowSums(djreduced.dtm)
djreduced.dtm
?apply
dim(djreduced.dtm)
rowSums(djreduced.dtm)
rowSums(as.matrix(djreduced.dtm))
as.matrix(djreduced.dtm)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*7]
summary(slam::col_sums(djreduced.dtm))
djreduced.dtm
as.matrix(djreduced.dtm)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*8]
djreduced.dtm
as.matrix(djreduced.dtm)
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*10]
as.matrix(djreduced.dtm)
djreduced.dtm
djreduced.dtm <- djtopic.dtm[, term_tfidf >= medianVal*12]
djreduced.dtm
as.matrix(djreduced.dtm)
1372718 * 21912
1372718 * 21912*8
1024*1024
1024*1024*1024
1024*1024*1024*224
1372718 * 21912*8
library(text2vec)
install.packages("text2vec")
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$id,              progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab = create_vocabulary(it_train)
vocab
train_tokens = train$review %>%   prep_fun %>%   tok_fun
train_tokens
it_train = itoken(train_tokens,                   ids = train$id, # turn off progressbar because it won't look nice in rmd                   progressbar = T)
vocab = create_vocabulary(it_train)
vocab
it_train = itoken(train$review,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$id,              progressbar = FALSE) vocab1 = create_vocabulary(it_train)
vocab == voca|1
vocab == vocab1
vectorizer = vocab_vectorizer(vocab)
vectorizer
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dtm_train
dim(dtm_train)
library(glmnet) NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],                               family = 'binomial', # L1 penalty                               alpha = 1, # interested in the area under ROC curve                               type.measure = "auc", # 5-fold cross-validation                               nfolds = NFOLDS, # high value is less accurate, but has faster training                               thresh = 1e-3, # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
install.packages("glmnet")
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
library(glmnet)
NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
typeof(train)
train = mydf[1:100,]$replace
train = mydf[1:100,]
names(mydf)
names(mydf)[1:20]
it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab1 = create_vocabulary(it_train)
it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
str(dtm_train)
rownames(dtm_train)
colnames(dtm_train)
dtm_train[1,1]
dtm_train[1,]$turn
dtm_train[1,7912]
dtm_train[,7912]
library(glmnet) NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
train = mydf[1:10000,] # define preprocessing function and tokenization function # create vocabulary with a memory friendly way (lazy function?) prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab = create_vocabulary(it_train)
vocab
vectorizer = vocab_vectorizer(vocab) t1 = Sys.time() dtm_train = create_dtm(it_train, vectorizer) print(difftime(Sys.time(), t1, units = 'sec'))
library(glmnet) NFOLDS = 4 t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                               family = 'binomial',                                 # L1 penalty                               alpha = 1,                                 # interested in the area under ROC curve                               type.measure = "auc",                                 # 5-fold cross-validation                               nfolds = NFOLDS,                                 # high value is less accurate, but has faster training                               thresh = 1e-3,                                 # again lower number of iterations for faster training                               maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec')) plot(glmnet_classifier)
dim(dtm_train)
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))
t1 = Sys.time()
dtm_train = create_dtm(it_train, h_vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                              family = 'binomial',                              alpha = 1,                              type.measure = "auc",                              nfolds = 5,                              thresh = 1e-3,                              maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
train = mydf[1:100000,]
it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) vocab = create_vocabulary(it_train)
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L)) t1 = Sys.time() dtm_train = create_dtm(it_train, h_vectorizer) print(difftime(Sys.time(), t1, units = 'sec'))
t1 = Sys.time() glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['CYC']],                              family = 'binomial',                              alpha = 1,                              type.measure = "auc",                              nfolds = 5,                              thresh = 1e-3,                              maxit = 1e3) print(difftime(Sys.time(), t1, units = 'sec')) plot(glmnet_classifier)
train = mydf[1:100000,]
train = mydf[,]
t1 = Sys.time()
vocab = create_vocabulary(it_train)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(vocab)
length(vocab)
vocab
train = mydf
prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = FALSE) t1 = Sys.time() vocab = create_vocabulary(it_train) print(difftime(Sys.time(), t1, units = 'sec'))
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))
t1 = Sys.time()
dtm_train = create_dtm(it_train, h_vectorizer)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
library(mldr)
system.time(load("mydf_full_columns_fixed.RData"))
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
library(mldr)
install.packages("XML")
library(mldr)
install.packages("mldr")
library(mldr)
install.packages("colorspace")
library(mldr)
install.packages("digest")
library(mldr)
install.packages("htmltools")
library(mldr)
install.packages("httpuv")
library(mldr)
install.packages("mime")
library(mldr)
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
mylabels
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
names(mydf)
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
colnames(mydf) == "FCL"
sum(colnames(mydf) == "FCL")
system.time(load("fullset_industry_topic_country_in_columns.RData"))
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
colnames(mydf)
sort(colnames(mydf))
train = mydf # define preprocessing function and tokenization function # create vocabulary with a memory friendly way (lazy function?) prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = TRUE) t1 = Sys.time() vocab = create_vocabulary(it_train) print(difftime(Sys.time(), t1, units = 'sec'))
library(text2vec)
library(data.table)
train = mydf # define preprocessing function and tokenization function # create vocabulary with a memory friendly way (lazy function?) prep_fun = tolower tok_fun = word_tokenizer it_train = itoken(train$replace,              preprocessor = prep_fun,              tokenizer = tok_fun,              ids = train$newsid,              progressbar = TRUE) t1 = Sys.time() vocab = create_vocabulary(it_train) print(difftime(Sys.time(), t1, units = 'sec'))
# feature hashing  h_vectorizer = hash_vectorizer(hash_size = 2^14, ngram = c(1L, 2L)) t1 = Sys.time() dtm_train = create_dtm(it_train, h_vectorizer) print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_matrix)
dim(dtm_train)
system.time(save(dtm_train, "dtm_train_1372718x16384.RData"))
system.time(save("dtm_train_1372718x16384.RData", dtm_train))
system.time(save(dtm_train, file = "dtm_train_1372718x16384.RData"))
ss <- dtm_train[1:1000,]
dim(ss)
library(caret)
install.packages("stringi")
library(caret)
ctrl <- trainControl(method = "cv", savePred = T, classProb = T)
mod <- train(Species ~ ., data = ss[1:800], method = "svmLinear", trControl = ctrl)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
system.time(load("dtm_train_1372718x16384.RData"))
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
library(mldr)
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RWTS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
colnames(mydf)
colnames(mydf)=="CYC"
mylabels <- c("FCL", "HCR", "TEC", "ENE", "BSC", "CYC", "IDU", "RECN", "NCY", "RTWS")
mylabels_mldr <- mldr_from_dataframe(mydf[, mylabels], labelIndices = 1:10)
library(doParallel);
library(caret)
#create a list of seed, here change the seed for each resampling
set.seed(123)
#length is = (n_repeats*nresampling)+1
seeds <- vector(mode = "list", length = 11)
#(3 is the number of tuning parameter, mtry for rf, here equal to ncol(iris)-2)
for (i in 1:10)
    seeds[[i]] <- sample.int(n = 1000, 3)
#for the last model
seeds[[11]] <- sample.int(1000, 1)
#control list
myControl <- trainControl(method = 'cv', seeds = seeds, index = createFolds(iris$Species))
#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)
model1 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
model2 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
stopCluster(cl)
all.equal(predict(model1, type = 'prob'), predict(model2, type = 'prob'))
ssmdf <- mydf[1:1000,]
ss <- dtm_train[1:1000,]
ndf <- cbind(ssm$CYC, ss)
ndf <- cbind(ssmdf$CYC, ss)
colnames(ndf)[1:10]
colnames(ndf)
dim(ss)
dim(ssmdf)
dim(ssmdf$cyc)
dim(ssmdf$CYC)
length(ssmdf$CYC)
dim(ndf)
colnames(ndf)[1] <- "CYC"
colnames(ndf)[1]
ndf <- cbind(ssmdf$CYC, as.matrix(ss))
colnames(ndf)[1]
ndf <- data.frame(cbind(ssmdf$CYC, as.matrix(ss)))
colnames(ndf)[1]
colnames(ndf)[1] <- "CYC"
dim(ndf)
ndf$CYC
ifelse(ndf$CYC==T, "YES", "NO")
ndf$CYC <- ifelse(ndf$CYC == T, "YES", "NO")ifelse(ndf$CYC==T, "YES", "NO")
ndf$CYC <- ifelse(ndf$CYC == T, "YES", "NO")
library(doParallel);
library(caret)
#create a list of seed, here change the seed for each resampling
set.seed(123)
#length is = (n_repeats*nresampling)+1
seeds <- vector(mode = "list", length = 11)
#(3 is the number of tuning parameter, mtry for rf, here equal to ncol(iris)-2)
for (i in 1:10)
    seeds[[i]] <- sample.int(n = 1000, 3)
#for the last model
seeds[[11]] <- sample.int(1000, 1)
#control list
myControl <- trainControl(method = 'cv', seeds = seeds, index = createFolds(iris$Species))
#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)
model1 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
model2 <- train(Species ~ ., iris, method = 'rf', trControl = myControl)
stopCluster(cl)
all.equal(predict(model1, type = 'prob'), predict(model2, type = 'prob'))
ndfCtrl <- trainControl(method = 'cv', seeds = seeds, index = createFolds(ndf$CYC))
#run model in parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)
model3 <- train(CYC ~ ., data=ndf, method = 'rf', trControl = ndfCtrl)
remove(dtm_train)
remove(mydf)
stopCluster(cl)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
stopCluster(cl)
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
model3 <- train(CYC ~ ., data=ndf, method = 'rf', trControl = ndfCtrl)
system.time(load("mydf_full_columns_fixed.RData"))
names(mydf)
mydf[1]$replace
mydf[1]$replace
mydf[1,]$replace
mydf[1,]$replace
mydf[1,15]
strwrap(mydf[1,15],100)
nchar(strwrap(mydf[1,15],100))
substr(mydf[1,15],70, 98)
substr(mydf[1,1],70, 98)
nchar(mydf[1:10,15])
substr(mydf[1,15],5600,5694)
substr(mydf[2,15],5600,5694)
nchar(mydf[1:10,15])
substr(mydf[2,15],4950,5002)
library(stringr)
str_sub(mydf[1:10,15], -2, -1)
str_sub(mydf[1:10,15], -20, -1)
str_sub(mydf[1:10,15], -50, -1)
str_sub(mydf[1:10,15], -40, -1)
str_sub(mydf[1:10,15], -40, -2)
str_sub(mydf[1:1,15], -40, -2)
str_sub(mydf[1:1,15], -40, -20)
?str_sub
str_sub(mydf[1:1,15], -40, -10)
str_sub(mydf[1:1,15], -40, -3)
str_sub(mydf[1:1,15], -40, -4)
str_sub(mydf[1:1,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -6)
str_sub(mydf[1:5,15], -40, -5)
foo <- "March 22, 2017 04:06 ET (08:06 GMT)"
as.date(foo)
as.Date(foo)
install.packages("parsedate")
library(parsedate)
parsedate(foo)
?parsedate
library(parsedate)
??parsedate
parse_date(foo)
foo
str_sub(mydf[1:5,15], -40, -5)
bar <- str_sub(mydf[1:5,15], -40, -5)
parse_date(bar)
str(parse_date(bar))
parse_date(str_sub(mydf[1:5,15], -40, -5))
parse_date(str_sub(mydf[1:5,15], -50, -5))
parse_date(str_sub(mydf[1:50,15], -50, -5))
parse_date(str_sub(mydf[1:150,15], -50, -5))
system.time(mydf$date <- parse_date(str_sub(mydf[,15], -50, -5)))
str_sub(mydf[125:150,15], -50, -5)
length(mydf$date)
length(mydf$date)
library(readr)
library(dplyr)
library(tm)
library(LDAvis)
# Function: Visualize LDA model (create JSON object) topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(doc_term)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) } create_dtm_from_isin <- function(isin_selection, dj.select) {     # load the bigram tokenizer     library(NLP)     BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)     # create a dtm     djtopic.dtm <- tm::DocumentTermMatrix(dj.select,     control = list(     #stemming = TRUE,             stopwords = TRUE,             minWordLength = 2,             removeNumbers = TRUE,             removePunctuation = TRUE             )             )     return(djtopic.dtm) } remove_sparse_terms <- function(djtopic.dtm) {     # sum up the dtm rows to gather aggregated counts     term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i],     djtopic.dtm$j, mean) * log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))     median <- summary(term_tfidf)[3]     # only include terms above the median     djreduced.dtm <- djtopic.dtm[, term_tfidf >= median]     summary(slam::col_sums(djreduced.dtm))     return(djreduced.dtm) } attribute_topics_to_docs <- function(dj.model, dj.topics) {     # Creates a dataframe to store the News Number and the most likely topic     doctopics.df <- as.data.frame(dj.topics)     doctopics.df <- dplyr::transmute(doctopics.df,     #NewsId = rownames(doctopics.df),         NewsId = isin_selection,         Topic = dj.topics)     doctopics.df$NewsId <- as.integer(doctopics.df$NewsId)     return(doctopics.df) } label_topics <- function(dj.terms) {     # label topics     topicTerms <- tidyr::gather(dj.terms, Topic)     topicTerms <- cbind(topicTerms, Rank = rep(1:30))     topTerms <- dplyr::filter(topicTerms, Rank < 4)     topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))     topTerms$Topic <- as.numeric(topTerms$Topic)     topicLabel <- data.frame()     for (i in 1:topicNumber) {         z <- dplyr::filter(topTerms, Topic == i)         l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "),             stringsAsFactors = FALSE)         topicLabel <- rbind(topicLabel, l)     }     colnames(topicLabel) <- c("Label")     return(topicLabel) } correlate_topics <- function(isin_selection, dj.model) {     baz <- strsplit(mydf[isin_selection,]$topic, ",")     seltops <- unlist(lapply(baz, `[[`, 1))     # correlate topics     theta <- as.data.frame(topicmodels::posterior(dj.model)$topics)     head(theta[1:5])     phi <- as.data.frame(topicmodels::posterior(dj.model)$terms)     head(phi[1])     x <- as.data.frame(row.names(theta), stringsAsFactors = FALSE)     colnames(x) <- c("NewsId")     x$NewsId <- as.numeric(x$NewsId)     theta2 <- cbind(x, theta)     theta2 <- dplyr::left_join(theta2, FirstCategorybyLesson, by = "NewsId")     ## Returns column means grouped by catergory     theta.mean.by <- by(theta2[, 2:28], theta2$Category, colMeans)     theta.mean <- do.call("rbind", theta.mean.by)     library(corrplot)     c <- cor(theta.mean)     corrplot(c, method = "circle")     return(theta.mean) } most_diagnostic_topics <- function(theta.mean) {     # most diagnostic topics     theta.mean.ratios <- theta.mean     for (ii in 1:nrow(theta.mean)) {         for (jj in 1:ncol(theta.mean)) {             theta.mean.ratios[ii, jj] <-             theta.mean[ii, jj] / sum(theta.mean[ii, - jj])         }     }     topics.by.ratio <- apply(theta.mean.ratios, 1,         function(x) sort(x, decreasing = TRUE, index.return = TRUE)$ix)     # The most diagnostic topics per category are found in the theta 1st row of the index matrix:     topics.most.diagnostic <- topics.by.ratio[1,]     head(topics.most.diagnostic) } find_optimal_topic_number <- function(dj.red.dtm) {     library(Rmpfr)     harmonicMean <- function(logLikelihoods, precision = 2000L) {         llMed <- median(logLikelihoods)         as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,                                        prec = precision) + llMed))))     }     k <- 25     burnin <- 1000     iter <- 1000     keep <- 50     fitted <- topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))     ## assuming that burnin is a multiple of keep     logLiks <- fitted@logLiks[-c(1:(burnin / keep))]     ## This returns the harmomnic mean for k = 25 topics.     harmonicMean(logLiks)     seqk <- seq(2, 100, 1)     burnin <- 1000     iter <- 1000     keep <- 50     system.time(fitted_many <- lapply(seqk, function(k) topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))))     cl <- makePSOCKcluster(8)     setDefaultCluster(cl)     #adder <- function(a, b) a + b     #clusterExport(NULL, c('adder'))     #parLapply(NULL, 1:8, function(z) adder(z, 100))     clusterExport(cl, c('LDA', 'dj.red.dtm', 'burnin', 'iter', 'keep'))     parLapply(cl, seqk, function(k)         LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep)))     # extract logliks from each topic     logLiks_many <- lapply(fitted_many, function(L) L@logLiks[-c(1:(burnin / keep))])     # compute harmonic means     hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))     ldaplot <- ggplot(data.frame(seqk, hm_many), aes(x = seqk, y = hm_many)) + geom_path(lwd = 1.5) +     theme(text = element_text(family = NULL),         axis.title.y = element_text(vjust = 1, size = 16),         axis.title.x = element_text(vjust = -.5, size = 16),         axis.text = element_text(size = 16),         plot.title = element_text(size = 20)) +     xlab('Number of Topics') +     ylab('Harmonic Mean') +      annotate("text", x = 25, y = -80000, label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +     ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of DJ Newswire", atop(italic("How many distinct topics in the abstracts?"), ""))))     return(ldaplot) } toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) { stop <- nchar(x); substring(x, start, stop) }) tokenize_corp <- function(corpus) {     newcorp <- corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Email.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Write.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(MORE TO FOLLOW.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(END\\).*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r") %>%     tm_map(toSpace, "\"")     #tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)")     #tm_map(replacener)     return(newcorp) }
b <- VectorSource(mydf$replace)
#dj.corpus <- Corpus(b, 
readerControl = list(language = "eng", reader = readPlain))
readerControl = list(language = "eng", reader = readPlain))
dj.corpus <- VCorpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
vw <- "DE0007664039"
stl <- "NO0010096985"
ibm <- "US4592001014"
tesla <- "US88160R1014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG)
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 50
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
mydf[1:10,]$date
mydf[1:10,]$date > 2017-03
mydf[1:10,]$date > 2017-03-01
mydf[1:10,]$date > as.Date(2017-03-01)
mydf[1:10,]$date > as.Date("2017-03-01")
str(mydf$date)
mydf[1,]$date > mydf[2,]$date
mydf[1,]$date < mydf[2,]$date
as.Date.date
?as.Date
as.Date("2017-03-1", "%y/%m/%d")
as.Date("2017-03-1", "%Y/%m/%d")
as.Date("2017-03-1", "%yy/%m/%d")
format(Sys.Date(), "%a %b %d")
format(Sys.Date(), "%a %b %d %y %m %d")
format(Sys.Date(), "%a %b %d %y %m %d %Y")
as.Date("2017-03-1", "%Y-%m-%d")
mydf[1:19]$date > as.Date("2017-03-1", "%Y-%m-%d")
mydf[1:19]$date > as.Date("2017-03-01", "%Y-%m-%d")
mydf[1:19]$date 
mydf[1:19,]$date > as.Date("2017-03-01", "%Y-%m-%d")
?as.Date
mydf[1:19,]$date > parse_date("2017-03-01")
str(mydf[1:19,]$date)
as.Date(mydf[1:19,]$date)
as.Date(mydf[1:19,]$date) > "2017-03-01"
system.time(mydf$date <- as.Date(parse_date(str_sub(mydf[, 15], -50, -5))))
system.time(mydf$date <- parse_date(str_sub(mydf[, 15], -50, -5)))
length(mydf$date)
str(mydf$date)
system.time(mydf$date <- parse_date(str_sub(mydf[, 15], -40, -5)))
names(mydf)[1:20]
names(mydf)[1:15]
system.time(mydf$date <- parse_date(str_sub(mydf$replace, -40, -5)))
parse_date(str_sub(mydf[1:100,]$replace, -40, -5))
head(mydf)
system.time(load("mydf_full_columns_fixed.RData"))
parse_date(str_sub(mydf[1:100, 15], -50, -5)))
parse_date(str_sub(mydf[1:100, 15], -50, -5))
str_sub(mydf[1:100, 15], -50, -5)
parse_date(str_sub(mydf[1:100, 15], -50, -5))
parse_date(str_sub(mydf[1:10, 15], -50, -5))
library(parsedate)
parse_date(str_sub(mydf[1:10, 15], -50, -5))
parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T)
parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T)
?parse_date
parsedate::parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T)
as.Date(parsedate::parse_date(str_sub(mydf[1:10, 15], -50, -5), approx = T))
system.time(mydf$date <- as.Date(parsedate::parse_date(str_sub(mydf[, 15], -50, -5))))
mydf[1:100,]$date > 2017-03-01
mydf[1:100,]$date < 2017-03-01
mydf[1:100,]$date
str(mydf[1:100,]$date)
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%D")
mydf[1:100,]$date < as.Date("2017-03-01", "%Y-%m-%D")
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%D")
as.Date("2017-03-01", "%Y-%m-%D")
as.Date("2017-03-01", "%Y-%m-%d")
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d")
mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d")
which(mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
strwrap(mydf[12,15], 100)
which(mydf[1:100,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
which(mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
which(mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[1:100,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-03-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-04-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-02-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-03-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2017-01-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-02-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-01-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
topicLabel
?mldr_transform
emo_lp <- mldr_transform(emotions, "LP")
emo_br <- mldr_transform(emotions, "BR")
names(emotions)
¨summary(emotions)
summary(emotions)
emotions$labels
library(mldr)
summary(emotions)
emotions$labels
emo_lp <- mldr_transform(emotions, "LP")
emo_br <- mldr_transform(emotions, "BR")
str(emo_br)
length(emo_br)
summary(emotions)
emotions$labels
str(emo_br[1])
names(emo_br[1])
str(emo_br[1])
length(emo_br[1])
length(emo_br[2])
str(emo_br[2])
str(emo_br[3])
str(emo_lp[3])
str(emo_lp)
?mldr_transform
emo_lp$classLabel
levels(emo_lp$classLabel)
length(levels(emo_lp$classLabel))
summary(emotions)
library(RWeka)summary(emotions)
library(RWeka)
classifier <- IBk(classLabel ~ ., data = emo_lp, control = Weka_control(K = 10))
evaluate_Weka_classifier(classifier, numFolds = 5)
summary(emotions)
summary(emotions)
evaluate_Weka_classifier(classifier, numFolds = 5)
summary(emotions)
summary(emotions)
topicLabel
topicLabel <- label_topics(dj.terms)
library(readr)
library(dplyr)
library(tm)
library(LDAvis)
# Function: Visualize LDA model (create JSON object) topicmodels_json_ldavis <- function(fitted, corpus, doc_term) {     ## Required packages     library(topicmodels)     library(dplyr)     library(stringi)     library(tm)     library(LDAvis)     ## Find required quantities     phi <- posterior(fitted)$terms %>% as.matrix     theta <- posterior(fitted)$topics %>% as.matrix     vocab <- colnames(phi)     length(vocab)     doc_length <- vector()     for (i in 1:length(corpus)) {         temp <- paste(corpus[[i]]$content, collapse = ' ')         doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))     }     #temp_frequency <- inspect(doc_term)     #freq_matrix <- data.frame(ST = colnames(temp_frequency),     #                           Freq = colSums(temp_frequency))     #rm(temp_frequency)     bar <- slam::col_sums(doc_term)     freq_matrix <- data.frame(ST = names(bar), Freq = bar)     #freq_matrix <- as.data.frame(t(slam::row_sums(doc_term)))     ## Convert to json     json_lda <- LDAvis::createJSON(phi = phi, theta = theta,                                     vocab = vocab,                                     doc.length = doc_length,                                     term.frequency = freq_matrix$Freq)     return(json_lda) } create_dtm_from_isin <- function(isin_selection, dj.select) {     # load the bigram tokenizer     library(NLP)     BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)     # create a dtm     djtopic.dtm <- tm::DocumentTermMatrix(dj.select,     control = list(     #stemming = TRUE,             stopwords = TRUE,             minWordLength = 2,             removeNumbers = TRUE,             removePunctuation = TRUE             )             )     return(djtopic.dtm) } remove_sparse_terms <- function(djtopic.dtm) {     # sum up the dtm rows to gather aggregated counts     term_tfidf <- tapply(djtopic.dtm$v / slam::row_sums(djtopic.dtm)[djtopic.dtm$i],     djtopic.dtm$j, mean) * log2(tm::nDocs(djtopic.dtm) / slam::col_sums(djtopic.dtm > 0))     median <- summary(term_tfidf)[3]     # only include terms above the median     djreduced.dtm <- djtopic.dtm[, term_tfidf >= median]     summary(slam::col_sums(djreduced.dtm))     return(djreduced.dtm) } attribute_topics_to_docs <- function(dj.model, dj.topics) {     # Creates a dataframe to store the News Number and the most likely topic     doctopics.df <- as.data.frame(dj.topics)     doctopics.df <- dplyr::transmute(doctopics.df,     #NewsId = rownames(doctopics.df),         NewsId = isin_selection,         Topic = dj.topics)     doctopics.df$NewsId <- as.integer(doctopics.df$NewsId)     return(doctopics.df) } label_topics <- function(dj.terms) {     # label topics     topicTerms <- tidyr::gather(dj.terms, Topic)     topicTerms <- cbind(topicTerms, Rank = rep(1:30))     topTerms <- dplyr::filter(topicTerms, Rank < 4)     topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))     topTerms$Topic <- as.numeric(topTerms$Topic)     topicLabel <- data.frame()     for (i in 1:topicNumber) {         z <- dplyr::filter(topTerms, Topic == i)         l <- as.data.frame(paste(z[1, 2], z[2, 2], z[3, 2], sep = " "),             stringsAsFactors = FALSE)         topicLabel <- rbind(topicLabel, l)     }     colnames(topicLabel) <- c("Label")     return(topicLabel) } correlate_topics <- function(isin_selection, dj.model) {     baz <- strsplit(mydf[isin_selection,]$topic, ",")     seltops <- unlist(lapply(baz, `[[`, 1))     # correlate topics     theta <- as.data.frame(topicmodels::posterior(dj.model)$topics)     head(theta[1:5])     phi <- as.data.frame(topicmodels::posterior(dj.model)$terms)     head(phi[1])     x <- as.data.frame(row.names(theta), stringsAsFactors = FALSE)     colnames(x) <- c("NewsId")     x$NewsId <- as.numeric(x$NewsId)     theta2 <- cbind(x, theta)     theta2 <- dplyr::left_join(theta2, FirstCategorybyLesson, by = "NewsId")     ## Returns column means grouped by catergory     theta.mean.by <- by(theta2[, 2:28], theta2$Category, colMeans)     theta.mean <- do.call("rbind", theta.mean.by)     library(corrplot)     c <- cor(theta.mean)     corrplot(c, method = "circle")     return(theta.mean) } most_diagnostic_topics <- function(theta.mean) {     # most diagnostic topics     theta.mean.ratios <- theta.mean     for (ii in 1:nrow(theta.mean)) {         for (jj in 1:ncol(theta.mean)) {             theta.mean.ratios[ii, jj] <-             theta.mean[ii, jj] / sum(theta.mean[ii, - jj])         }     }     topics.by.ratio <- apply(theta.mean.ratios, 1,         function(x) sort(x, decreasing = TRUE, index.return = TRUE)$ix)     # The most diagnostic topics per category are found in the theta 1st row of the index matrix:     topics.most.diagnostic <- topics.by.ratio[1,]     head(topics.most.diagnostic) } find_optimal_topic_number <- function(dj.red.dtm) {     library(Rmpfr)     harmonicMean <- function(logLikelihoods, precision = 2000L) {         llMed <- median(logLikelihoods)         as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,                                        prec = precision) + llMed))))     }     k <- 25     burnin <- 1000     iter <- 1000     keep <- 50     fitted <- topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))     ## assuming that burnin is a multiple of keep     logLiks <- fitted@logLiks[-c(1:(burnin / keep))]     ## This returns the harmomnic mean for k = 25 topics.     harmonicMean(logLiks)     seqk <- seq(2, 100, 1)     burnin <- 1000     iter <- 1000     keep <- 50     system.time(fitted_many <- lapply(seqk, function(k) topicmodels::LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep))))     cl <- makePSOCKcluster(8)     setDefaultCluster(cl)     #adder <- function(a, b) a + b     #clusterExport(NULL, c('adder'))     #parLapply(NULL, 1:8, function(z) adder(z, 100))     clusterExport(cl, c('LDA', 'dj.red.dtm', 'burnin', 'iter', 'keep'))     parLapply(cl, seqk, function(k)         LDA(dj.red.dtm, k = k, method = "Gibbs", control = list(burnin = burnin, iter = iter, keep = keep)))     # extract logliks from each topic     logLiks_many <- lapply(fitted_many, function(L) L@logLiks[-c(1:(burnin / keep))])     # compute harmonic means     hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))     ldaplot <- ggplot(data.frame(seqk, hm_many), aes(x = seqk, y = hm_many)) + geom_path(lwd = 1.5) +     theme(text = element_text(family = NULL),         axis.title.y = element_text(vjust = 1, size = 16),         axis.title.x = element_text(vjust = -.5, size = 16),         axis.text = element_text(size = 16),         plot.title = element_text(size = 20)) +     xlab('Number of Topics') +     ylab('Harmonic Mean') +      annotate("text", x = 25, y = -80000, label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +     ggtitle(expression(atop("Latent Dirichlet Allocation Analysis of DJ Newswire", atop(italic("How many distinct topics in the abstracts?"), ""))))     return(ldaplot) } toSpace <- content_transformer(function(x, pattern) gsub(pattern, "", x)) toSmth <- content_transformer(function(x, pattern, smth) gsub(pattern, smth, x)) chophead <- content_transformer(function(x, start) { stop <- nchar(x); substring(x, start, stop) }) tokenize_corp <- function(corpus) {     newcorp <- corpus %>% tm_map(chophead, 33) %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Email.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Write.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(MORE TO FOLLOW.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(END\\).*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r") %>%     tm_map(toSpace, "\"")     #tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)")     #tm_map(replacener)     return(newcorp) }
# import the column in the dataframe as a tm corpus object (presupposes a mydf from the file cvsreader.R b <- VectorSource(mydf$replace) #dj.corpus <- Corpus(b,  #readerControl = list(language = "eng", reader = readPlain)) # Need VCorpus in order exploit bigram tokenization dj.corpus <- VCorpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain)) test.select <- test.corpus[isin_selection] vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
b <- VectorSource(mydf$replace)
system.time(load("mydf_full_columns_fixed.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed.RData"))
b <- VectorSource(mydf$replace) #dj.corpus <- Corpus(b,  #readerControl = list(language = "eng", reader = readPlain)) # Need VCorpus in order exploit bigram tokenization dj.corpus <- VCorpus(b, ## I change DireSource(a) by a           readerControl = list(language = "eng", reader = readPlain))
vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
system.time(mydf$date <- as.Date(parsedate::parse_date(str_sub(mydf[, 15], -50, -5))))
library(stringr)
library(parsedate)
system.time(mydf$date <- as.Date(parsedate::parse_date(str_sub(mydf[, 15], -50, -5))))
vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
vw <- "DE0007664039" stl <- "NO0010096985" ibm <- "US4592001014" tesla <- "US88160R1014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-01-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2016-12-08", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus[isin_selection]
system.time(dj.select_tok <- tokenize_corp(dj.select))
dj.dtm <- create_dtm_from_isin(isin_selection, dj.select_tok)
dj.red.dtm <- remove_sparse_terms(dj.dtm)
dj.red.dtm
# select number of topics
topicNumber <- 10
# Run model
system.time(dj.model <- topicmodels::LDA(dj.red.dtm, topicNumber, method = "Gibbs", control = list(iter = 2000, seed = 0622)))
## Adds topic number to original dataframe of lessons
## dj.display <- dplyr::inner_join(dj.display, doctopics.df, by = "LessonId")
## Pull out topics and terms 
dj.topics <- topicmodels::topics(dj.model, 1)
dj.terms <- as.data.frame(topicmodels::terms(dj.model, 30), stringsAsFactors = FALSE)
dj.terms[1:5]
doctopics.df <- attribute_topics_to_docs(dj.model, dj.topics)
doctopics.df[which(doctopics.df$Topic == 1),]$NewsId
topicLabel <- label_topics(dj.terms)
topicLabel
dj.json <- topicmodels_json_ldavis(dj.model, dj.select, dj.red.dtm)
serVis(dj.json)
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
phi <- posterior(dj.model)$terms %>% as.matrix
theta <- posterior(dj.model)$topics %>% as.matrix
phi
dim(phi)
dim(theta)
head(theta)
phi
head(theta)
length(mydf)
mydf[1:10,]$date
system.time(save(mydf, file = c("mydf_full_columns_fixed_1346_with_date.RData")))
tokenize_corp <- function(corpus) {     newcorp <- corpus %>% tm_map(toSpace, "[0-9]{4}.*\\\\r\\\\t\\\\t\\\\t\\\\t\\\\t\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Email.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+Write.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(MORE TO FOLLOW.*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r [[:space:]]+\\(END\\).*\\)\\\\r\\\\t") %>%     tm_map(toSpace, "\\\\r\\\\t\\\\t\\\\r") %>%     tm_map(toSpace, "\"")     #tm_map(toSpace, "\\( \\w+.*\\w+ @.* \\)")     #tm_map(replacener)     return(newcorp) }
library(parallel)
cl <- makePSOCKcluster(7)
setDefaultCluster(cl)
tm_parLapply_engine(cl)
library(tm)
tm_parLapply_engine(cl)
install.packages("tm")
tm_parLapply_engine(cl)
?tm
??tm
tm_parLapply_engine(cl)
?tm_parLapply_engine
??tm_parLapply_engine
??tm
?tm
??tm_parLapply_engine
?tm_parLapply_engine
str(tm)
tm
tm_map
sessionInfo()
install.packages("tm")
install.packages("Rccp")
install.packages('Rcpp')
library(tm)
tm_parLapply_engine
tm
detach_package(tm, TRUE)
detach("package:tm", unload = TRUE)
detach("package:topicmodels", unload = TRUE)
detach("package:tm", unload = TRUE)
install.packages('tm')
library(tm)
install.packages('tm')
library(tm)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
dj.corpus_tok
summary(dj.corpus_tok)
head(mydf$data)
head(mydf$date)
dj.corpus_tok$meta
str(dj.corpus_tok$meta)
str(dj.corpus_tok)
dj.corpus_tok$meta$id
str(dj.corpus_tok$meta$id)
str(dj.corpus_tok$meta)
attr(dj.corpus_tok$meta)
attr(dj.corpus_tok$meta)
attributes(dj.corpus_tok$meta)
metadata(dj.corpus_tok)
library(tm)
install.packages("tm")
library(tm)
metadata(dj.corpus_tok)
meta(dj.corpus_tok)
meta(dj.corpus_tok[[1]])
head(meta(dj.corpus_tok, id))
head(meta(dj.corpus_tok, tag = id))
head(meta(dj.corpus_tok[[1]], tag = id))
meta(dj.corpus_tok[[1]], tag = id)
head(meta(dj.corpus_tok[[1]], tag = "id"))
head(meta(dj.corpus_tok, tag = "id"))
system.time(meta(dj.corpus_tok, tag = "date") <- mydf$date)
meta(dj.corpus_tok[[1]], tag = id)
meta(dj.corpus_tok[[1]])
meta(dj.corpus_tok, tag = "date") <- mydf$date
meta(dj.corpus_tok)
meta(dj.corpus_tok, tag = "date") <- c("")
meta(dj.corpus_tok)
for (i in 1:length(mydf)) {  meta(dj.corpus_tok[[i]], tag = "date") <- mydf[[i]]$date }
for (i in 1:length(mydf)) {  meta(dj.corpus_tok[[i]], tag = "date") <- mydf[[i,]]$date }
for (i in 1:length(mydf)) {  meta(dj.corpus_tok[[i]], tag = "date") <- mydf[i,]$date }
meta(dj.corpus_tok[[1]])
dj.corpus_tok[[1]]$content
mydf[1]$replace
mydf[1,]$replace
mydf[1,]$date
meta(dj.corpus_tok[[1]])
tesla <- "US88160R1014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2016-12-08", "%Y-%m-%d"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2016-12-08", "%Y-%m-%d"))
length(isin_selection)
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-01-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
dj.select$meta
str(dj.select$meta)
dj.select$meta
dj.select
meta(dj.select)
meta(dj.select, tag="date")
meta(dj.select[[1]])
meta(dj.corpus_tok[[1]])
meta(dj.corpus_tok[[100]])
meta(dj.corpus_tok[[1000]])
meta(dj.corpus_tok[[11000]])
meta(dj.corpus_tok[[11000]])
length(dj.corpus_tok)
meta(dj.corpus_tok[[11000]])
meta(dj.corpus_tok[[5000]])
meta(dj.corpus_tok[[1000]])
meta(dj.corpus_tok[[2000]])
meta(dj.corpus_tok[[1100]])
meta(dj.corpus_tok[[1500]])
meta(dj.corpus_tok[[1240]])
meta(dj.corpus_tok[[1340]])
meta(dj.corpus_tok[[1440]])
meta(dj.corpus_tok[[1400]])
meta(dj.corpus_tok[[1390]])
meta(dj.corpus_tok[[1360]])
meta(dj.corpus_tok[[1340]])
meta(dj.corpus_tok[[1350]])
meta(dj.corpus_tok[[1346]])
meta(dj.corpus_tok[[1349]])
meta(dj.corpus_tok[[1347]])
meta(dj.corpus_tok[[1346]])
meta(dj.corpus_tok[[1346]])
lengt(dj.corpus_tok)
length(dj.corpus_tok)
length(mydf)
dim(mydf)
dim(mydf)[1]
for (i in 1:dim(mydf)[1]) { meta(dj.corpus_tok[[i]], tag = "date") <- mydf[i,]$date }
meta(dj.select[[1]])
dj.select[[1]]$content
mydf[[68497]]$replace
mydf[[68497,]]$replace
mydf[68497,]$replace
length(dj.select)
mydf[meta(dj.select[[68497]], tag = "id"),]$date)
meta(dj.select[[68497]], tag = "id")
mydf[meta(dj.select[[i]], tag = "id"),]$date)
meta(dj.select[[i]], tag = "id"),]
meta(dj.select[[i]], tag = "id")
meta(dj.select[[1]], tag = "id")
mydf[meta(dj.select[[1]], tag = "id"),]
mydf[meta(dj.select[[1]], tag = "id"),]$date
for (i in 1:length(dj.select)) {     print(mydf[meta(dj.select[[i]], tag = "id"),]$date) }
for (i in 1:length(dj.select)) {     meta(dj.select[[i]], tag = "date") <-  mydf[meta(dj.select[[i]], tag = "id"),]$date }
meta(dj.select[[1]], tag = "date")
meta(dj.select[[33]], tag = "date")
mydf[isin_selection,]$date
processed <- textProcessor(dj.select$documents, metadata = mydf[isin_selection,]$date)
library(stm)
install.packages("stm")
library(stm)
processed <- textProcessor(dj.select$documents, metadata = mydf[isin_selection,]$date)
mydf[isin_selection,]$date
data.frame(mydf[isin_selection,]$date)
processed <- textProcessor(dj.select$documents, metadata = data.frame(mydf[isin_selection,]$date))
processed <- textProcessor(dj.select$content, metadata = data.frame(mydf[isin_selection,]$date))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
vocab
meta
plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
poliblogPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 20,     prevalence = ~rating + s(day),     max.em.its = 75,     data = out$meta, init.type = "Spectral")
teslaPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = s(date),     max.em.its = 75,     data = out$meta, init.type = "Spectral")
teslaPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = date,     max.em.its = 75,     data = out$meta, init.type = "Spectral")
teslaPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = NULL,     max.em.its = 75,     data = out$meta, init.type = "Spectral")
setwd("C:\\Users\\bungum\\Documents")
library(tm)
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
for (i in 1:length(dj.select)) {     meta(dj.select[[i]], tag = "date") <- mydf[meta(dj.select[[i]], tag = "id"),]$date }
processed <- textProcessor(dj.select$content, metadata = data.frame(mydf[isin_selection,]$date))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
library(stm)
processed <- textProcessor(dj.select$content, metadata = data.frame(mydf[isin_selection,]$date))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
teslaPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = ~s(date),     max.em.its = 75,     data = out$meta, init.type = "Spectral")
teslaPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = ~date,     max.em.its = 75,     data = out$meta, init.type = "Spectral")
data.frame(mydf[isin_selection,]$date)
names(data.frame(mydf[isin_selection,]$date))
isindf <- data.frame(mydf[isin_selection,]$date)
colnames(isindf)
colnames(isindf)[1] <- c("date")
processed <- textProcessor(dj.select$content, metadata = isindf)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
teslaPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = ~date,     max.em.its = 75,     data = out$meta, init.type = "Spectral")
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
colnames(mydf)[1:15]
colnames(mydf)
colnames(mydf)[1346]
for (i in 1:length(dj.select)) {     meta(dj.select[[i]], tag = "date") <- mydf[meta(dj.select[[i]], tag = "id"),1346] }
library(tm)
for (i in 1:length(dj.select)) {     meta(dj.select[[i]], tag = "date") <- mydf[meta(dj.select[[i]], tag = "id"),1346] }
meta(dj.select[[1]], tag = "id")
dj.select
dj.selectdj.select <- dj.corpus_tok[isin_selection]
rm(dj.selectdj.select, envir = as.environment(".GlobalEnv"))
dj.select <- dj.corpus_tok[isin_selection]
dj.select[[1]]
dj.select[[1]]$meta
dj.select[[1]]$content
meta(dj.select[[1]])
meta(dj.select[[1]], tag =  "date")
meta(dj.select[[1]], tag =  "id")
meta(dj.select[[1]], tag =  "id")
meta(dj.select[[2]], tag =  "id")
mydf[meta(dj.select[[2]], tag =  "id"),1346]
for (i in 1:length(dj.select)) {     meta(dj.select[[i]], tag = "date") <- mydf[meta(dj.select[[i]], tag = "id"),1346] }
system.time(save(dj.select, file = c("dj.select.RData")))
isindf <- data.frame(mydf[isin_selection,1346])
processed <- textProcessor(dj.select$content, metadata = isindf)
library(stm)
processed <- textProcessor(dj.select$content, metadata = isindf)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
colnames(isindf)[1] <- c("date")
ibmPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = ~date,     max.em.its = 50,     data = out$meta, init.type = "Spectral")
ibmPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = ~date,     max.em.its = 50,     data = out$meta, init.type = "Spectral")
processed <- textProcessor(dj.select$content, metadata = isindf)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 5,     prevalence = ~date,     max.em.its = 50,     data = out$meta, init.type = "Spectral")
library(tm)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
system.time(load("dj.select.RData"))
dj.select
dj.select[[1]]
meta(dj.select[[1]])
download.file(polit_url, "poliboligs2008.csv")
polit_url <- "http://scholar.princeton.edu/sites/default/files/bstewart/files/poliblogs2008.csv"
download.file(polit_url, "poliboligs2008.csv")
data <- read.csv("poliboligs2008.csv")
setwd("C:\\Users\\bungum\\Documents")
download.file(polit_url, "poliboligs2008.csv")
download.file(polit_url, "poliboligs2008.csv")
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.select.RData"))
isindf <- data.frame(mydf[isin_selection,1346])
colnames(isindf)[1] <- c("date")
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
isindf <- data.frame(mydf[isin_selection,1346])
colnames(isindf)[1] <- c("date")
data <- read.csv("poliboligs2008.csv")
download.file(polit_url, "poliblogs2008.csv")
data <- read.csv("poliblogs2008.csv")
processed <- textProcessor(data$documents, metadata = data)
library(stm)
processed <- textProcessor(data$documents, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
str(meta)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(processed$documents, processed$vocab, processed$meta)
mydocs <- out$documents
myvocab <- out$vocab
mymeta <- out$meta
plotRemoved(myprocessed$documents, lower.thresh = seq(1, 200, by = 100))
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
library(tm)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.select.RData"))
data <- read.csv("poliblogs2008.csv")
processed <- textProcessor(data$documents, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
library(stm)
processed <- textProcessor(data$documents, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
library(stm)
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
isindf <- data.frame(mydf[isin_selection,1346])
colnames(isindf)[1] <- c("date")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(processed$documents, processed$vocab, processed$meta)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- out$documents
myvocab <- out$vocab
mymeta <- out$meta
plotRemoved(myprocessed$documents, lower.thresh = seq(1, 200, by = 100))
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
str(meta)''
str(meta)
str(mymeta)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- out$documents
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
str(mymeta)
str(meta)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
str(myout)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
str(mymeta)
str(mydocs)
str(mymeta)
mymeta$date
as.Date(mymeta$date )
as.Date(mymeta$date)
as.Date(mymeta[1]$date)
as.Date(mymeta[[1]]$date)
mymeta$date
mymeta$date[1]
as.Date(mymeta$date[1])
as.numeric(mymeta$date[1])
min(as.numeric(mymeta$date[1]))
min(as.numeric(mymeta$date))
which(min(as.numeric(mymeta$date)))
min(as.numeric(mymeta$date))
which(as.mumeric(mymeta$date) == min(as.numeric(mymeta$date)))
which(as.numeric(mymeta$date) == min(as.numeric(mymeta$date)))
mymeta$date[102]
mymeta$date[107]
mymeta$date
as.numberic(mymeta$date)
as.numeric(mymeta$date)
mymeta$date <- as.numeric(mymeta$date)-min(as.numeric(mymeta$date))
mymeta$date
str(mymeta)
mymeta$date <- as.integer(mymeta$date)
str(mymeta)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = ~date,     max.em.its = 50,     data = myout$meta, init.type = "Spectral")
setwd("C:\\Users\\bungum\\Documents")
library(tm)
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.select.RData"))
isindf <- data.frame(mydf[isin_selection,1346])
colnames(isindf)[1] <- c("date")
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
mymeta$date <- as.numeric(mymeta$date) - min(as.numeric(mymeta$date))
library(stm)
myprocessed <- textProcessor(dj.select$content, metadata = isindf) myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta) mydocs <- myout$documents myvocab <- myout$vocab mymeta <- myout$meta mymeta$date <- as.numeric(mymeta$date) - min(as.numeric(mymeta$date))
isindf <- data.frame(mydf[isin_selection,1346]) colnames(isindf)[1] <- c("date") myprocessed <- textProcessor(dj.select$content, metadata = isindf) myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta) mydocs <- myout$documents myvocab <- myout$vocab mymeta <- myout$meta mymeta$date <- as.numeric(mymeta$date) - min(as.numeric(mymeta$date))
str(mymeta)
mymeta$date <- as.integer(mymeta$date) - min(as.integer(mymeta$date))
str(mymeta)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = ~s(date),     max.em.its = 50,     data = myout$meta, init.type = "Spectral")
data <- read.csv("poliblogs2008.csv") processed <- textProcessor(data$documents, metadata = data) out <- prepDocuments(processed$documents, processed$vocab, processed$meta) docs <- out$documents vocab <- out$vocab meta <- out$meta
library(stm)
data <- read.csv("poliblogs2008.csv") processed <- textProcessor(data$documents, metadata = data) out <- prepDocuments(processed$documents, processed$vocab, processed$meta) docs <- out$documents vocab <- out$vocab meta <- out$meta
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
poliblogPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 20,     prevalence = ~rating + s(day),     max.em.its = 15,     data = out$meta, init.type = "Spectral")
out$documents
str(out$documents)
str(out$vocab)
str(out$meta)
str(myout$meta)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.select.RData"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
isindf <- data.frame(mydf[isin_selection,])
colnames(isindf)[1:19]
colnames(isindf)[1346]
colnames(mydf)[1346]
length(dj.select)
length(isindf)
dim(isindf)
mydf$replace <- dj.select$content
isindf$replace <- dj.select$content
myprocessed <- textProcessor(isindf$replace, metadata = isindf)
head(isindf)
isindf[1:10,]$replace
dj.select[[1]]$content
dj.select[1:10]$content
lapply(dj.select[1:10], '[[')
lapply(dj.select[1:10], '[[', 1)
isindf$replace <- lapply(dj.select[1:10], '[[', 1)
isindf$replace <- lapply(dj.select, '[[', 1)
isindf[1:10,]$replace
myprocessed <- textProcessor(isindf$replace, metadata = isindf)
isindf <- data.frame(mydf[isin_selection,])
myprocessed <- textProcessor(isindf$replace, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
mymeta
mymeta$date <- as.integer(mymeta$date) - min(as.integer(mymeta$date))
plotRemoved(myprocessed$documents, lower.thresh = seq(1, 200, by = 100))
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
head(myvocab)
head(vocab)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = ~s(date),     max.em.its = 50,     data = myout$meta, init.type = "Spectral")
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.select.RData"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-12-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
isindf <- data.frame(mydf[isin_selection,])
colnames(isindf)
names(isindf)[1:19]
system.time(save(isindf, file = c("isindf_ibm.RData")))
library(stm)
myprocessed <- textProcessor(isindf$replace, metadata = isindf[,1346])
myprocessed <- textProcessor(isindf$replace, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
mymeta$date <- as.integer(mymeta$date) - min(as.integer(mymeta$date))
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = NULL,     max.em.its = 50,     data = myout$meta, init.type = "Spectral")
system.time(load("isindf_ibm.RData"))
library(stm)
myprocessed <- textProcessor(isindf$replace, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = NULL,     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
myprocessed <- textProcessor(isindf$replace, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
str(mymeta)
mymeta[1:10,]$date
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = NULL,     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = ~s(date),     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
labelTopics(ibmPrevFit, c(1, 3, 5))
thoughts3 <- findThoughts(ibmPrevFit, texts = shortdoc,     n = 2, topics = 3)$docs[[1]]
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, "day", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(ibmContent, type = "perspectives", topics = 1)
ibmPrevFit <- stm(documents = dj.select$content,     vocab = myout$vocab, K = 5,     prevalence = ~s(date),     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
system.time(load("dj.select.RData"))
ibmPrevFit <- stm(documents = dj.select$content,     vocab = myout$vocab, K = 5,     prevalence = ~s(date),     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 5,     prevalence = ~s(date),     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
labelTopics(ibmPrevFit, c(1, 3, 5))
storage <- searchK(out$documents, out$vocab, K = c(1, 5),     prevalence = ~s(day),     data = meta)
storage <- searchK(myout$documents, out$vocab, K = c(1, 5),     prevalence = ~s(day),     data = meta)
storage <- searchK(myout$documents, myout$vocab, K = c(1, 5),     prevalence = ~s(day),     data = meta)
storage <- searchK(myout$documents, myout$vocab, K = c(2, 5),     prevalence = ~s(day),     data = meta)
storage <- searchK(myout$documents, myout$vocab, K = c(2, 5),     prevalence = ~s(day),     data = mymeta)
storage <- searchK(myout$documents, myout$vocab, K = c(2, 5),     prevalence = ~s(date),     data = mymeta)
storage
data <- read.csv("poliblogs2008.csv")
names(data)
thoughts3 <- findThoughts(poliblogPrevFit, texts = shortdoc,     n = 2, topics = 3)$docs[[1]]
thoughts3 <- findThoughts(ibmPrevFit, texts = shortdoc,     n = 2, topics = 3)$docs[[1]]
processed <- textProcessor(data$documents, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
poliblogPrevFit <- stm(documents = out$documents,     vocab = out$vocab, K = 20,     prevalence = ~rating + s(day),     max.em.its = 15,     data = out$meta, init.type = "Spectral")
labelTopics(poliblogPrevFit, c(3, 7, 20))
thoughts3 <- findThoughts(poliblogPrevFit, texts = shortdoc,     n = 2, topics = 3)$docs[[1]]
load(url("http://goo.gl/VPdxlS"))
dj.select[1:10]
dj.select[1:10]$content
substr(dj.select[1]$content,0, 100)
substr(mydf[1:10],0, 100)
substr(isindf[1:10,]$replace,0, 100)
substr(isindf[1:10,]$replace,50, 100)
substr(isindf[1:10,]$replace,40, 240)
myshortdoc <- substr(isindf$replace,40, 240)
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 3)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = shortdoc,     n = 2, topics = 5)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 5)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts20, width = 30, main = "Topic 20")
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts5, width = 30, main = "Topic 20")
substr(dj.select[1]$content,0, 100)
names(dj.select[1])
dj.select[1][1]
dj.select[1][,1]
dj.select[1,1]
dj.select[1]
dj.select[1]$content
lapply(dj.select[1:10], '[', 1)
lapply(dj.select[1:10], '$content')
lapply(dj.select[1:10], $content)
lapply(dj.select[1:10], '[', 1)
lapply(dj.select[1:10], '[[', 1)
sapply(dj.select[1:10], '[[', 1)
lapply(dj.select[1:10], '[[', 1)
lapply(dj.select[1:10], '[[', 2)
lapply(dj.select[1:10], '[[', 1)
substr(lapply(dj.select[1:10], '[[', 1), 0, 200)
mtshortdoc <- substr(lapply(dj.select[1:10], '[[', 1), 0, 200)
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 3)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 5)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts5, width = 30, main = "Topic 20")
thoughts3
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
prep <- estimateEffect(1:20 ~ s(day), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
prep <- estimateEffect(1:5 ~ s(day), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
prep <- estimateEffect(1:5 ~ s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
plot(prep, "date", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(ibmContent, type = "perspectives", topics = 1)
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = ~s(date),     max.em.its = 75, data = out$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = ~s(date),     max.em.its = 75, data = myout$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = s(date),     max.em.its = 75, data = myout$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = ~s(date),     max.em.its = 75, data = myout$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = NULL, content = ~s(date),     max.em.its = 75, data = myout$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = NULL, content = NULL,     max.em.its = 75, data = myout$meta, init.type = "Spectral")
plot(ibmContent, type = "perspectives", topics = 1)
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = NULL, content = ~s(date),     max.em.its = 75, data = myout$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = NULL,     max.em.its = 75, data = myout$meta, init.type = "Spectral")
plot(prep, "date", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
labelTopics(ibmPrevFit, c(1:5))
plot(prep, "date", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 2,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 3,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~s(date),     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
labelTopics(ibmPrevFit, c(1:5))
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 3)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 5)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts5, width = 30, main = "Topic 5")
prep <- estimateEffect(1:5 ~ s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, "date", method = "continuous", topics = 3,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 8,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 3,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 2,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 3,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 4,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 5,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 6,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
prep <- estimateEffect(1:50 ~ s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
monthseq
as.numeric(monthseq)
monthnames
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "day", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2008)") monthseq <- seq(from = as.Date("2008-01-01"),     to = as.Date("2008-12-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
out$meta$rating <- as.factor(out$meta$rating) prep <- estimateEffect(1:20 ~ rating + s(day), poliblogPrevFit,     meta = out$meta,     uncertainty = "Global")
plot(poliblogPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, "day", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2008)") monthseq <- seq(from = as.Date("2008-01-01"),     to = as.Date("2008-12-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "day", method = "continuous", topics = 1,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2008)") monthseq <- seq(from = as.Date("2008-01-01"),     to = as.Date("2008-12-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "day", method = "continuous", topics = 7,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2008)") monthseq <- seq(from = as.Date("2008-01-01"),     to = as.Date("2008-12-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "day", method = "continuous", topics = 7,     model = z, printlegend = TRUE, xaxt = "n", xlab = "Time (2008)") monthseq <- seq(from = as.Date("2008-01-01"),     to = as.Date("2008-12-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, covariate = "rating", topics = c(3, 7, 20),     model = poliblogPrevFit, method = "difference",     cov.value1 = "Liberal", cov.value2 = "Conservative",     xlab = "More Conservative ... More Liberal",     main = "Effect of Liberal vs. Conservative",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(poliblogContent, type = "perspectives", topics = 11)
head(data)
as.factor(data$rating)
colnames(isindf)
colSums(isindf)
colSums(as.matrix(isindf))
colSums(isindf[,19:1346])
colnames(isindf)[1:20]
colSums(isindf[,20:1346])
colSums(isindf[,20:1346])>0
isindf[,20:1346][,colSums(isindf[,20:1346])>0]
colSums(isindf[,20:1346][,colSums(isindf[,20:1346])>0])
sort(colSums(isindf[,20:1345][,colSums(isindf[,20:1345])>0]))
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~CYC + s(date),     max.em.its = 40,     data = myout$meta, init.type = "Spectral")
labelTopics(ibmPrevFit, c(1:5))
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 3)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 5)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts5, width = 30, main = "Topic 5")
myout$meta$CYC <- as.factor(myout$meta$CYC)
myout$meta$CYC
myout$meta
myout$meta$CYC
library(tm)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("dj.select.RData"))
system.time(load("isindf_ibm.RData"))
library(stm)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
str(mymeyta)
str(mymeta)
mymeta$CYC
colSums(isinsdf[,20:1345])==0
colSums(isindf[,20:1345])==0
colSums(isinsdf[,20:1345])>0
colSums(isindf[,20:1345])>0
isindf[,20:1345][colSums(isindf[,20:1345])>0]
colSums(isindf[,20:1345][colSums(isindf[,20:1345])>0])
sort(colSums(isindf[,20:1345][colSums(isindf[,20:1345])>0]))
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta, init.type = "Spectral")
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
labelTopics(ibmPrevFit, c(1:5))
prep <- estimateEffect(1:50 ~PRC + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta, init.type = "Spectral")
labelTopics(ibmPrevFit, c(1:5))
prep <- estimateEffect(1:50 ~PRC + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
myout$meta$PRC <- as.factor(myout$meta$PRC)
prep <- estimateEffect(1:50 ~ PRC + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
levels(myout$meta$PRC)
mymeta$PRC
sort(colSums(isindf[,20:1345][colSums(isindf[,20:1345])>0]))
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")sort(colSums(isindf[,20:1345][colSums(isindf[,20:1345])>0]))
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, covariate = "PRL", topics = c(3, 7, 20),     model = ibmPrevFit, method = "difference",     cov.value1 = "TRUE", cov.value2 = "FALSE",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(prep, covariate = "PRL", topics = c(3, 7, 20),     model = ibmPrevFit, method = "difference",     cov.value1 = "TRUE", cov.value2 = "FALSE",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
ibmInteraction <- stm(myout$documents, myout$vocab,     K = 20,     prevalence = ~PRL * date, max.em.its = 75,     data = out$meta, init.type = "Spectral")
ibmInteraction <- stm(myout$documents, myout$vocab,     K = 20,     prevalence = ~PRL * date, max.em.its = 75,     data = myout$meta, init.type = "Spectral")
prep <- estimateEffect(c(20) ~ PRL * date, ibmInteraction,     metadata = out$meta, uncertainty = "None")
prep <- estimateEffect(c(20) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None")
plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "Liberal", linecol = "blue", ylim = c(0, .12),     printlegend = F)
plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "Conservative", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("Liberal", "Conservative"),     lwd = 2, col = c("blue", "red"))
myout$meta$PRL
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta, init.type = "Spectral")
labelTopics(ibmPrevFit, c(1:5))
myout$meta$PRL <- as.factor(myout$meta$PRL)
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, covariate = "PRL", topics = c(3, 7, 20),     model = ibmPrevFit, method = "difference",     cov.value1 = "TRUE", cov.value2 = "FALSE",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(prep, covariate = "PRL", topics = c(3, 7, 20),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "YES",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, covariate = "PRL", topics = c(2, 6, 30),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "YES",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
monthseq <- seq(from = as.Date("2016-12-01"),     to = as.Date("2017-06-01"), by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = PRL,     max.em.its = 75, data = myout$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = "PRL",     max.em.its = 75, data = myout$meta, init.type = "Spectral")
ibmContent <- stm(myout$documents, myout$vocab, K = 5,     prevalence = ~s(date), content = ~PRL,     max.em.its = 75, data = myout$meta, init.type = "Spectral")
plot(ibmContent, type = "perspectives", topics = 1)
labelTopics(ibmPrevFit, c(1:5))
labelTopics(ibmPrevFit, c(1:50))
plot(ibmContent, type = "perspectives", topics = 30)
ibmContent <- stm(myout$documents, myout$vocab, K = 50,     prevalence = ~ PRL + s(date), content = ~PRL,     max.em.its = 75, data = myout$meta, init.type = "Spectral")
plot(ibmContent, type = "perspectives", topics = 30)
plot(ibmContent, type = "perspectives", topics = 50)
ibmInteraction <- stm(myout$documents, myout$vocab,     K = 20,     prevalence = ~PRL * date, max.em.its = 75,     data = myout$meta, init.type = "Spectral")
prep <- estimateEffect(c(20) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None")
plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "Liberal", linecol = "blue", ylim = c(0, .12),     printlegend = F)
plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "Conservative", linecol = "red", add = T,     printlegend = F)
ibmInteraction <- stm(myout$documents, myout$vocab,     K = 20,     prevalence = ~ PRL * date, max.em.its = 75,     data = myout$meta, init.type = "Spectral")
prep <- estimateEffect(c(20) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None")
plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "Liberal", linecol = "blue", ylim = c(0, .12),     printlegend = F)
poliblogInteraction <- stm(out$documents, out$vocab,     K = 20,     prevalence = ~rating * day, max.em.its = 75,     data = out$meta, init.type = "Spectral")
data <- read.csv("poliblogs2008.csv")
processed <- textProcessor(data$documents, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15)
prep <- estimateEffect(c(20) ~ rating * day, poliblogInteraction,     metadata = out$meta, uncertainty = "None")
poliblogInteraction <- stm(out$documents, out$vocab,     K = 20,     prevalence = ~rating * day, max.em.its = 75,     data = out$meta, init.type = "Spectral")
prep <- estimateEffect(c(20) ~ rating * day, poliblogInteraction,     metadata = out$meta, uncertainty = "None")
plot(prep, covariate = "day", model = poliblogInteraction,     method = "continuous", xlab = "Days", moderator = "rating",     moderator.value = "Liberal", linecol = "blue", ylim = c(0,.12),     printlegend = F)
plot(prep, covariate = "day", model = poliblogInteraction,     method = "continuous", xlab = "Days", moderator = "rating",     moderator.value = "Conservative", linecol = "red", add = T,     printlegend = F)
legend(0, .08, c("Liberal", "Conservative"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(20) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None")
plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F)
plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F)
legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(20) ~ rating * day, poliblogInteraction,     metadata = out$meta, uncertainty = "None") plot(prep, covariate = "day", model = poliblogInteraction,     method = "continuous", xlab = "Days", moderator = "rating",     moderator.value = "Liberal", linecol = "blue", ylim = c(0,.12),     printlegend = F) plot(prep, covariate = "day", model = poliblogInteraction,     method = "continuous", xlab = "Days", moderator = "rating",     moderator.value = "Conservative", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("Liberal", "Conservative"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(10) ~ rating * day, poliblogInteraction,     metadata = out$meta, uncertainty = "None") plot(prep, covariate = "day", model = poliblogInteraction,     method = "continuous", xlab = "Days", moderator = "rating",     moderator.value = "Liberal", linecol = "blue", ylim = c(0,.12),     printlegend = F) plot(prep, covariate = "day", model = poliblogInteraction,     method = "continuous", xlab = "Days", moderator = "rating",     moderator.value = "Conservative", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("Liberal", "Conservative"),     lwd = 2, col = c("blue", "red"))
labelTopics(ibmPrevFit, c(1:5))
labelTopics(ibmPrevFit, c(1:50))
prep <- estimateEffect(c(45) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
ibmInteraction <- stm(myout$documents, myout$vocab,     K = 50,     prevalence = ~ PRL * date, max.em.its = 75,     data = myout$meta, init.type = "Spectral")
prep <- estimateEffect(c(45) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(38) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(16) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(33) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
install.packages("shiny")
library(shiny)
runExample("01_hello")
runExample("07_widgets")
mod.out.corr <- topicCorr(ibmPrevFit)
cloud(ibmPrevFit, topic = 7, scale = c(2, .25))
plot(mod.out.corr)
install.packages("igraph")
plot(mod.out.corr)
labelTopics(ibmPrevFit, c(1:50))
labelTopics(ibmPrevFit, c(1:50))
ibmInteraction <- stm(myout$documents, myout$vocab,     K = 50,     prevalence = ~ PRL * date, max.em.its = 75,     data = myout$meta,     init.type = "Spectral",     LDAbeta = FALSE     )
labelTopics(ibmInteraction, c(1:50))
mod.out.corr <- topicCorr(ibmInteraction) cloud(ibmInteraction, topic = 50, scale = c(2, .25)) plot(mod.out.corr)
prep <- estimateEffect(c(50) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
plot(mod.out.corr)
labelTopics(ibmInteraction, c(1:50))
isin_selection <- which(grepl(ibm, mydf$isins))
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
isin_selection <- which(grepl(ibm, mydf$isins))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins))
length(isin_selection)
isindf <- data.frame(mydf[isin_selection,])
dj.select <- dj.corpus_tok[isin_selection]
isindf$date <- as.numeric(isindf$date) - min(as.numeric(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
dj.select <- dj.corpus_tok[isin_selection]
system.time(load("dj.corpus_tok.RData"))
dj.select <- dj.corpus_tok[isin_selection]
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
system.time(save(dj.select, file = c("dj.select_ibm_full.RData")))
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta, init.type = "Spectral")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
isindf <- data.frame(mydf[isin_selection,])
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins))
length(isin_selection)
isindf <- data.frame(mydf[isin_selection,])
colnames(isindf)[1346] <- c("date")
isindf$date <- as.numeric(isindf$date) - min(as.numeric(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
library(stm)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
dj.select <- dj.corpus_tok[isin_selection]
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
system.time(load("dj.select.RData"))
system.time(load("isindf_ibm.RData"))
setwd("C:\\Users\\bungum\\Documents")
system.time(load("dj.select.RData"))
system.time(load("isindf_ibm.RData"))
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
library(stm)
myprocessed <- textProcessor(dj.select$content, metadata = isindf) myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta) mydocs <- myout$documents myvocab <- myout$vocab mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta, init.type = "Spectral")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
isindf_orig <- isindf
dj.select_orig <- dj.select
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins))
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
isindf <- data.frame(mydf[isin_selection,])
colnames(isindf)[1346] <- c("date")
isindf$date <- as.numeric(isindf$date) - min(as.numeric(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
dim(isindf)
dim(isindf_orig)
dim(isindf_orig)isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
length(dj.select)
length(dj.select_orig)
colnames(isindf) == colnames(isindf_orig)
sum(colnames(isindf) == colnames(isindf_orig))
str(isindf$date)
str(isindf_orig$date)
str(isindf_orig$PRL)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
system.time(load("dj.select.RData"))
system.time(load("isindf_ibm.RData"))
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
library(stm)
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf) myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta) mydocs <- myout$documents myvocab <- myout$vocab mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta, init.type = "Spectral")
dim(isindf)
isindf$date
str(isindf$date)
str(isindf$PRL)
levels(isindf$PRL)
system.time(load("isindf_ibm.RData"))
str(isindf$PRL)
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
str(isindf$PRL)
str(isindf$PRL)ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta, init.type = "Spectral")
str(isindf$PRL)
as.factor(isindf$PRL)
sum(as.factor(isindf$PRL))
str(myouy$meta)
str(myout$meta)
str(myout$meta$PRL)
sum(myout$meta$PRL)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
str(myout$meta$PRL)
library(tm)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("dj.select.RData"))
system.time(load("isindf_ibm.RData"))
library(stm)
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf) myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta) mydocs <- myout$documents myvocab <- myout$vocab mymeta <- myout$meta
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
isindf_orig <- isindf
dj.select_orig <- dj.select
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
system.time(load("dj.select.RData"))
system.time(load("isindf_ibm.RData"))
library(stm)
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf) myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta) mydocs <- myout$documents myvocab <- myout$vocab mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
isindf_orig <- isindf
dj.select_orig <- dj.select
system.time(load("isindf_ibm_full.RData"))
setwd("C:\\Users\\bungum\\Documents")
dim(isindf)
system.time(load("dj.select_ibm_full.RData"))
length(dj.select)
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins))
length(isin_selection)
isindf <- data.frame(mydf[isin_selection,])
colnames(isindf)[1346] <- c("date")
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
as.factor(isindf$PRL)
system.time(save(isindf, file = c("isindf_ibm_full.RData")))
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
str(mymeta$PRL)
as.factor(str(mymeta$PRL))
mymeta$date
sort(mymeta$date)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
setwd("C:\\Users\\bungum\\Documents")
system.time(load("dj.select_ibm_full.RData"))
library(stm)
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
system.time(load("isindf_ibm_full.RData"))
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
isindf$PRL
system.time(load("isindf_ibm_full.RData"))
isindf$PRL
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta, init.type = "Spectral")
source("~/visual studio 2017/Projects/rproject1/rproject1/stmDJ.R", echo = TRUE, encoding = "Windows-1252")
labelTopics(ibmPrevFit, c(1:50))
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
mod.out.corr <- topicCorr(ibmPrevFit)
cloud(ibmInteraction, topic = 50, scale = c(2, .25))
plot(mod.out.corr)
cloud(ibmPrevFit, topic = 50, scale = c(2, .25))
plot(mod.out.corr)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta,     init.type = "Spectral",     LDAbeta = FALSE     )
labelTopics(ibmPrevFit, c(1:50))
?labelTopics
sapply(system.time(load("isindf_ibm.RData")), '[[')
sapply(system.time(load("isindf_ibm.RData")), '[[', 2)
sapply(system.time(load("isindf_ibm.RData")), '[[', 1)
sapply(system.time(load("isindf_ibm.RData")), '[', 1)
sapply(labelTopics(ibmPrevFit, c(1:50)), '[[',1)
sapply(labelTopics(ibmPrevFit, c(1:50)), '[[', 2)
sapply(labelTopics(ibmPrevFit, c(1:50)), '[', 2)
sapply(labelTopics(ibmPrevFit, c(1:50)), '[', 10)
typeof(labelTopics(ibmPrevFit, c(1:50)))
system.time(load("isindf_ibm.RData"))[3]
labelTopics(ibmPrevFit, c(1:50))[[1]]
labelTopics(ibmPrevFit, c(1:50))[[1]][,3]
labelTopics(ibmPrevFit, c(1:50))
labelTopics(ibmPrevFit, c(1:50))[[1]][,4]
labelTopics(ibmPrevFit, c(1:50))[[1]][,3]
labelTopics(ibmPrevFit, c(1:50))[[1]]
labelTopics(ibmPrevFit, c(1:50))[[3]]
labelTopics(ibmPrevFit, c(1:50))[[4]]
labelTopics(ibmPrevFit, c(1:50))[[4]]
labelTopics(ibmPrevFit, c(1:50))[[4]]
labelTopics(ibmPrevFit, c(1:50))[[4]]
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 3)$docs[[1]]
thoughts3 <- findThoughts(ibmPrevFit, texts = dj.select$content,     n = 2, topics = 3)$docs[[1]]
thoughts3
plot(thoughts3)
plotQuote(thoughts3)
str(thoughts3)
attributes(thoughts3)
attr(thoughts3, content)
attr(thoughts3, "content")
attr(thoughts3)
attr(thoughts3, "meta.id")
attr(thoughts3, meta.id)
attr(thoughts3, names)
attr(thoughts3, "names")
attr(thoughts3, "names")[1]
str(thoughts3)
strwrap(thoughts3)
mtshortdoc <- substr(lapply(dj.select[1:10], '[[', 1), 0, 200)
myshortdoc <- substr(lapply(dj.select, '[[', 1), 0, 200)
myshortdoc[[1]]
myshortdoc[[2]]
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 3)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 5)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts5, width = 30, main = "Topic 5")
labelTopics(ibmPrevFit, c(1:50))[[3]]
labelTopics(ibmPrevFit, c(1:50))[[4]]
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 2, topics = 50)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts50, width = 30, main = "Topic 50")
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 5)$docs[[1]]
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts50, width = 30, main = "Topic 50")
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 3)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts50, width = 30, main = "Topic 50")
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 5)
str(thoughts5)
dj.select[732]
dj.select[732]$content
dj.select[[732]]$content
dj.select[[479]]$content
dj.select[[6]]$content
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)
thoughts50
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)
thoughts5
str(thoughts50)
dj.select[[125]]$content
dj.select[[809]]$content
dj.select[[276]]$content
dj.select[[809]]$content == dj.select[[125]]$content
isin_selection <- which(!isindf$NENG)
length(isin_selection)
isin_selection <- which(isindf$NENG)
length(isin_selection)
dim(isindf)
length(dj.select)
system.time(save(isindf, file = c("isindf_ibm_full.RData")))
dim(isindf)
system.time(load("isindf_ibm_full.RData"))
dim(isindf)
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG)
length(isin_selection)
length(dj.select)
dj.select <- dj.corpus_tok[isin_selection]
isindf <- data.frame(mydf[isin_selection,])
colnames(isindf)[1346] <- c("date")
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
isindf$PRL
isindf$date
system.time(save(dj.select, file = c("dj.select_ibm_full.RData")))
system.time(save(isindf, file = c("isindf_ibm_full.RData")))
dim(isindf)
dj.select[[1]] in %dj.select
dj.select[[1]] %in% dj.select
dj.select[[2]] %in% dj.select
dj.select[[2]] %in% dj.select[[2]]
dj.select[[2]] %in% dj.select[1:2]
dj.select[2] %in% dj.select[1:2]
any(dj.select[2], dj.select)
any(dj.select[[2]], dj.select)
any(dj.select[[2]]$content, dj.select)
any(dj.select[[2]]$content, dj.select$content)
dj.select[[2]]$content
dj.select[[2]]$content == dj.select[[2]]$content
dj.select[[2]]$content %in% dj.select$content
dj.select$content
lapply(dj.select[1:2], '[[', content)
lapply(dj.select[1:2],  content)
lapply(dj.select[1:2],  "content")
lapply(dj.select[1:2], '[[',.  "content")
lapply(dj.select[1:2], '[[', 1)
dj.select[[2]]$content %in% lapply(dj.select[1:2], '[[', 1)
dj.select[[2]]$content %in% lapply(dj.select, '[[', 1)
dj.select[[2]]$content %in% lapply(dj.select[!2], '[[', 1)
dj.select[[1]]$content %in% lapply(dj.select[!2], '[[', 1)
for (i in 1:10) {     print(dj.select[[i]]$content %in% lapply(dj.select[!2], '[[', 1)) }
for (i in 1:100) {     print(dj.select[[i]]$content %in% lapply(dj.select[!2], '[[', 1)) }
for (i in 1:length(dj.select)) {     print(dj.select[[i]]$content %in% lapply(dj.select[!2], '[[', 1)) }
for (i in 1:length(dj.select)) {     print(dj.select[[i]]$content %in% lapply(dj.select[!i], '[[', 1)) }
isindf$PRL
isindf$date
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta,     init.type = "Spectral",     LDAbeta = FALSE     )
setwd("C:\\Users\\bungum\\Documents")
system.time(load("dj.select_ibm_full.RData"))
system.time(load("isindf_ibm_full.RData"))
isindf$PRL
isindf$date
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
library(stm)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     )
isindf$PRL
system.time(load("isindf_ibm_full.RData"))
isindf$PRL
isindf$date
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     )
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 3)$docs[[1]]
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts50, width = 30, main = "Topic 50")
myshortdoc <- substr(lapply(dj.select, '[[', 1), 0, 200)
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 3)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 5)$docs[[1]]
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts50, width = 30, main = "Topic 50")
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc, + n = 3, topics = 5)
findThoughts(ibmPrevFit, texts = myshortdoc,  n = 3, topics = 5)
str(findThoughts(ibmPrevFit, texts = myshortdoc, n = 3, topics = 5))
str(findThoughts(ibmPrevFit, texts = myshortdoc, n = 3, topics = 50))
for (i in 1:length(dj.select)) {     print(dj.select[[i]]$content %in% lapply(dj.select[!i], '[[', 1)) }
print(dj.select[[468]]$content %in% lapply(dj.select[!468], '[[', 1))
str(findThoughts(ibmPrevFit, texts = myshortdoc, n = 3, topics = 50))
dj.select[[658]]$content == dj.select[[656]]$content
print(dj.select[[468]]$content %in% lapply(dj.select, '[[', 1))
print(dj.select[[656]]$content %in% lapply(dj.select[!656], '[[', 1))
print(dj.select[[656]]$content %in% lapply(dj.select, '[[', 1))
print(dj.select[[656]]$content %in% lapply(dj.select[658], '[[', 1))
print(dj.select[[656]]$content %in% lapply(dj.select[659], '[[', 1))
dj.select[659]
dj.select[!659]
dj.select[659]
dj.select[655:659]
dj.select[655:659]
dj.select[-658]
print(dj.select[[656]]$content %in% lapply(dj.select[-656], '[[', 1))
print(dj.select[[656]]$content %in% lapply(dj.select[-656], '[[', 1))
lapply(dj.select$content %in% lapply(dj.select[-656], '[[', 1))
lapply(dj.select$content %in% lapply(dj.select[-656], '[[', 1)))
for (i in 1:length(dj.select)) {     print(dj.select[[i]]$content %in% lapply(dj.select[-i], '[[', 1)) }
lapply(dj.select$content %in% lapply(dj.select[-656], '[[', 1)))
any(dj.select[[656]]$content, dj.select$content )
any(dj.select$content ==  dj.select[[656]]$content)
any(dj.select$content == dj.select[[656]]$content)
any(dj.select[[658]]$content == dj.select[[656]]$content)
any(lapply(dj.select[-i], '[[', 1) == dj.select[[656]]$content)
any(lapply(dj.select[-656], '[[', 1) == dj.select[[656]]$content)
dj.select = dj.select
any(lapply(dj.select[-656], '[[', 1) == dj.select[[656]]$content)
duplicated
duplicated(dj.select[656], dj.select[-656])
?duplicated
duplicated(dj.select)
duplicated(lapply(dj.select[-656], '[[', 1))
sum(duplicated(lapply(dj.select[-656], '[[', 1)))
sum(duplicated(lapply(dj.select, '[[', 1)))
duplicated(lapply(dj.select, '[[', 1))
duplicated(dj.select)
duplicated(lapply(dj.select, '[[', 1))
duplicated(lapply(dj.select, '[[', 1))
isindf[duplicated(lapply(dj.select, '[[', 1))]
dim(isindf[duplicated(lapply(dj.select, '[[', 1))])
length(dj.select[duplicated(lapply(dj.select, '[[', 1))])
length(dj.select[!duplicated(lapply(dj.select, '[[', 1))])
length(dj.select[!duplicated(lapply(dj.select, '[[', 1))])
dim(isindf[!duplicated(lapply(dj.select, '[[', 1)),])
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))])
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))]
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
length(dj.select)
dim(isindf)
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
dim(isindf)
system.time(load("isindf_ibm_full.RData"))
system.time(load("dj.select_ibm_full.RData"))
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))]
length(dj.select)
dim(isindf)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 15,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     )
ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     )
sageLabels
sageLabels(ibmPrevFit)
myshortdoc <- substr(lapply(dj.select, '[[', 1), 0, 200)
thoughts3 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 3)$docs[[1]]
thoughts5 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 5)$docs[[1]]
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)$docs[[1]]
par(mfrow = c(1, 2), mar = c(.5, .5, 1, .5))
plotQuote(thoughts3, width = 30, main = "Topic 3")
plotQuote(thoughts50, width = 30, main = "Topic 50")
sageLabels(ibmPrevFit)
labelTopics(ibmPrevFit)
sageLabels(ibmPrevFit)
labelTopics(ibmPrevFit, c(1:10))
labelTopics(ibmPrevFit, c(40:50))
sageLabels(ibmPrevFit)
thoughts41 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 41)$docs[[1]] thoughts42 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 42)$docs[[1]] thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)$docs[[1]]
par(mfrow = c(1, 3), mar = c(.5, .5, 1, .5)) plotQuote(thoughts41, width = 30, main = "Topic 41") plotQuote(thoughts42, width = 30, main = "Topic 42") plotQuote(thoughts50, width = 30, main = "Topic 50")
sageLabels(ibmPrevFit)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("dj.select_ibm_full.RData"))
system.time(load("isindf_ibm_full.RData"))
library(stm)
str(isindf$PRL)
str(isindf$date)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta)
mydocs <- myout$documents
myvocab <- myout$vocab
mymeta <- myout$meta
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
system.time(ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     ))
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))]
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
system.time(ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     ))
sageLabels(ibmPrevFit)
plot(ibmPrevFit)
sageLabels(ibmPrevFit)
sageLabels(ibmPrevFit)[1]
sageLabels(ibmPrevFit)[1,2]
sageLabels(ibmPrevFit)[,1]
sageLabels(ibmPrevFit)[[1]]
sageLabels(ibmPrevFit)[[1]]
sageLabels(ibmPrevFit)[[4]]
sageLabels(ibmPrevFit)[[4]][1]
sageLabels(ibmPrevFit)[[4]][[1]]
str(sageLabels(ibmPrevFit)[[4]])
sageLabels(ibmPrevFit)[[4]][[1]]
sageLabels(ibmPrevFit)[[4]]
unlist(sageLabels(ibmPrevFit)[[4]])
sageLabels(ibmPrevFit)[[4]]
str(sageLabels(ibmPrevFit)[[4]])
str(sageLabels(ibmPrevFit))
str(sageLabels(ibmPrevFit))
sageLabels(ibmPrevFit)$marginal
str(sageLabels(ibmPrevFit)$marginal)
sageLabels(ibmPrevFit)$marginal$frex
sageLabels(ibmPrevFit)$marginal$frex[1]
sageLabels(ibmPrevFit)$marginal$frex[1,]
sageLabels(ibmPrevFit)$marginal$frex[50,]
sageLabels(ibmPrevFit)$marginal$frex[1,]
sageLabels(ibmPrevFit)$marginal
str(sageLabels(ibmPrevFit)$marginal)
sageLabels(ibmPrevFit)$marginal$lift
sageLabels(ibmPrevFit)$marginal$lift[1,]
sageLabels(ibmPrevFit)$marginal$prob[1,]
sageLabels(ibmPrevFit)$marginal$lift[1,]
sageLabels(ibmPrevFit)$marginal$marginal[1,]
str(sageLabels(ibmPrevFit)$marginal)
sageLabels(ibmPrevFit)$marginal$frex[1,]
sageLabels(ibmPrevFit)$marginal$score[1,]
sageLabels(ibmPrevFit)$marginal$lift[1,]
sageLabels(ibmPrevFit)$marginal$prob[1,]
sageLabels(ibmPrevFit)$marginal$prob[49,]
sageLabels(ibmPrevFit)$marginal$frex[49,]
sageLabels(ibmPrevFit)$marginal$frex[21,]
thoughts41 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 41)$docs[[1]]
myshortdoc <- substr(lapply(dj.select, '[[', 1), 0, 200)
par(mfrow = c(1, 3), mar = c(.5, .5, 1, .5))
plotQuote(thoughts41, width = 30, main = "Topic 41")
plotQuote(thoughts42, width = 30, main = "Topic 42")
plotQuote(thoughts50, width = 30, main = "Topic 50")
thoughts41 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 41)$docs[[1]]
thoughts42 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 42)$docs[[1]]
thoughts50 <- findThoughts(ibmPrevFit, texts = myshortdoc,     n = 3, topics = 50)$docs[[1]]
par(mfrow = c(1, 3), mar = c(.5, .5, 1, .5))
plotQuote(thoughts41, width = 30, main = "Topic 41")
plotQuote(thoughts42, width = 30, main = "Topic 42")
plotQuote(thoughts50, width = 30, main = "Topic 50")
plot(ibmContent, type = "perspectives", topics = 50)
ibmContent <- stm(myout$documents, myout$vocab, K = 50,     prevalence = ~ PRL + s(date), content = ~PRL,     max.em.its = 75, data = myout$meta, init.type = "Spectral")
plot(ibmContent, type = "perspectives", topics = 50)
sageLabels(ibmContent)$marginals$frex
sageLabels(ibmContent)$marginal
sageLabels(ibmContent)$marginal$frex
plot(ibmContent, type = "perspectives", topics = 49)
plot(ibmContent, type = "perspectives", topics = 45)
plot(ibmContent, type = "perspectives", topics = 12)
plot(ibmContent, type = "perspectives", topics = 44)
labelTopics(ibmContent)
sageTopics(ibmContent)
sageLabels(ibmContent)
system.time(ibmInteraction <- stm(myout$documents, myout$vocab,     K = 50,     prevalence = ~ PRL * date, max.em.its = 75,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     ) )
prep <- estimateEffect(c(50) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
labelTopics(ibmContent)
prep <- estimateEffect(c(38) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
mod.out.corr <- topicCorr(ibmPrevFit)
cloud(ibmPrevFit, topic = 50, scale = c(2, .25))
plot(mod.out.corr)
prep <- estimateEffect(c(1) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(2) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(3) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-06-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
plot(prep, covariate = "PRL", topics = c(2, 6, 30),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "YES",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, covariate = "PRL", topics = c(2, 6, 30),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "NO",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, covariate = "PRL", topics = c(2, 6, 30),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "NO",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
plot(prep, covariate = "PRL", topics = c(2, 6, 30),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "NO",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('Obama', 'Sarah Palin', 'Bush Presidency'))
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-06-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
sageLabels(ibmPrevFit)
str(sageLabels(ibmPrevFit))
sageLabels(ibmPrevFit)$marginal$fex
sageLabels(ibmPrevFit)$marginal$frex
plot(prep, covariate = "PRL", topics = c(42, 49, 50),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "NO",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('airline', 'nvidia', 'salesforce'))
isindf$date
sort(isindf$date)
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG)
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
system.time(load("dj.corpus_tok.RData"))
dj.select <- dj.corpus_tok[isin_selection]
isindf <- data.frame(mydf[isin_selection,])
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))]
dim(isindf)
length(dj.select)
system.time(save(dj.select, file = c("dj.select_ibm_full_origdate.RData")))
system.time(save(isindf, file = c("isindf_ibm_full_origdate.RData")))
isindf$date
sort(isindf$date)
sort(isindf$date)[1:10]
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2015-01-27"),     to = as.Date("2017-07-19"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
monthnames
hist(isindf$date)
hist(as.Date(isindf$date))
hist(as.Date(isindf$date), breaks=10)
as.Date(isindf$date)
as.integer(isindf$date)
hist(as.integer(isindf$date))
hist(as.integer(isindf$date)- min(as.integer(isindf$date)))
as.integer(isindf$date) - min(as.integer(isindf$date))
sort(as.integer(isindf$date) - min(as.integer(isindf$date)))
sort(isindf$date - min(isindf$date))
isindf$date - min(isindf$date)
sort(isindf$date - min(isindf$date))
isindf$date - min(isindf$date)
which((isindf$date - min(isindf$date))==534)
isindf$date[534]
isindf$date[635]
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-07-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
isindf <- data.frame(mydf[isin_selection,])
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))]
dim(isindf)
isindf$date <- as.integer(isindf$date) - min(as.integer(isindf$date))
isindf$PRL <- ifelse(isindf$PRL == T, "YES", "NO")
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
setwd("C:\\Users\\bungum\\Documents")
system.time(load("mydf_full_columns_fixed_1346_with_date.RData"))
system.time(load("dj.corpus_tok.RData"))
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-07-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
ibm <- "US4592001014"
isin_selection <- which(grepl(ibm, mydf$isins) & !mydf$NENG & mydf[,]$date > as.Date("2016-07-01", "%Y-%m-%d") & mydf[,]$date < as.Date("2017-06-01", "%Y-%m-%d"))
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
isindf <- data.frame(mydf[isin_selection,])
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))]
dim(isndf)
dim(isindf)
length(dj.select)
length(isin_selection)
dj.select <- dj.corpus_tok[isin_selection]
isindf <- data.frame(mydf[isin_selection,])
length(isin_selection)
length(dj.select)
dim(isindf)
duplicated(lapply(dj.select, '[[', 1))
dj.select[[1]]$content
dj.select[[1]]
dj.corpus_tok[[1]]
dj.corpus_tok[[1]]$content
dj.corpus_tok[1]$content
dj.corpus_tok[[1]]$content
str(dj.corpus_tok)
length(dj.corpus_tok)
system.time(load("dj.corpus_tok.RData"))
length(dj.corpus_tok)
dj.corpus_tok$content[[3]]
dj.corpus_tok[[3]]$content
dj.corpus_tok[3]$content
dj.corpus_tok$content[[3]]$content
dj.select
dj.select <- dj.corpus_tok$content[isin_selection]
dj.select[[3]]$content
length(dj.select)
dim(isindf)
system.time(save(dj.select, file = c("dj.select_ibm_1y.RData")))
system.time(save(isindf, file = c("isindf_ibm_1y_full_origdate.RData")))
rm(dj.corpus_tok, envir = as.environment(".GlobalEnv"))
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
library(stm)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
dim(isindf)
length(dj.select)
djmyprocessed <- textProcessor(dj.select$content, metadata = isindf)
head(dj.select)
head(dj.select$content)
head(isindf)
head(dj.select$content)
dj.select
unlist(dj.select)
system.time(load("dj.select_ibm_full.RData"))
system.time(load("isindf_ibm_full.RData"))
dj.select
dj.select[[1]]$content
dj.select[1]$content
dj.select$content[[1]]
library(stm)
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
which(isindf$date > 549)
which(isindf$date > 539)
length(which(isindf$date > 539))
length(which(isindf$date > 1))
length(which(isindf$date > 539))
isindf <- isindf[!duplicated(lapply(dj.select, '[[', 1)),]
dj.select <- dj.select[!duplicated(lapply(dj.select, '[[', 1))]
length(which(isindf$date > 539))
isindf <- isindf[which(isindf$date > 539),]
dim(isindf)
dj.select <- dj.select[which(isindf$date > 539)]
myprocessed <- textProcessor(dj.select$content, metadata = isindf)
myout <- prepDocuments(myprocessed$documents, myprocessed$vocab, myprocessed$meta, lower.thresh = 15)
system.time(ibmPrevFit <- stm(documents = myout$documents,     vocab = myout$vocab, K = 50,     prevalence = ~PRL + s(date),     max.em.its = 75,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     ))
labelTopics(ibmPrevFit, c(1:50))
sageLabels(ibmPrevFit)$marginal$frex
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "None")
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "None")
plot(ibmPrevFit, type = "summary", xlim = c(0, .3))
plot(prep, covariate = "PRL", topics = c(42, 49, 50),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "NO",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('airline', 'nvidia', 'salesforce'))
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2015-06-01"),     to = as.Date("2017-07-19"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-06-01"),     to = as.Date("2017-07-19"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
monthseq
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)") monthseq <- seq(from = as.Date("2016-07-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),     labels = monthnames)
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)")
isindf$date
sort(isindf$date)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq))+541,     labels = monthnames)
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "Global")
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2016-2017)") monthseq <- seq(from = as.Date("2016-07-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq))+541,     labels = monthnames)
system.time(ibmInteraction <- stm(myout$documents, myout$vocab,     K = 50,     prevalence = ~ PRL * date, max.em.its = 75,     data = myout$meta,     init.type = "Spectral",     LDAbeta = TRUE     ) )
prep <- estimateEffect(c(42) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
sageLabels(ibmInteraction)$marginal$frex
sageLabels(ibmInteraction)$marginal$top
sageLabels(ibmInteraction)$marginal$prob
sageLabels(ibmInteraction)$marginal$frex
plot(sageLabels(ibmInteraction))
plot(ibmInteraction)
plot(sageLabels(ibmInteraction))
sageLabels(ibmInteraction)$marginal$frex
sageLabels(ibmInteraction)$marginal$frex
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2016-2017)") monthseq <- seq(from = as.Date("2016-07-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq))+541,     labels = monthnames)
plot(ibmInteraction)
sageLabels(ibmInteraction)$marginal$frex
prep <- estimateEffect(1:50 ~ PRL + s(date), ibmPrevFit,     meta = myout$meta,     uncertainty = "None")
plot(prep, covariate = "PRL", topics = c(42, 49, 50),     model = ibmPrevFit, method = "difference",     cov.value1 = "YES", cov.value2 = "NO",     xlab = "More True ... More False",     main = "Effect of True vs. False (PRL)",     xlim = c(-.1, .1), labeltype = "custom",     custom.labels = c('airline', 'nvidia', 'salesforce'))
plot(prep, "date", method = "continuous", topics = 42,     model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2016-2017)") monthseq <- seq(from = as.Date("2016-07-01"),     to = as.Date("2017-06-01"), by = "month") monthnames <- months(monthseq) axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq))+541,     labels = monthnames)
prep <- estimateEffect(c(42) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
prep <- estimateEffect(c(42) ~ PRL * date, ibmInteraction,     metadata = myout$meta, uncertainty = "None") plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Date", moderator = "PRL",     moderator.value = "YES", linecol = "blue", ylim = c(0, .12),     printlegend = F) plot(prep, covariate = "date", model = ibmInteraction,     method = "continuous", xlab = "Days", moderator = "PRL",     moderator.value = "NO", linecol = "red", add = T,     printlegend = F) legend(0, .08, c("YES", "NO"),     lwd = 2, col = c("blue", "red"))
plot(ibmContent, type = "perspectives", topics = 42)
system.time(ibmContent <- stm(myout$documents, myout$vocab, K = 50,     prevalence = ~ PRL + s(date), content = ~PRL,     max.em.its = 75, data = myout$meta, init.type = "Spectral"))
plot(ibmContent, type = "perspectives", topics = 42)
sageLabels(ibmContent)$marginal$frex
plot(ibmContent, type = "perspectives", topics = 44)
plot(ibmContent, type = "perspectives", topics = 49)
plot(ibmContent, type = "perspectives", topics = 43)
plot(ibmContent, type = "perspectives", topics = 36)
str(ibmContent)
sageLabels(ibmContent)$marginal$frex
labelTopics(ibmContent)
labelTopics(ibmPrevFit)
sageLabels(ibmContent)$marginal$frex
sageLabels(ibmContent)$marginal$frex
str(sageLabels(ibmContent)$marginal)
sageLabels(ibmContent)$marginal$prob
sageLabels(ibmContent)$marginal$frex
sageLabels(ibmContent)$marginal$prob[36]
sageLabels(ibmContent)$marginal$prob
str(ibmContent)
ibmContent$beta
str(ibmContent$beta)
str(ibmPrevFit$beta)
str(ibmPrevFit$beta$logbeta)
ibmPrevFit$beta$logbeta
ibmPrevFit$beta$logbeta[42,]
lev <- function (s1, s2) {     if (length(s1)) == 0     return(length(s2))     if (length(s2) == 0)     return(length(s1)) }
lev <- function (s1, s2) {     if (length(s1)) == 0     return(length(s2))     if (length(s2) == 0)     return(length(s1)) }
lev <- function (s1, s2) {     if (length(s1)) == 0     return(length(s2))     if (length(s2) == 0)     return(length(s1)) }
lev <- function(s1, s2) {     if (length(s1)) == 0     return(length(s2))     if (length(s2) == 0)     return(length(s1)) }
lev <- function(s1, s2) {     return(s1) }
lev <- function(s1, s2) {     return(s2) }
lev <- function(s1, s2) {     if (length(s1) == 0)         return(length(s2))      if (length(s2) == 0)     return(length(s1)) }
lev("foo", "bar")
foo <- "lasse"
substr(foo, 2, lengtth(foo))
substr(foo, 2, length(foo))
substr(foo, 2, nchar(foo))
substr(foo, 2, nchar(foo))
foo <- "k"
substr(foo, 2, nchar(foo))
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     min(1 + lev(substr(s1, 2, nchar(s1))),         1 + lev(substr(s2, 2, nchar(s2)))         ) }
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2))),         lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))         ) }
lev("lasse", "liten")
foo[1]
foo <- "foobar"
foo[4]
foo[[4]]
char(foo)
str(foo)
foo[1,2]
foo[c(1)]
foo[c(1)]
letters
letter(foobar, 1)
install.packages("biostrings")
install.packages("Biostrings")
substr(string1, 3, nchar(string1))
substr(foobar, 3, nchar(foobar))
substr(foo, 3, nchar(foo))
substr(foo, 1, 2)
substr(foo, 1, 1)
substr(foo, 1)
substr(foo, 1, 1)
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s2, 1, 1) == substr(s2, 1, 1))     lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))     min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))) }
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s2, 1, 1) == substr(s2, 1, 1))     lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))     min(1 + lev(substr(s1, 2, nchar(s1)), s2)) }
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s2, 1, 1) == substr(s2, 1, 1)) {         lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))     }     min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))         ) }
lev("lasse", "liten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s2, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))         ) } lev("lasse", "liten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))         ) } lev("lasse", "liten")
lev("asse", "iten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     else {         min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))         )     } }
lev("lasse", "liten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     print(s1,s2)     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     else {         min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))         )     } } lev("lasse", "liten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     print(s1)     print(s2)     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     else {         min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))         )     } } lev("lasse", "liten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     print(s1)     print(s2)     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     else {         return(min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2)))         ))     } } lev("lasse", "liten")
lev("lasse", "liten")
lev("se", "en")
lev("sen", "ten")
lev("sen", "ten")lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     print(s1)     print(s2)     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     else {         return(min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2))),         1 + lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))         ))     } } lev("lasse", "liten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     print(s1)     print(s2)     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     else {         return(min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2))),         1 + lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))         ))     } }
lev("lasse", "liten")
lev("lasse", "liten")
lev("sen", ten")     " )
lev("sen", "ten")
lev("lasse", "liten")
lev("asse", "iten")
lev("Lasse", "lars")
lev("lasse", "lars")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     print(s1)     print(s2)     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         return(lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2))))     }     else {         min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2))),         1 + lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))         )     } } lev("lasse", "liten")
lev <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))      if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))     }     else {         min(1 + lev(substr(s1, 2, nchar(s1)), s2),         1 + lev(s1, substr(s2, 2, nchar(s2))),         1 + lev(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))         )     } } lev("lasse", "liten")
system.time(lev("lasse", "liten"))
system.time(levmemo("lasse", "liten"))
levmemo <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))     if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         levmemo(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))     }     else {         min(1 + levmemo(substr(s1, 2, nchar(s1)), s2),         1 + levmemo(s1, substr(s2, 2, nchar(s2))),         1 + levmemo(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))         )     } }
system.time(lev("lasse", "liten"))
system.time(levmemo("lasse", "liten"))
system.time(lev("lasse", "liten"))
system.time(levmemo("lasse", "liten"))
if (!memo) memo = df
if (!exists(memo)) memo = df
if (!exists(memo)) 
1
if (!exists("memo")) 
T
if (!exists("memo")) T
if (!exists("memo")) memo = df()
if (!exists("memo")) memo = list()
memo
if (!exists("memo")) memo = list()
library(hashmap)
install.packages("hashmap")
library(hashmap)
memo = hashmap()
memo = hashmap()
memo = hashmap()$clear()
memo = hashmap(1)$clear()
memo = hashmap(1,2)$clear()
memo
memo$insert(1,2)
memo$insert(c(1,2),c(3,4))
H$insert(c("a", "y", "z"), c(100, 200, 300))
memo$insert(c("a", "y", "z"), c(100, 200, 300))
memo$insert
memo$insert()
memo$empty
memo = hashmap(1,2)
memo$insert(c("a", "y", "z"), c(100, 200, 300))
memo$insert(c(1,2,3), c(100, 200, 300))
memo
memo = hashmap(as.integer(1),as.integer(2))
memo
memo$insert(c(1,2,3), c(100, 200, 300))
memo
if (!memo) memo = hashmap(list(-1, -1), 2)
if (!exists("memo")) memo = hashmap(list(-1, -1), 2)
memo$clear
memo$clear()
if (!exists("memo")) memo = hashmap(list(-1, -1), 2)
memo
    if (!exists("memo")) memo = hashmap(list(-1, -1), 2)
memo
memo = hashmap(list(-1, -1), 2)
memo = hashmap(c(-1, -1), 2)
memo = hashmap(list(-1, -1), 2)
install.packages("memoise")
library(memoise)
levm <- memoise(lev)
replicate(5, lev("lasse", "liten"))
replicate(5, levm("lasse", "liten"))
system.time(replicate(5, lev("lasse", "liten")))
system.time(replicate(5, levm("lasse", "liten")))
replicate(5, levm("lassdgsdfgsdfgsgsdfgsfgsdfgsdfgsdfgsfsdfgsdfgsdfgsdfgse", "litesdfgsdfgsgfsdfgsdfgsfdgsfdgsfdgsfdgsfdgsfdgsdfgsfgn"))
replicate(5, levm("lasselasselasse", "litenlitenliten"))
system.time(replicate(5, lev("lasse", "liten")))
system.time(replicate(5, lev("lasselasse", "litenliten")))
system.time(replicate(5, lev("lassel", "litenl")))
system.time(replicate(5, lev("lassela", "litenli")))
system.time(replicate(5, lev("lasselas", "litenlit")))
system.time(replicate(5, lev("lasselass", "litenlite")))
system.time(replicate(5, lev("lasselasse", "litenliten")))
system.time(replicate(5, levm("lasselasse", "litenliten")))
system.time(replicate(5, levm("lasselasse", "litenliten")))
system.time(replicate(5, levm("lasselasselasse", "litenlitenliten")))
system.time(levm("lasselasselasse", "litenlitenliten"))
levmemo <- function(s1, s2) {     if (nchar(s1) == 0)         return(nchar(s2))     if (nchar(s2) == 0)         return(nchar(s1))     if (substr(s1, 1, 1) == substr(s2, 1, 1)) {         levm(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))     }     else {         min(1 + levm(substr(s1, 2, nchar(s1)), s2),         1 + levm(s1, substr(s2, 2, nchar(s2))),         1 + levm(substr(s1, 2, nchar(s1)), substr(s2, 2, nchar(s2)))         )     } }
system.time(levmemo("lasse", "liten"))
system.time(levmemo("lasselasse", "litenliten"))
system.time(levmemo("lasselassela", "litenlitenli"))
system.time(levmemo("lasselassela", "litenlitenli"))
levmemo("lasselassela", "litenlitenli")
levmemo("lasselassela", "litenlite")
levmemo("lasselassela", "litenlite")
levmemo("lasselassela", "litenlitenli")
